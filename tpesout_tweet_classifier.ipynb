{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "################################################\n",
    "# intialize word enumeration\n",
    "################################################\n",
    "\n",
    "# config\n",
    "INCLUDE_BIGRAMS = False\n",
    "\n",
    "# definitions\n",
    "PADDING = 0\n",
    "UNKNOWN_WORD = 1\n",
    "ENUMERATION_BEGIN = 2\n",
    "\n",
    "# coverts this string to: ['converts', 'this', 'string', 'to', 'converts this', 'this string', 'string to']\n",
    "def add_bigrams_to_tweet(tweet):\n",
    "    # collect bigrams\n",
    "    bigrams = list()\n",
    "    prev = None\n",
    "    for word in tweet:\n",
    "        if prev is not None: bigrams.append(prev + \" \" + word)\n",
    "        prev = word\n",
    "    # append bigrams\n",
    "    for bigram in bigrams:\n",
    "        tweet.append(bigram)\n",
    "    # return, but this is the same list we passed in\n",
    "    return tweet\n",
    "\n",
    "\n",
    "def enumerate_words(tweets):\n",
    "    # enumerate all words (bigram or otherwise)\n",
    "    corpus = set()\n",
    "    for tweet in tweets:\n",
    "        for word in tweet:\n",
    "            corpus.add(word)\n",
    "    # save in dictionary\n",
    "    w2i = dict()\n",
    "    i = ENUMERATION_BEGIN\n",
    "    for word in list(corpus):\n",
    "        w2i[word] = i\n",
    "        i += 1\n",
    "    # return dict\n",
    "    return w2i\n",
    "\n",
    "\n",
    "def get_word_vector(w2i, tweet, enforce_length=None):\n",
    "    # prep\n",
    "    vec = list()\n",
    "    i = 0\n",
    "    # convert words to integers\n",
    "    for w in tweet:\n",
    "        if w in w2i: vec.append(w2i[w])\n",
    "        else: vec.append(UNKNOWN_WORD)\n",
    "        i += 1\n",
    "        # stop at enforce_length (if set)\n",
    "        if enforce_length is not None and i + 1 == enforce_length:\n",
    "            break\n",
    "    # pad\n",
    "    if enforce_length is not None:\n",
    "        while i < enforce_length:\n",
    "            vec.append(PADDING)\n",
    "            i += 1   \n",
    "    # fin\n",
    "    return vec\n",
    "        \n",
    "    \n",
    "# from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "# if bigram:\n",
    "#     vectorizer = CountVectorizer(ngram_range=(1, 2), token_pattern=r'\\\\b\\\\w+\\\\b',\n",
    "#                                  min_df=1, decode_error='ignore', stop_words=stop_words,\n",
    "#                                  lowercase=lowercase)\n",
    "#     vectorizer = CountVectorizer(decode_error='ignore', stop_words=stop_words, lowercase=lowercase)\\n\","
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "################################################\n",
    "# file parsing functions\n",
    "################################################\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import string, re\n",
    "import collections\n",
    "import numpy as np\n",
    "import glove\n",
    "from glove.glove_cython import fit_vectors, transform_paragraph\n",
    "\n",
    "# definitions\n",
    "HC=\"HillaryClinton\"\n",
    "DT=\"realDonaldTrump\"\n",
    "NA=\"none\"\n",
    "HANDLES = [HC,DT,NA]\n",
    "HANDLE_MAP = {NA:-1, HC:0, DT:1}\n",
    "\n",
    "# read csv file, return handles and tweets\n",
    "def parse_tweet_csv(file, file_encoding=\"utf8\"):\n",
    "    # init\n",
    "    handles, tweets = [], []\n",
    "    \n",
    "    # read file\n",
    "    linenr = -1\n",
    "    with open(file, encoding=file_encoding) as input:\n",
    "        try:\n",
    "            for line in input:\n",
    "                linenr += 1\n",
    "                if linenr == 0: continue\n",
    "                \n",
    "                # get contents\n",
    "                line = line.split(\",\")\n",
    "                if line[0] in HANDLES: #label and irst line of tweet\n",
    "                    handles.append(line[0])\n",
    "                    tweet = ','.join(line[1:])\n",
    "                    tweets.append(tweet)\n",
    "                else: #second+ line of tweet\n",
    "                    tweet = tweets.pop()\n",
    "                    tweet += ','.join(line)\n",
    "                    tweets.append(tweet)\n",
    "        except Exception as e:\n",
    "            print(\"Exception at line {}: {}\".format(linenr, e))\n",
    "            raise e\n",
    "    \n",
    "    # sanity checks\n",
    "    assert len(handles) == len(tweets)\n",
    "    print(\"Found {} tweets in {} lines\".format(len(tweets), linenr + 1))\n",
    "    \n",
    "    # return data\n",
    "    return handles, tweets\n",
    "\n",
    "\n",
    "##########################################\n",
    "### coverting tweet strings to numbers ###\n",
    "\n",
    "# coverting labels to integers\n",
    "def int_labels(labels):\n",
    "    return list(map(lambda x: HANDLE_MAP[x], labels))\n",
    "\n",
    "#tokenizing\n",
    "_tokenizer = TweetTokenizer()\n",
    "_punctuation = set(string.punctuation)\n",
    "def tokenize(tweet, lowercase=True, strip_urls=True, strip_punctuation=True):\n",
    "    tokens = _tokenizer.tokenize(tweet)\n",
    "    if lowercase: tokens = list(map(lambda x: x.lower(), tokens))\n",
    "    if strip_urls: tokens = list(filter(lambda x: not x.startswith(\"http\"), tokens))\n",
    "    if strip_punctuation: #https://stackoverflow.com/questions/265960/best-way-to-strip-punctuation-from-a-string-in-python\n",
    "        tokens = list(filter(lambda x: x.startswith(u'@') or x.startswith(u'#') or x not in _punctuation and not re.match(u\"[^\\w\\d'\\s$]+\", x), tokens))\n",
    "    return tokens\n",
    "\n",
    "# get all tweets\n",
    "def import_text(tweets):\n",
    "    tokenized_tweets = [tokenize(tweet) for tweet in tweets]\n",
    "    if INCLUDE_BIGRAMS:\n",
    "        for tweet in tokenized_tweets:\n",
    "            add_bigrams_to_tweet(tweet)\n",
    "    w2i = enumerate_words(tokenized_tweets)\n",
    "    return [get_word_vector(w2i, tweet, enforce_length=50) for tweet in tokenized_tweets], tokenized_tweets, w2i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4743 tweets in 6251 lines\n",
      "\n",
      "The question in this election: Who can put the plans into action that will make your life better? https://t.co/XreEY9OicG\n",
      "['the', 'question', 'in', 'this', 'election', 'who', 'can', 'put', 'the', 'plans', 'into', 'action', 'that', 'will', 'make', 'your', 'life', 'better']\n",
      "[6205, 1260, 6973, 3470, 5228, 6486, 1561, 4059, 6205, 4799, 4669, 6197, 901, 3193, 6351, 2951, 1598, 2850, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "################################################\n",
    "# get raw test data\n",
    "################################################\n",
    "import random\n",
    "\n",
    "# init\n",
    "TEST_RATIO = 0.1\n",
    "assert TEST_RATIO > 0 and TEST_RATIO < 1\n",
    "\n",
    "# get data\n",
    "text_handles, raw_tweets = parse_tweet_csv(\"train.csv\")\n",
    "handles = int_labels(text_handles)\n",
    "tweets, tokenized_tweets, word_mapping = import_text(raw_tweets)   \n",
    "data_vector_size = len(tweets[0])\n",
    "\n",
    "### validation\n",
    "for i in range(1):\n",
    "    rand_i = random.randint(0, len(raw_tweets))\n",
    "    print()\n",
    "    print(raw_tweets[i].strip())\n",
    "    print(tokenized_tweets[i])\n",
    "    print(tweets[i])\n",
    "    print(handles[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Separated into 4251 train and 492 test (10%)\n",
      "\n",
      "   handle  length                                         tweet_data\n",
      "0       0      50  [6205, 1260, 6973, 3470, 5228, 6486, 1561, 405...\n",
      "1       0      50  [575, 7959, 7954, 4374, 5501, 177, 4904, 5882,...\n",
      "2       0      50  [4785, 4460, 112, 2603, 4014, 2423, 5996, 2290...\n",
      "3       1      50  [6490, 3403, 683, 4914, 5717, 7838, 3399, 6561...\n",
      "4       0      50  [1540, 7954, 4374, 7236, 6593, 1708, 7991, 676...\n"
     ]
    }
   ],
   "source": [
    "################################################\n",
    "# split test data into train and test\n",
    "################################################\n",
    "import pandas as pd\n",
    "\n",
    "LABEL = 'handle'\n",
    "DATA = 'tweet_data'\n",
    "LENGTH = 'length'\n",
    "\n",
    "VOCAB_SIZE = len(word_mapping) + ENUMERATION_BEGIN\n",
    "\n",
    "# split into test and train\n",
    "train_labels, train_data, test_labels, test_data = list(), list(), list(), list()\n",
    "for handle, tweet in zip(handles, tweets):\n",
    "    if random.random() < TEST_RATIO:\n",
    "        test_labels.append(handle)\n",
    "        test_data.append(tweet)\n",
    "    else:\n",
    "        train_labels.append(handle)\n",
    "        train_data.append(tweet)\n",
    "\n",
    "# document and validate\n",
    "print(\"Separated into {} train and {} test ({}%)\\n\".format(len(train_data), len(test_data), \n",
    "                                                         int(100.0 * len(test_data) / len(raw_tweets))))\n",
    "assert len(train_labels) == len(train_data) and len(train_data) > 0\n",
    "assert len(test_labels) == len(test_data) and len(test_data) > 0\n",
    "assert len(test_labels) > len(tweets) * (TEST_RATIO - .05)\n",
    "assert len(test_labels) < len(tweets) * (TEST_RATIO + .05) \n",
    "\n",
    "# save to dataframe\n",
    "train = pd.DataFrame({\n",
    "    LABEL: train_labels,\n",
    "    DATA: train_data,\n",
    "    LENGTH: [data_vector_size for _ in range(len(train_data))]\n",
    "})\n",
    "test = pd.DataFrame({\n",
    "    LABEL: test_labels,\n",
    "    DATA: test_data,\n",
    "    LENGTH: [data_vector_size for _ in range(len(test_data))]\n",
    "})\n",
    "print(train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequences:\n",
      " <class 'numpy.ndarray'>: \n",
      "[[1368 7441 2203 4090 1415 5844 1189  420 6205 6522  824 4977  901 3193\n",
      "  4059 7441 1261 2203 6381 1411 1640    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0]\n",
      " [4116 5321 4914  134 3198 4014 3403 2451 5992 8320  758 4914 2236 3685\n",
      "   253 6549 1504 2360 6853 5992  593 7285 6787  814 5526    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0]]\n",
      " <class 'numpy.ndarray'>: \n",
      "[1368 7441 2203 4090 1415 5844 1189  420 6205 6522  824 4977  901 3193 4059\n",
      " 7441 1261 2203 6381 1411 1640    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0]\n",
      " <class 'numpy.int32'>: \n",
      "1368\n",
      "\n",
      "\n",
      "Target values\n",
      " <class 'numpy.ndarray'>: \n",
      "[1 1]\n",
      " <class 'numpy.int64'>: \n",
      "1\n",
      "\n",
      "\n",
      "Sequence lengths\n",
      " <class 'numpy.ndarray'>: \n",
      "[50 50]\n",
      " <class 'numpy.int64'>: \n",
      "50\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "################################################\n",
    "# initializing our tensor\n",
    "#\n",
    "# based off of blogpost david parks showed us:\n",
    "# https://r2rt.com/recurrent-neural-networks-in-tensorflow-iii-variable-length-sequences.html\n",
    "################################################\n",
    "import tensorflow as tf\n",
    " \n",
    "class DataIterator():\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.size = len(self.df)\n",
    "        self.epochs = 0\n",
    "        self.shuffle()\n",
    "\n",
    "    def shuffle(self):\n",
    "        self.df = self.df.sample(frac=1).reset_index(drop=True)\n",
    "        self.cursor = 0\n",
    "\n",
    "    def next_batch(self, n):\n",
    "        if self.cursor + n - 1 > self.size:\n",
    "            self.epochs += 1\n",
    "            self.shuffle()\n",
    "        res = self.df.ix[self.cursor:self.cursor + n - 1]\n",
    "        start_idx = self.cursor\n",
    "        self.cursor += n\n",
    "        # return res[DATA], res[LABEL], res[LENGTH]\n",
    "        # the above line fails.  an error is thrown when tf attempts to call np.asarray on this.\n",
    "        # what is different about how our data is organized compared to the blog post this came from?\n",
    "        # TODO \n",
    "        data = res[DATA]\n",
    "        labels = res[LABEL]\n",
    "        length = res[LENGTH]\n",
    "        return np.asarray([data[i] for i in range(start_idx, start_idx + len(data))]), \\\n",
    "               np.asarray([labels[i] for i in range(start_idx, start_idx + len(labels))]), \\\n",
    "               np.asarray([length[i] for i in range(start_idx, start_idx + len(length))])\n",
    "\n",
    "# validate data iterator\n",
    "d = DataIterator(test).next_batch(2)\n",
    "print('Input sequences:\\n', \n",
    "      \"{}: \\n{}\\n\".format(type(d[0]), d[0]), \n",
    "      \"{}: \\n{}\\n\".format(type(d[0][0]), d[0][0]), \n",
    "      \"{}: \\n{}\\n\".format(type(d[0][0][0]), d[0][0][0]), \n",
    "      end='\\n\\n')\n",
    "print('Target values\\n', \n",
    "      \"{}: \\n{}\\n\".format(type(d[1]), d[1]), \n",
    "      \"{}: \\n{}\\n\".format(type(d[1][0]), d[1][0]), \n",
    "      end='\\n\\n')\n",
    "print('Sequence lengths\\n', \n",
    "      \"{}: \\n{}\\n\".format(type(d[2]), d[2]), \n",
    "      \"{}: \\n{}\\n\".format(type(d[2][0]), d[2][0]), \n",
    "      end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "################################################\n",
    "# initializing our tensor\n",
    "#\n",
    "# based off of blogpost david parks showed us:\n",
    "# https://r2rt.com/recurrent-neural-networks-in-tensorflow-iii-variable-length-sequences.html\n",
    "################################################\n",
    "import tensorflow as tf\n",
    "\n",
    "# this is a global variable we use to graph results.  it should be reset to 'list()' before running\n",
    "PLOTTING_INFO = None\n",
    "\n",
    "def reset_graph():\n",
    "    if 'sess' in globals() and sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "\n",
    "def build_graph(vocab_size = VOCAB_SIZE, state_size = 64, batch_size = 256, num_classes = 2):\n",
    "\n",
    "    reset_graph()\n",
    "\n",
    "    # Placeholders\n",
    "    x = tf.placeholder(tf.int32, [batch_size, None]) # [batch_size, num_steps]\n",
    "    seqlen = tf.placeholder(tf.int32, [batch_size])\n",
    "    y = tf.placeholder(tf.int32, [batch_size])\n",
    "    keep_prob = tf.placeholder_with_default(1.0, [])\n",
    "\n",
    "    # Embedding layer\n",
    "    embeddings = tf.get_variable('embedding_matrix', [vocab_size, state_size])\n",
    "    rnn_inputs = tf.nn.embedding_lookup(embeddings, x)\n",
    "\n",
    "    # RNN\n",
    "    cell = tf.nn.rnn_cell.GRUCell(state_size)\n",
    "    init_state = tf.get_variable('init_state', [1, state_size],\n",
    "                                 initializer=tf.constant_initializer(0.0))\n",
    "    init_state = tf.tile(init_state, [batch_size, 1])\n",
    "    rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, rnn_inputs, sequence_length=seqlen,\n",
    "                                                 initial_state=init_state)\n",
    "\n",
    "    # Add dropout, as the model otherwise quickly overfits\n",
    "    rnn_outputs = tf.nn.dropout(rnn_outputs, keep_prob)\n",
    "    idx = tf.range(batch_size)*tf.shape(rnn_outputs)[1] + (seqlen - 1)\n",
    "    # last_rnn_output = tf.gather(tf.reshape(rnn_outputs, [-1, state_size]), idx)\n",
    "    # last_rnn_output = tf.gather_nd(rnn_outputs, tf.pack([tf.range(batch_size), seqlen-1], axis=1))\n",
    "    last_rnn_output = tf.gather_nd(rnn_outputs, tf.stack([tf.range(batch_size), seqlen-1], axis=1))\n",
    "\n",
    "    # Softmax layer\n",
    "    with tf.variable_scope('softmax'):\n",
    "        W = tf.get_variable('W', [state_size, num_classes]) # weights?\n",
    "        b = tf.get_variable('b', [num_classes], initializer=tf.constant_initializer(0.0)) # bias?\n",
    "    logits = tf.matmul(last_rnn_output, W) + b\n",
    "    preds = tf.nn.softmax(logits)\n",
    "    correct = tf.equal(tf.cast(tf.argmax(preds,1),tf.int32), y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "\n",
    "    return {\n",
    "        'x': x,\n",
    "        'seqlen': seqlen,\n",
    "        'y': y,\n",
    "        'dropout': keep_prob,\n",
    "        'loss': loss,\n",
    "        'ts': train_step,\n",
    "        'preds': preds,\n",
    "        'accuracy': accuracy\n",
    "    }\n",
    "\n",
    "\n",
    "def train_graph(g, batch_size = 256, num_epochs = 10, iterator = DataIterator):\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        tr = iterator(train)\n",
    "        te = iterator(test)\n",
    "\n",
    "        step, accuracy = 0, 0\n",
    "        tr_losses, te_losses = [], []\n",
    "        current_epoch = 0\n",
    "        while current_epoch < num_epochs:\n",
    "            step += 1\n",
    "            batch = tr.next_batch(batch_size)\n",
    "            feed = {g['x']: batch[0], g['y']: batch[1], g['seqlen']: batch[2], g['dropout']: 0.6}\n",
    "            accuracy_, _ = sess.run([g['accuracy'], g['ts']], feed_dict=feed)\n",
    "            accuracy += accuracy_\n",
    "\n",
    "            if tr.epochs > current_epoch:\n",
    "                current_epoch += 1\n",
    "                tr_losses.append(accuracy / step)\n",
    "                step, accuracy = 0, 0\n",
    "\n",
    "                #eval test set\n",
    "                te_epoch = te.epochs\n",
    "                while te.epochs == te_epoch:\n",
    "                    step += 1\n",
    "                    batch = te.next_batch(batch_size)\n",
    "                    feed = {g['x']: batch[0], g['y']: batch[1], g['seqlen']: batch[2]}\n",
    "                    accuracy_ = sess.run([g['accuracy']], feed_dict=feed)[0]\n",
    "                    accuracy += accuracy_\n",
    "\n",
    "                te_losses.append(accuracy / step)\n",
    "                step, accuracy = 0,0\n",
    "                print(\"Accuracy after epoch\", current_epoch, \" - tr:\", tr_losses[-1], \"- te:\", te_losses[-1])\n",
    "\n",
    "    return tr_losses, te_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after epoch 1  - tr: 0.487821691176 - te: 0.513671875\n",
      "Accuracy after epoch 2  - tr: 0.50390625 - te: 0.53515625\n",
      "Accuracy after epoch 3  - tr: 0.4912109375 - te: 0.50390625\n",
      "Accuracy after epoch 4  - tr: 0.507568359375 - te: 0.4921875\n",
      "Accuracy after epoch 5  - tr: 0.512451171875 - te: 0.52734375\n",
      "Accuracy after epoch 6  - tr: 0.57373046875 - te: 0.81640625\n",
      "Accuracy after epoch 7  - tr: 0.66943359375 - te: 0.83203125\n",
      "Accuracy after epoch 8  - tr: 0.87841796875 - te: 0.87109375\n",
      "Accuracy after epoch 9  - tr: 0.9208984375 - te: 0.87890625\n",
      "Accuracy after epoch 10  - tr: 0.928955078125 - te: 0.88671875\n",
      "Accuracy after epoch 11  - tr: 0.947265625 - te: 0.90625\n",
      "Accuracy after epoch 12  - tr: 0.963134765625 - te: 0.9296875\n",
      "Accuracy after epoch 13  - tr: 0.97509765625 - te: 0.90625\n",
      "Accuracy after epoch 14  - tr: 0.980712890625 - te: 0.92578125\n",
      "Accuracy after epoch 15  - tr: 0.986328125 - te: 0.9296875\n",
      "Accuracy after epoch 16  - tr: 0.9892578125 - te: 0.90625\n",
      "Accuracy after epoch 17  - tr: 0.9921875 - te: 0.91796875\n",
      "Accuracy after epoch 18  - tr: 0.992431640625 - te: 0.91796875\n",
      "Accuracy after epoch 19  - tr: 0.992919921875 - te: 0.93359375\n",
      "Accuracy after epoch 20  - tr: 0.99560546875 - te: 0.91796875\n",
      "Accuracy after epoch 21  - tr: 0.994873046875 - te: 0.94140625\n",
      "Accuracy after epoch 22  - tr: 0.995849609375 - te: 0.92578125\n",
      "Accuracy after epoch 23  - tr: 0.995849609375 - te: 0.94921875\n",
      "Accuracy after epoch 24  - tr: 0.996826171875 - te: 0.92578125\n",
      "Accuracy after epoch 25  - tr: 0.997314453125 - te: 0.921875\n",
      "Accuracy after epoch 26  - tr: 0.997314453125 - te: 0.91796875\n",
      "Accuracy after epoch 27  - tr: 0.996337890625 - te: 0.91015625\n",
      "Accuracy after epoch 28  - tr: 0.99755859375 - te: 0.92578125\n",
      "Accuracy after epoch 29  - tr: 0.997314453125 - te: 0.9375\n",
      "Accuracy after epoch 30  - tr: 0.998046875 - te: 0.9140625\n",
      "Accuracy after epoch 31  - tr: 0.998291015625 - te: 0.91015625\n",
      "Accuracy after epoch 32  - tr: 0.99853515625 - te: 0.9140625\n"
     ]
    }
   ],
   "source": [
    "################################################\n",
    "# run it!\n",
    "################################################\n",
    "\n",
    "# this fails, just like us\n",
    "g = build_graph()\n",
    "tr_losses, te_losses = train_graph(g, num_epochs=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEZCAYAAACervI0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n2PtJGAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x18aa7c67668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "################################################\n",
    "# plot it!\n",
    "################################################\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "title = \"Word Enumeration\"\n",
    "title += \" (Bigrams)\" if INCLUDE_BIGRAMS else \" (No Bigrams)\"\n",
    "\n",
    "x_coords = [i+1 for i in range(len(tr_losses))]\n",
    "tr_line, = plt.plot(x_coords, tr_losses, 'r-', label=\"Train\")\n",
    "te_line, = plt.plot(x_coords, te_losses, 'b-', label=\"Test\")\n",
    "plt.axis([0, len(tr_losses), min(min(tr_losses), min(te_losses)) - .05, 1.05])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(title)\n",
    "plt.legend(handles=[tr_line, te_line], loc=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nWhat we tried:\\n\\nglove library - broke even\\n    overfit and data was varied\\nglove \"average tweet value\"\\n    overfit and test data was varied\\nglove ATV with variance \\n    this worked worse than ATV\\ntwitter glove worked worse (but need better tokenizing)\\n\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TODO: we're overfitting, why?\n",
    "\n",
    "#TODO: try something better than averaging the word vec values\n",
    "# maybe we could do three-d arrays?  encode each word and pad the data\n",
    "\n",
    "#TODO: we strip a lot away (ie punctuation, smilies) and lose other\n",
    "# data to glove (#hashtags, @handles). how can we keep this?\n",
    "\n",
    "#TODO: how do we run this on the test dataset?\n",
    "\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "\"\"\"\n",
    "What we tried:\n",
    "\n",
    "glove library - broke even\n",
    "    overfit and data was varied\n",
    "glove \"average tweet value\"\n",
    "    overfit and test data was varied\n",
    "glove ATV with variance \n",
    "    this worked worse than ATV\n",
    "twitter glove worked worse (but need better tokenizing)\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
