{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "################################################\n",
    "# intialize word enumeration\n",
    "################################################\n",
    "\n",
    "# config\n",
    "INCLUDE_BIGRAMS = False\n",
    "\n",
    "# definitions\n",
    "PADDING = 0\n",
    "UNKNOWN_WORD = 1\n",
    "ENUMERATION_BEGIN = 2\n",
    "\n",
    "# coverts this string to: ['converts', 'this', 'string', 'to', 'converts this', 'this string', 'string to']\n",
    "def add_bigrams_to_tweet(tweet):\n",
    "    # collect bigrams\n",
    "    bigrams = list()\n",
    "    prev = None\n",
    "    for word in tweet:\n",
    "        if prev is not None: bigrams.append(prev + \" \" + word)\n",
    "        prev = word\n",
    "    # append bigrams\n",
    "    for bigram in bigrams:\n",
    "        tweet.append(bigram)\n",
    "    # return, but this is the same list we passed in\n",
    "    return tweet\n",
    "\n",
    "\n",
    "def enumerate_words(tweets):\n",
    "    # enumerate all words (bigram or otherwise)\n",
    "    corpus = set()\n",
    "    for tweet in tweets:\n",
    "        for word in tweet:\n",
    "            corpus.add(word)\n",
    "    # save in dictionary\n",
    "    w2i = dict()\n",
    "    i = ENUMERATION_BEGIN\n",
    "    for word in list(corpus):\n",
    "        w2i[word] = i\n",
    "        i += 1\n",
    "    # return dict\n",
    "    return w2i\n",
    "\n",
    "\n",
    "def get_word_vector(w2i, tweet, enforce_length=None):\n",
    "    # prep\n",
    "    vec = list()\n",
    "    i = 0\n",
    "    # convert words to integers\n",
    "    for w in tweet:\n",
    "        if w in w2i: vec.append(w2i[w])\n",
    "        else: vec.append(UNKNOWN_WORD)\n",
    "        i += 1\n",
    "        # stop at enforce_length (if set)\n",
    "        if enforce_length is not None and i + 1 == enforce_length:\n",
    "            break\n",
    "    # pad\n",
    "    if enforce_length is not None:\n",
    "        while i < enforce_length:\n",
    "            vec.append(PADDING)\n",
    "            i += 1   \n",
    "    # fin\n",
    "    return vec\n",
    "        \n",
    "    \n",
    "# from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "# if bigram:\n",
    "#     vectorizer = CountVectorizer(ngram_range=(1, 2), token_pattern=r'\\\\b\\\\w+\\\\b',\n",
    "#                                  min_df=1, decode_error='ignore', stop_words=stop_words,\n",
    "#                                  lowercase=lowercase)\n",
    "#     vectorizer = CountVectorizer(decode_error='ignore', stop_words=stop_words, lowercase=lowercase)\\n\","
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "################################################\n",
    "# file parsing functions\n",
    "################################################\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import string, re\n",
    "import collections\n",
    "import numpy as np\n",
    "import glove\n",
    "from glove.glove_cython import fit_vectors, transform_paragraph\n",
    "\n",
    "# definitions\n",
    "HC=\"HillaryClinton\"\n",
    "DT=\"realDonaldTrump\"\n",
    "NA=\"none\"\n",
    "HANDLES = [HC,DT,NA]\n",
    "HANDLE_MAP = {NA:-1, HC:0, DT:1}\n",
    "\n",
    "# read csv file, return handles and tweets\n",
    "def parse_tweet_csv(file, file_encoding=\"utf8\"):\n",
    "    # init\n",
    "    handles, tweets = [], []\n",
    "    \n",
    "    # read file\n",
    "    linenr = -1\n",
    "    with open(file, encoding=file_encoding) as input:\n",
    "        try:\n",
    "            for line in input:\n",
    "                linenr += 1\n",
    "                if linenr == 0: continue\n",
    "                \n",
    "                # get contents\n",
    "                line = line.split(\",\")\n",
    "                if line[0] in HANDLES: #label and irst line of tweet\n",
    "                    handles.append(line[0])\n",
    "                    tweet = ','.join(line[1:])\n",
    "                    tweets.append(tweet)\n",
    "                else: #second+ line of tweet\n",
    "                    tweet = tweets.pop()\n",
    "                    tweet += ','.join(line)\n",
    "                    tweets.append(tweet)\n",
    "        except Exception as e:\n",
    "            print(\"Exception at line {}: {}\".format(linenr, e))\n",
    "            raise e\n",
    "    \n",
    "    # sanity checks\n",
    "    assert len(handles) == len(tweets)\n",
    "    print(\"Found {} tweets in {} lines\".format(len(tweets), linenr + 1))\n",
    "    \n",
    "    # return data\n",
    "    return handles, tweets\n",
    "\n",
    "\n",
    "##########################################\n",
    "### coverting tweet strings to numbers ###\n",
    "\n",
    "# coverting labels to integers\n",
    "def int_labels(labels):\n",
    "    return list(map(lambda x: HANDLE_MAP[x], labels))\n",
    "\n",
    "#tokenizing\n",
    "_tokenizer = TweetTokenizer()\n",
    "_punctuation = set(string.punctuation)\n",
    "def tokenize(tweet, lowercase=True, strip_urls=True, strip_punctuation=True):\n",
    "    tokens = _tokenizer.tokenize(tweet)\n",
    "    if lowercase: tokens = list(map(lambda x: x.lower(), tokens))\n",
    "    if strip_urls: tokens = list(filter(lambda x: not x.startswith(\"http\"), tokens))\n",
    "    if strip_punctuation: #https://stackoverflow.com/questions/265960/best-way-to-strip-punctuation-from-a-string-in-python\n",
    "        tokens = list(filter(lambda x: x.startswith(u'@') or x.startswith(u'#') or x not in _punctuation and not re.match(u\"[^\\w\\d'\\s$]+\", x), tokens))\n",
    "    return tokens\n",
    "\n",
    "# get all tweets\n",
    "def import_text(tweets, w2i = None):\n",
    "    tokenized_tweets = [tokenize(tweet) for tweet in tweets]\n",
    "    if INCLUDE_BIGRAMS:\n",
    "        for tweet in tokenized_tweets:\n",
    "            add_bigrams_to_tweet(tweet)\n",
    "    if w2i is None:\n",
    "        w2i = enumerate_words(tokenized_tweets)\n",
    "    return [get_word_vector(w2i, tweet, enforce_length=50) for tweet in tokenized_tweets], tokenized_tweets, w2i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4743 tweets in 6251 lines\n",
      "\n",
      "The question in this election: Who can put the plans into action that will make your life better? https://t.co/XreEY9OicG\n",
      "['the', 'question', 'in', 'this', 'election', 'who', 'can', 'put', 'the', 'plans', 'into', 'action', 'that', 'will', 'make', 'your', 'life', 'better']\n",
      "[3609, 3151, 6248, 867, 5497, 1613, 2986, 2312, 3609, 4128, 2066, 4859, 3302, 4653, 1855, 2931, 7437, 4757, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "################################################\n",
    "# get raw test data\n",
    "################################################\n",
    "import random\n",
    "\n",
    "# init\n",
    "TEST_RATIO = 0.1\n",
    "assert TEST_RATIO > 0 and TEST_RATIO < 1\n",
    "\n",
    "# get data\n",
    "text_handles, raw_tweets = parse_tweet_csv(\"train.csv\")\n",
    "handles = int_labels(text_handles)\n",
    "tweets, tokenized_tweets, word_mapping = import_text(raw_tweets)   \n",
    "data_vector_size = len(tweets[0])\n",
    "\n",
    "### validation\n",
    "for i in range(1):\n",
    "    rand_i = random.randint(0, len(raw_tweets))\n",
    "    print()\n",
    "    print(raw_tweets[i].strip())\n",
    "    print(tokenized_tweets[i])\n",
    "    print(tweets[i])\n",
    "    print(handles[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Separated into 4261 train and 482 test (10%)\n",
      "\n",
      "   handle  length                                         tweet_data\n",
      "0       0      50  [3609, 3151, 6248, 867, 5497, 1613, 2986, 2312...\n",
      "1       0      50  [1199, 1557, 2668, 4290, 3297, 1022, 1557, 719...\n",
      "2       1      50  [5928, 265, 7519, 5272, 7774, 2503, 6743, 8155...\n",
      "3       0      50  [4438, 6656, 965, 2513, 5272, 898, 5181, 7987,...\n",
      "4       1      50  [2527, 1358, 2301, 6224, 7987, 1855, 3660, 941...\n"
     ]
    }
   ],
   "source": [
    "################################################\n",
    "# split test data into train and test\n",
    "################################################\n",
    "import pandas as pd\n",
    "\n",
    "LABEL = 'handle'\n",
    "DATA = 'tweet_data'\n",
    "LENGTH = 'length'\n",
    "\n",
    "VOCAB_SIZE = len(word_mapping) + ENUMERATION_BEGIN\n",
    "\n",
    "# split into test and train\n",
    "train_labels, train_data, test_labels, test_data = list(), list(), list(), list()\n",
    "for handle, tweet in zip(handles, tweets):\n",
    "    if random.random() < TEST_RATIO:\n",
    "        test_labels.append(handle)\n",
    "        test_data.append(tweet)\n",
    "    else:\n",
    "        train_labels.append(handle)\n",
    "        train_data.append(tweet)\n",
    "\n",
    "# document and validate\n",
    "print(\"Separated into {} train and {} test ({}%)\\n\".format(len(train_data), len(test_data), \n",
    "                                                         int(100.0 * len(test_data) / len(raw_tweets))))\n",
    "assert len(train_labels) == len(train_data) and len(train_data) > 0\n",
    "assert len(test_labels) == len(test_data) and len(test_data) > 0\n",
    "assert len(test_labels) > len(tweets) * (TEST_RATIO - .05)\n",
    "assert len(test_labels) < len(tweets) * (TEST_RATIO + .05) \n",
    "\n",
    "# save to dataframe\n",
    "train = pd.DataFrame({\n",
    "    LABEL: train_labels,\n",
    "    DATA: train_data,\n",
    "    LENGTH: [data_vector_size for _ in range(len(train_data))]\n",
    "})\n",
    "test = pd.DataFrame({\n",
    "    LABEL: test_labels,\n",
    "    DATA: test_data,\n",
    "    LENGTH: [data_vector_size for _ in range(len(test_data))]\n",
    "})\n",
    "print(train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1701 tweets in 2198 lines\n"
     ]
    }
   ],
   "source": [
    "################################################\n",
    "# get the test data\n",
    "################################################\n",
    "\n",
    "\n",
    "output_text_handles, output_raw_tweets = parse_tweet_csv(\"test.csv\")\n",
    "\n",
    "output_handles = int_labels(output_text_handles)\n",
    "output_tweets, output_tokenized_tweets, _ = import_text(output_raw_tweets, word_mapping)   \n",
    "assert len(output_tweets[0]) == data_vector_size\n",
    "\n",
    "# save to dataframe\n",
    "output_dataframe = pd.DataFrame({\n",
    "    LABEL: output_handles,\n",
    "    DATA: output_tweets,\n",
    "    LENGTH: [data_vector_size for _ in range(len(output_tweets))]\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequences:\n",
      " <class 'numpy.ndarray'>: \n",
      "[[7640 6656 7646 1868 7572 1682 5535 7694 6527  824 6077 3965 7519  131\n",
      "  7250  831   12 1868 7572 1682 2986 4148 6005 4432  131 4290 5825    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0]\n",
      " [1056 5872 4373 2213  958  763 5319 5272 3185 3689 1801    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0]]\n",
      " <class 'numpy.ndarray'>: \n",
      "[7640 6656 7646 1868 7572 1682 5535 7694 6527  824 6077 3965 7519  131 7250\n",
      "  831   12 1868 7572 1682 2986 4148 6005 4432  131 4290 5825    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0]\n",
      " <class 'numpy.int32'>: \n",
      "7640\n",
      "\n",
      "\n",
      "Target values\n",
      " <class 'numpy.ndarray'>: \n",
      "[0 1]\n",
      " <class 'numpy.int64'>: \n",
      "0\n",
      "\n",
      "\n",
      "Sequence lengths\n",
      " <class 'numpy.ndarray'>: \n",
      "[50 50]\n",
      " <class 'numpy.int64'>: \n",
      "50\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "################################################\n",
    "# initializing our tensor\n",
    "#\n",
    "# based off of blogpost david parks showed us:\n",
    "# https://r2rt.com/recurrent-neural-networks-in-tensorflow-iii-variable-length-sequences.html\n",
    "################################################\n",
    "import tensorflow as tf\n",
    " \n",
    "class DataIterator():\n",
    "    def __init__(self, df, do_shuffle = True):\n",
    "        self.df = df\n",
    "        self.size = len(self.df)\n",
    "        self.epochs = 0\n",
    "        self.cursor = 0\n",
    "        self.do_shuffle = do_shuffle\n",
    "        if self.do_shuffle: self.shuffle()\n",
    "\n",
    "    def shuffle(self):\n",
    "        self.df = self.df.sample(frac=1).reset_index(drop=True)\n",
    "        self.cursor = 0\n",
    "\n",
    "    def next_batch(self, n):\n",
    "        if self.cursor + n - 1 > self.size:\n",
    "            self.epochs += 1\n",
    "            if self.do_shuffle: self.shuffle()\n",
    "        res = self.df.ix[self.cursor:self.cursor + n - 1] \n",
    "        if len(res) < n:\n",
    "            i = self.size\n",
    "            print(\"{}: last block has {} elements\".format(self.epochs, len(res)))\n",
    "            while len(res) < n:\n",
    "#                 res[i] = [None, data_vector_size, [PADDING for _ in range (50)]]\n",
    "#                 res[i] = {LABEL: [None], DATA: [[PADDING for _ in range (50)]], \n",
    "#                                   LENGTH: [data_vector_size]}\n",
    "                res = res.append(pd.DataFrame({LABEL: [None], DATA: [[PADDING for _ in range (50)]], \n",
    "                                  LENGTH: [data_vector_size]}, index=[i]))\n",
    "                i += 1\n",
    "            print(res)\n",
    "            \n",
    "        start_idx = self.cursor\n",
    "        self.cursor += n\n",
    "        # return res[DATA], res[LABEL], res[LENGTH]\n",
    "        # the above line fails.  an error is thrown when tf attempts to call np.asarray on this.\n",
    "        # what is different about how our data is organized compared to the blog post this came from?\n",
    "        # TODO \n",
    "        data = res[DATA]\n",
    "        labels = res[LABEL]\n",
    "        length = res[LENGTH]\n",
    "        return np.asarray([data[i] for i in range(start_idx, start_idx + len(data))]), \\\n",
    "               np.asarray([labels[i] for i in range(start_idx, start_idx + len(labels))]), \\\n",
    "               np.asarray([length[i] for i in range(start_idx, start_idx + len(length))])\n",
    "\n",
    "# validate data iterator\n",
    "d = DataIterator(test).next_batch(2)\n",
    "print('Input sequences:\\n', \n",
    "      \"{}: \\n{}\\n\".format(type(d[0]), d[0]), \n",
    "      \"{}: \\n{}\\n\".format(type(d[0][0]), d[0][0]), \n",
    "      \"{}: \\n{}\\n\".format(type(d[0][0][0]), d[0][0][0]), \n",
    "      end='\\n\\n')\n",
    "print('Target values\\n', \n",
    "      \"{}: \\n{}\\n\".format(type(d[1]), d[1]), \n",
    "      \"{}: \\n{}\\n\".format(type(d[1][0]), d[1][0]), \n",
    "      end='\\n\\n')\n",
    "print('Sequence lengths\\n', \n",
    "      \"{}: \\n{}\\n\".format(type(d[2]), d[2]), \n",
    "      \"{}: \\n{}\\n\".format(type(d[2][0]), d[2][0]), \n",
    "      end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "################################################\n",
    "# initializing our tensor\n",
    "#\n",
    "# based off of blogpost david parks showed us:\n",
    "# https://r2rt.com/recurrent-neural-networks-in-tensorflow-iii-variable-length-sequences.html\n",
    "################################################\n",
    "import tensorflow as tf\n",
    "\n",
    "# this is a global variable we use to graph results.  it should be reset to 'list()' before running\n",
    "PLOTTING_INFO = None\n",
    "\n",
    "def reset_graph():\n",
    "    if 'sess' in globals() and sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "\n",
    "def build_graph(vocab_size = VOCAB_SIZE, state_size = 64, batch_size = 256, num_classes = 2):\n",
    "\n",
    "    reset_graph()\n",
    "\n",
    "    # Placeholders\n",
    "    x = tf.placeholder(tf.int32, [batch_size, None]) # [batch_size, num_steps]\n",
    "    seqlen = tf.placeholder(tf.int32, [batch_size])\n",
    "    y = tf.placeholder(tf.int32, [batch_size])\n",
    "#     x = tf.placeholder(tf.int32, [None, None]) # [batch_size, num_steps]\n",
    "#     seqlen = tf.placeholder(tf.int32, [None])\n",
    "#     y = tf.placeholder(tf.int32, [None])\n",
    "    keep_prob = tf.placeholder_with_default(1.0, [])\n",
    "\n",
    "    # Embedding layer\n",
    "    embeddings = tf.get_variable('embedding_matrix', [vocab_size, state_size])\n",
    "    rnn_inputs = tf.nn.embedding_lookup(embeddings, x)\n",
    "\n",
    "    # RNN\n",
    "    cell = tf.nn.rnn_cell.GRUCell(state_size)\n",
    "    init_state = tf.get_variable('init_state', [1, state_size],\n",
    "                                 initializer=tf.constant_initializer(0.0))\n",
    "    init_state = tf.tile(init_state, [batch_size, 1])\n",
    "    rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, rnn_inputs, sequence_length=seqlen,\n",
    "                                                 initial_state=init_state)\n",
    "\n",
    "    # Add dropout, as the model otherwise quickly overfits\n",
    "    rnn_outputs = tf.nn.dropout(rnn_outputs, keep_prob)\n",
    "    idx = tf.range(batch_size)*tf.shape(rnn_outputs)[1] + (seqlen - 1)\n",
    "    # last_rnn_output = tf.gather(tf.reshape(rnn_outputs, [-1, state_size]), idx)\n",
    "    # last_rnn_output = tf.gather_nd(rnn_outputs, tf.pack([tf.range(batch_size), seqlen-1], axis=1))\n",
    "    last_rnn_output = tf.gather_nd(rnn_outputs, tf.stack([tf.range(batch_size), seqlen-1], axis=1))\n",
    "\n",
    "    # Softmax layer\n",
    "    with tf.variable_scope('softmax'):\n",
    "        W = tf.get_variable('W', [state_size, num_classes]) # weights?\n",
    "        b = tf.get_variable('b', [num_classes], initializer=tf.constant_initializer(0.0)) # bias?\n",
    "    logits = tf.matmul(last_rnn_output, W) + b\n",
    "    preds = tf.nn.softmax(logits)\n",
    "    correct = tf.equal(tf.cast(tf.argmax(preds,1),tf.int32), y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "\n",
    "    return {\n",
    "        'x': x,\n",
    "        'seqlen': seqlen,\n",
    "        'y': y,\n",
    "        'dropout': keep_prob,\n",
    "        'loss': loss,\n",
    "        'ts': train_step,\n",
    "        'preds': preds,\n",
    "        'accuracy': accuracy\n",
    "    }\n",
    "\n",
    "\n",
    "def train_graph(g, batch_size = 256, num_epochs = 10, iterator = DataIterator, output_data=None):\n",
    "    \n",
    "    predictions = None\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        tr = iterator(train)\n",
    "        te = iterator(test)\n",
    "\n",
    "        step, accuracy = 0, 0\n",
    "        tr_losses, te_losses = [], []\n",
    "        current_epoch = 0\n",
    "        while current_epoch < num_epochs:\n",
    "            step += 1\n",
    "            batch = tr.next_batch(batch_size)\n",
    "            feed = {g['x']: batch[0], g['y']: batch[1], g['seqlen']: batch[2], g['dropout']: 0.6}\n",
    "            accuracy_, _ = sess.run([g['accuracy'], g['ts']], feed_dict=feed)\n",
    "            accuracy += accuracy_\n",
    "\n",
    "            if tr.epochs > current_epoch:\n",
    "                current_epoch += 1\n",
    "                tr_losses.append(accuracy / step)\n",
    "                step, accuracy = 0, 0\n",
    "\n",
    "                #eval test set\n",
    "                te_epoch = te.epochs\n",
    "                while te.epochs == te_epoch:\n",
    "                    step += 1\n",
    "                    batch = te.next_batch(batch_size)\n",
    "                    feed = {g['x']: batch[0], g['y']: batch[1], g['seqlen']: batch[2]}\n",
    "                    accuracy_ = sess.run([g['accuracy']], feed_dict=feed)[0]\n",
    "                    accuracy += accuracy_\n",
    "\n",
    "                te_losses.append(accuracy / step)\n",
    "                step, accuracy = 0,0\n",
    "                print(\"Accuracy after epoch\", current_epoch, \" - tr:\", tr_losses[-1], \"- te:\", te_losses[-1])\n",
    "                \n",
    "        if output_data is not None:\n",
    "            print(\"Predicting on {} output elements ({} batches)\".format(len(output_data), 1 + int(len(output_data) / 256)))\n",
    "            data = iterator(output_data, do_shuffle=False)\n",
    "            predictions = list()\n",
    "            while data.epochs == 0:\n",
    "                print(\"\\tbatch \", len(predictions))\n",
    "                batch = data.next_batch(batch_size)\n",
    "                feed = {g['x']: batch[0], g['seqlen']: batch[2]}\n",
    "                pred_batch = sess.run([g['preds']], feed_dict=feed)[0]\n",
    "                predictions.append(pred_batch)\n",
    "            print(\"Made {} predictions\".format(len(predictions)))\n",
    "            \n",
    "                \n",
    "\n",
    "    return tr_losses, te_losses, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after epoch 1  - tr: 0.504595588235 - te: 0.4765625\n",
      "Predicting on 1701 output elements (7 batches)\n",
      "\tbatch  0\n",
      "\tbatch  1\n",
      "\tbatch  2\n",
      "\tbatch  3\n",
      "\tbatch  4\n",
      "\tbatch  5\n",
      "\tbatch  6\n",
      "1: last block has 165 elements\n",
      "     handle  length                                         tweet_data\n",
      "1536     -1      50  [6038, 6960, 482, 1894, 6310, 4460, 332, 5272,...\n",
      "1537     -1      50  [3711, 3302, 1614, 3591, 6869, 5235, 7519, 603...\n",
      "1538     -1      50  [6038, 3591, 7253, 1808, 3664, 6002, 7898, 1, ...\n",
      "1539     -1      50  [5993, 7519, 554, 5677, 5359, 4148, 2312, 1353...\n",
      "1540     -1      50  [8036, 3468, 3602, 2329, 2564, 1093, 1348, 665...\n",
      "1541     -1      50  [1, 1801, 3027, 1, 3609, 6661, 2338, 3027, 0, ...\n",
      "1542     -1      50  [3948, 6361, 2185, 265, 5549, 6661, 2956, 1093...\n",
      "1543     -1      50  [7039, 968, 7987, 4148, 3301, 4432, 3609, 2179...\n",
      "1544     -1      50  [2338, 3027, 1209, 573, 1557, 4653, 1855, 7640...\n",
      "1545     -1      50  [1, 1801, 1329, 3106, 339, 965, 608, 6507, 671...\n",
      "1546     -1      50  [1, 6450, 3883, 4432, 5656, 1, 3814, 332, 5847...\n",
      "1547     -1      50  [6883, 7565, 228, 63, 562, 7250, 1329, 528, 80...\n",
      "1548     -1      50  [1456, 764, 106, 7492, 899, 5935, 6025, 5687, ...\n",
      "1549     -1      50  [3547, 1711, 8315, 2586, 6248, 562, 4837, 3185...\n",
      "1550     -1      50  [3769, 2986, 1808, 3664, 4148, 528, 3836, 341,...\n",
      "1551     -1      50  [2338, 3027, 4947, 317, 4294, 1557, 4653, 1851...\n",
      "1552     -1      50  [3664, 8133, 8255, 6248, 3970, 336, 1, 332, 63...\n",
      "1553     -1      50  [1557, 6002, 5281, 603, 63, 3750, 4734, 3302, ...\n",
      "1554     -1      50  [6038, 675, 2778, 6038, 824, 3044, 4254, 7253,...\n",
      "1555     -1      50  [5932, 7250, 1329, 3609, 7321, 78, 5503, 3609,...\n",
      "1556     -1      50  [7829, 6002, 5487, 1, 7519, 3609, 5650, 5687, ...\n",
      "1557     -1      50  [2338, 3027, 742, 1209, 573, 1851, 8367, 0, 0,...\n",
      "1558     -1      50  [912, 5272, 2458, 6599, 6248, 819, 7732, 8053,...\n",
      "1559     -1      50  [1557, 4653, 4307, 4925, 7253, 8337, 2877, 725...\n",
      "1560     -1      50  [6766, 6869, 6272, 2956, 5160, 1665, 5487, 663...\n",
      "1561     -1      50  [2338, 3027, 7519, 5272, 2458, 2074, 8155, 360...\n",
      "1562     -1      50  [1855, 7640, 2458, 8377, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "1563     -1      50  [3609, 6361, 6002, 3711, 1223, 3302, 411, 7805...\n",
      "1564     -1      50  [1, 6656, 4885, 6769, 7519, 3162, 5503, 6191, ...\n",
      "1565     -1      50  [1557, 6002, 4941, 7987, 2185, 5272, 2956, 761...\n",
      "...     ...     ...                                                ...\n",
      "1762   None      50  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "1763   None      50  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "1764   None      50  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "1765   None      50  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "1766   None      50  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "1767   None      50  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "1768   None      50  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "1769   None      50  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "1770   None      50  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "1771   None      50  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "1772   None      50  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "1773   None      50  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "1774   None      50  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "1775   None      50  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "1776   None      50  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "1777   None      50  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "1778   None      50  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "1779   None      50  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "1780   None      50  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "1781   None      50  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "1782   None      50  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "1783   None      50  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "1784   None      50  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "1785   None      50  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "1786   None      50  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "1787   None      50  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "1788   None      50  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "1789   None      50  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "1790   None      50  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "1791   None      50  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "\n",
      "[256 rows x 3 columns]\n",
      "Made 7 predictions\n",
      "[[ 0.49144754  0.50855249]\n",
      " [ 0.49155942  0.50844055]\n",
      " [ 0.49154475  0.50845528]\n",
      " [ 0.4915598   0.5084402 ]\n",
      " [ 0.49153903  0.50846094]\n",
      " [ 0.49155459  0.50844544]\n",
      " [ 0.49152488  0.50847507]\n",
      " [ 0.49148026  0.50851971]\n",
      " [ 0.49154988  0.50845009]\n",
      " [ 0.4915567   0.5084433 ]\n",
      " [ 0.49156392  0.50843602]\n",
      " [ 0.49152273  0.50847727]\n",
      " [ 0.49155182  0.50844818]\n",
      " [ 0.49152666  0.50847328]\n",
      " [ 0.49155918  0.50844085]\n",
      " [ 0.49155241  0.50844765]\n",
      " [ 0.49156445  0.50843555]\n",
      " [ 0.49155325  0.50844669]\n",
      " [ 0.49156198  0.50843793]\n",
      " [ 0.49155867  0.50844127]\n",
      " [ 0.49156243  0.50843751]\n",
      " [ 0.49157032  0.50842965]\n",
      " [ 0.49155214  0.50844789]\n",
      " [ 0.49152622  0.50847381]\n",
      " [ 0.49158704  0.50841302]\n",
      " [ 0.49158037  0.50841957]\n",
      " [ 0.49156544  0.50843459]\n",
      " [ 0.49156278  0.50843716]\n",
      " [ 0.49148026  0.50851971]\n",
      " [ 0.49152473  0.5084753 ]\n",
      " [ 0.49156031  0.50843966]\n",
      " [ 0.49156997  0.50843   ]\n",
      " [ 0.49153775  0.50846231]\n",
      " [ 0.49144179  0.50855815]\n",
      " [ 0.49156171  0.50843829]\n",
      " [ 0.49154747  0.50845253]\n",
      " [ 0.49155688  0.50844306]\n",
      " [ 0.4915427   0.50845736]\n",
      " [ 0.49155733  0.5084427 ]\n",
      " [ 0.49156141  0.50843865]\n",
      " [ 0.4915365   0.5084635 ]\n",
      " [ 0.49156219  0.50843775]\n",
      " [ 0.49152407  0.5084759 ]\n",
      " [ 0.49151614  0.50848389]\n",
      " [ 0.49155509  0.50844491]\n",
      " [ 0.49156737  0.50843263]\n",
      " [ 0.49155873  0.50844127]\n",
      " [ 0.49155289  0.50844711]\n",
      " [ 0.49151671  0.50848329]\n",
      " [ 0.49156502  0.50843495]\n",
      " [ 0.49156401  0.50843602]\n",
      " [ 0.49155834  0.50844157]\n",
      " [ 0.49156368  0.50843638]\n",
      " [ 0.49155614  0.50844389]\n",
      " [ 0.4915581   0.50844193]\n",
      " [ 0.49146479  0.50853515]\n",
      " [ 0.49151877  0.5084812 ]\n",
      " [ 0.49157652  0.50842345]\n",
      " [ 0.49155885  0.50844109]\n",
      " [ 0.49147865  0.50852138]\n",
      " [ 0.49155572  0.50844425]\n",
      " [ 0.49156466  0.50843537]\n",
      " [ 0.49156135  0.50843865]\n",
      " [ 0.49156359  0.5084365 ]\n",
      " [ 0.49156278  0.50843722]\n",
      " [ 0.49154058  0.50845945]\n",
      " [ 0.49155325  0.50844669]\n",
      " [ 0.491566    0.50843394]\n",
      " [ 0.49156779  0.50843215]\n",
      " [ 0.49156085  0.50843918]\n",
      " [ 0.49155894  0.50844103]\n",
      " [ 0.49155548  0.50844449]\n",
      " [ 0.4915618   0.50843811]\n",
      " [ 0.49153662  0.50846332]\n",
      " [ 0.49153942  0.50846052]\n",
      " [ 0.49155775  0.50844228]\n",
      " [ 0.4914521   0.50854784]\n",
      " [ 0.49152756  0.50847244]\n",
      " [ 0.49153218  0.50846779]\n",
      " [ 0.49153367  0.50846636]\n",
      " [ 0.49154368  0.50845641]\n",
      " [ 0.49154341  0.50845659]\n",
      " [ 0.49155316  0.50844687]\n",
      " [ 0.49156076  0.50843924]\n",
      " [ 0.49154398  0.50845599]\n",
      " [ 0.49152035  0.5084796 ]\n",
      " [ 0.49153519  0.50846481]\n",
      " [ 0.49147558  0.50852436]\n",
      " [ 0.49150181  0.50849825]\n",
      " [ 0.4915528   0.50844729]\n",
      " [ 0.49154523  0.5084548 ]\n",
      " [ 0.49151313  0.50848693]\n",
      " [ 0.49155769  0.50844228]\n",
      " [ 0.49155557  0.50844437]\n",
      " [ 0.49155685  0.50844318]\n",
      " [ 0.49156407  0.50843596]\n",
      " [ 0.49155831  0.50844169]\n",
      " [ 0.49156284  0.5084371 ]\n",
      " [ 0.49152964  0.5084703 ]\n",
      " [ 0.49153122  0.50846875]\n",
      " [ 0.49155387  0.50844616]\n",
      " [ 0.49156222  0.50843781]\n",
      " [ 0.49146399  0.50853604]\n",
      " [ 0.49155554  0.50844443]\n",
      " [ 0.49146718  0.50853288]\n",
      " [ 0.49152774  0.5084722 ]\n",
      " [ 0.49158967  0.50841033]\n",
      " [ 0.49149939  0.50850064]\n",
      " [ 0.49155515  0.50844485]\n",
      " [ 0.49151573  0.50848424]\n",
      " [ 0.49155658  0.50844342]\n",
      " [ 0.49152565  0.50847441]\n",
      " [ 0.49155322  0.50844675]\n",
      " [ 0.49155518  0.50844485]\n",
      " [ 0.49151981  0.50848019]\n",
      " [ 0.49150723  0.50849277]\n",
      " [ 0.49156722  0.50843287]\n",
      " [ 0.49154237  0.5084576 ]\n",
      " [ 0.49156877  0.50843126]\n",
      " [ 0.4915114   0.50848854]\n",
      " [ 0.49155197  0.50844806]\n",
      " [ 0.49155283  0.50844723]\n",
      " [ 0.49152264  0.50847739]\n",
      " [ 0.49146828  0.50853163]\n",
      " [ 0.49155679  0.50844318]\n",
      " [ 0.4914602   0.50853974]\n",
      " [ 0.49154681  0.50845325]\n",
      " [ 0.49155915  0.50844079]\n",
      " [ 0.49153543  0.50846457]\n",
      " [ 0.49157363  0.50842637]\n",
      " [ 0.49154818  0.50845188]\n",
      " [ 0.49156192  0.50843805]\n",
      " [ 0.49143958  0.50856036]\n",
      " [ 0.4914951   0.50850499]\n",
      " [ 0.49155757  0.50844246]\n",
      " [ 0.49155006  0.50844997]\n",
      " [ 0.49156833  0.50843167]\n",
      " [ 0.49151519  0.50848478]\n",
      " [ 0.49159023  0.50840974]\n",
      " [ 0.49153098  0.50846905]\n",
      " [ 0.49151215  0.50848788]\n",
      " [ 0.49156168  0.50843835]\n",
      " [ 0.49154755  0.50845242]\n",
      " [ 0.49147305  0.50852692]\n",
      " [ 0.49149746  0.50850254]\n",
      " [ 0.49154827  0.50845176]\n",
      " [ 0.49120653  0.50879347]\n",
      " [ 0.49155447  0.50844562]\n",
      " [ 0.49152714  0.50847286]\n",
      " [ 0.49153671  0.50846326]\n",
      " [ 0.49152258  0.50847745]\n",
      " [ 0.49151126  0.50848877]\n",
      " [ 0.49146745  0.50853252]\n",
      " [ 0.49155632  0.50844377]\n",
      " [ 0.49155021  0.50844979]\n",
      " [ 0.49155903  0.50844103]\n",
      " [ 0.49156624  0.50843376]\n",
      " [ 0.49157479  0.50842518]\n",
      " [ 0.49153078  0.50846922]\n",
      " [ 0.49155125  0.50844872]\n",
      " [ 0.49150753  0.50849253]\n",
      " [ 0.4915275   0.5084725 ]\n",
      " [ 0.49149239  0.50850761]\n",
      " [ 0.49156633  0.50843364]\n",
      " [ 0.49155265  0.50844741]\n",
      " [ 0.49149773  0.50850224]\n",
      " [ 0.4915213   0.50847864]\n",
      " [ 0.49151689  0.50848311]\n",
      " [ 0.49148843  0.50851154]\n",
      " [ 0.49157244  0.50842756]\n",
      " [ 0.49155489  0.50844508]\n",
      " [ 0.49156529  0.50843477]\n",
      " [ 0.49149165  0.50850827]\n",
      " [ 0.49155462  0.50844538]\n",
      " [ 0.49156252  0.50843751]\n",
      " [ 0.49156487  0.50843519]\n",
      " [ 0.49155587  0.50844419]\n",
      " [ 0.49156663  0.5084334 ]\n",
      " [ 0.49150047  0.50849944]\n",
      " [ 0.49152154  0.50847852]\n",
      " [ 0.49154857  0.50845146]\n",
      " [ 0.4915632   0.50843686]\n",
      " [ 0.49153158  0.50846839]\n",
      " [ 0.49155784  0.50844222]\n",
      " [ 0.49155822  0.50844181]\n",
      " [ 0.49154693  0.50845307]\n",
      " [ 0.4915553   0.50844467]\n",
      " [ 0.49154884  0.50845122]\n",
      " [ 0.49155721  0.50844282]\n",
      " [ 0.49152747  0.50847256]\n",
      " [ 0.49147314  0.50852686]\n",
      " [ 0.49144989  0.50855011]\n",
      " [ 0.49156478  0.50843525]\n",
      " [ 0.49153447  0.50846553]\n",
      " [ 0.49157923  0.50842077]\n",
      " [ 0.4915947   0.50840527]\n",
      " [ 0.49150226  0.50849777]\n",
      " [ 0.49154803  0.50845194]\n",
      " [ 0.49153772  0.50846231]\n",
      " [ 0.49151376  0.50848627]\n",
      " [ 0.49155653  0.50844347]\n",
      " [ 0.49155685  0.50844312]\n",
      " [ 0.4915584   0.50844157]\n",
      " [ 0.49149644  0.50850362]\n",
      " [ 0.49148339  0.50851667]\n",
      " [ 0.4915565   0.50844342]\n",
      " [ 0.49156436  0.50843567]\n",
      " [ 0.49155053  0.50844944]\n",
      " [ 0.49155858  0.50844145]\n",
      " [ 0.49156427  0.50843567]\n",
      " [ 0.49155498  0.50844502]\n",
      " [ 0.49155203  0.508448  ]\n",
      " [ 0.49154952  0.50845045]\n",
      " [ 0.49152988  0.50847012]\n",
      " [ 0.49155623  0.50844377]\n",
      " [ 0.49155885  0.50844115]\n",
      " [ 0.4915196   0.50848043]\n",
      " [ 0.49154437  0.50845557]\n",
      " [ 0.49153182  0.50846821]\n",
      " [ 0.49155554  0.50844443]\n",
      " [ 0.4915559   0.50844407]\n",
      " [ 0.49154893  0.50845104]\n",
      " [ 0.49154738  0.50845265]\n",
      " [ 0.49153262  0.50846744]\n",
      " [ 0.49138013  0.5086199 ]\n",
      " [ 0.49153307  0.5084669 ]\n",
      " [ 0.49144909  0.50855088]\n",
      " [ 0.49156544  0.50843459]\n",
      " [ 0.4915036   0.50849646]\n",
      " [ 0.49157798  0.50842202]\n",
      " [ 0.4915067   0.50849336]\n",
      " [ 0.49157226  0.5084278 ]\n",
      " [ 0.49153826  0.50846177]\n",
      " [ 0.49156415  0.50843585]\n",
      " [ 0.49155608  0.50844389]\n",
      " [ 0.49156827  0.50843173]\n",
      " [ 0.49148306  0.50851691]\n",
      " [ 0.49155301  0.50844699]\n",
      " [ 0.49154347  0.50845659]\n",
      " [ 0.49152175  0.50847828]\n",
      " [ 0.49155855  0.50844145]\n",
      " [ 0.49155128  0.50844866]\n",
      " [ 0.49160072  0.50839925]\n",
      " [ 0.49153394  0.50846606]\n",
      " [ 0.49155703  0.508443  ]\n",
      " [ 0.4915491   0.50845093]\n",
      " [ 0.49156719  0.50843281]\n",
      " [ 0.4915407   0.50845927]\n",
      " [ 0.49156123  0.50843883]\n",
      " [ 0.49155822  0.50844187]\n",
      " [ 0.49155867  0.50844127]\n",
      " [ 0.49155992  0.50844014]\n",
      " [ 0.49155366  0.50844634]\n",
      " [ 0.49157146  0.50842851]\n",
      " [ 0.49155855  0.50844139]\n",
      " [ 0.49152359  0.50847638]]\n",
      "7\n",
      "256\n"
     ]
    }
   ],
   "source": [
    "################################################\n",
    "# run it!\n",
    "################################################\n",
    "\n",
    "# this fails, just like us\n",
    "g = build_graph()\n",
    "\n",
    "tr_losses, te_losses, predictions = train_graph(g, num_epochs=1, output_data=output_dataframe)\n",
    "print(predictions[0])\n",
    "print(len(predictions))\n",
    "print(len(predictions[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "################################################\n",
    "# plot it!\n",
    "################################################\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "title = \"Word Enumeration\"\n",
    "title += \" (Bigrams)\" if INCLUDE_BIGRAMS else \" (No Bigrams)\"\n",
    "\n",
    "x_coords = [i+1 for i in range(len(tr_losses))]\n",
    "tr_line, = plt.plot(x_coords, tr_losses, 'r-', label=\"Train\")\n",
    "te_line, = plt.plot(x_coords, te_losses, 'b-', label=\"Test\")\n",
    "plt.axis([0, len(tr_losses), min(min(tr_losses), min(te_losses)) - .05, 1.05])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(title)\n",
    "plt.legend(handles=[tr_line, te_line], loc=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################\n",
    "# save it\n",
    "################################################\n",
    "\n",
    "import csv\n",
    "\n",
    "with open(output_csv, 'w+') as csv_file:\n",
    "            spamwriter = csv.writer(csv_file, delimiter=',')  # ,quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "            spamwriter.writerow([\"id\", \"realDonaldTrump\", \"HillaryClinton\"])\n",
    "            spamwriter.writerows([[i, x[trump_index], x[clinton_index]] for i, x in enumerate(all_data_list)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#TODO: we're overfitting, why?\n",
    "\n",
    "#TODO: try something better than averaging the word vec values\n",
    "# maybe we could do three-d arrays?  encode each word and pad the data\n",
    "\n",
    "#TODO: we strip a lot away (ie punctuation, smilies) and lose other\n",
    "# data to glove (#hashtags, @handles). how can we keep this?\n",
    "\n",
    "#TODO: how do we run this on the test dataset?\n",
    "\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "\"\"\"\n",
    "What we tried:\n",
    "\n",
    "glove library - broke even\n",
    "    overfit and data was varied\n",
    "glove \"average tweet value\"\n",
    "    overfit and test data was varied\n",
    "glove ATV with variance \n",
    "    this worked worse than ATV\n",
    "twitter glove worked worse (but need better tokenizing)\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
