{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "################################################\n",
    "# intialize glove\n",
    "################################################\n",
    "\n",
    "PADDING = 0\n",
    "UNKNOWN_WORD = 1\n",
    "ENUMERATION_BEGIN = 2\n",
    "\n",
    "# coverts this string to: ['converts', 'this', 'string', 'to', 'converts this', 'this string', 'string to']\n",
    "def add_bigrams_to_tweet(tweet):\n",
    "    # collect bigrams\n",
    "    bigrams = list()\n",
    "    prev = None\n",
    "    for word in tweet:\n",
    "        if prev is not None: bigrams.append(prev + \" \" + word)\n",
    "        prev = word\n",
    "    # append bigrams\n",
    "    for bigram in bigrams:\n",
    "        tweet.append(bigram)\n",
    "    # return, but this is the same list we passed in\n",
    "    return tweet\n",
    "\n",
    "\n",
    "def enumerate_words(tweets):\n",
    "    # enumerate all words (bigram or otherwise)\n",
    "    corpus = set()\n",
    "    for tweet in tweets:\n",
    "        for word in tweet:\n",
    "            corpus.add(word)\n",
    "    # save in dictionary\n",
    "    w2i = dict()\n",
    "    i = ENUMERATION_BEGIN\n",
    "    for word in list(corpus):\n",
    "        w2i[word] = i\n",
    "        i += 1\n",
    "    # return dict\n",
    "    return w2i\n",
    "\n",
    "\n",
    "def get_word_vector(w2i, tweet, enforce_length=None):\n",
    "    # prep\n",
    "    vec = list()\n",
    "    i = 0\n",
    "    # convert words to integers\n",
    "    for w in tweet:\n",
    "        if w in w2i: vec.append(w2i[w])\n",
    "        else: vec.append(UNKNOWN_WORD)\n",
    "        i += 1\n",
    "        # stop at enforce_length (if set)\n",
    "        if enforce_length is not None and i + 1 == enforce_length:\n",
    "            break\n",
    "    # pad\n",
    "    if enforce_length is not None:\n",
    "        while i < enforce_length:\n",
    "            vec.append(PADDING)\n",
    "            i += 1   \n",
    "    # fin\n",
    "    return vec\n",
    "        \n",
    "    \n",
    "# from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "# if bigram:\n",
    "#     vectorizer = CountVectorizer(ngram_range=(1, 2), token_pattern=r'\\\\b\\\\w+\\\\b',\n",
    "#                                  min_df=1, decode_error='ignore', stop_words=stop_words,\n",
    "#                                  lowercase=lowercase)\n",
    "#     vectorizer = CountVectorizer(decode_error='ignore', stop_words=stop_words, lowercase=lowercase)\\n\","
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "################################################\n",
    "# file parsing functions\n",
    "################################################\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import string, re\n",
    "import collections\n",
    "import numpy as np\n",
    "import glove\n",
    "from glove.glove_cython import fit_vectors, transform_paragraph\n",
    "\n",
    "# definitions\n",
    "HC=\"HillaryClinton\"\n",
    "DT=\"realDonaldTrump\"\n",
    "NA=\"none\"\n",
    "HANDLES = [HC,DT,NA]\n",
    "HANDLE_MAP = {NA:-1, HC:0, DT:1}\n",
    "\n",
    "# read csv file, return handles and tweets\n",
    "def parse_tweet_csv(file, file_encoding=\"utf8\"):\n",
    "    # init\n",
    "    handles, tweets = [], []\n",
    "    \n",
    "    # read file\n",
    "    linenr = -1\n",
    "    with open(file, encoding=file_encoding) as input:\n",
    "        try:\n",
    "            for line in input:\n",
    "                linenr += 1\n",
    "                if linenr == 0: continue\n",
    "                \n",
    "                # get contents\n",
    "                line = line.split(\",\")\n",
    "                if line[0] in HANDLES: #label and irst line of tweet\n",
    "                    handles.append(line[0])\n",
    "                    tweet = ','.join(line[1:])\n",
    "                    tweets.append(tweet)\n",
    "                else: #second+ line of tweet\n",
    "                    tweet = tweets.pop()\n",
    "                    tweet += ','.join(line)\n",
    "                    tweets.append(tweet)\n",
    "        except Exception as e:\n",
    "            print(\"Exception at line {}: {}\".format(linenr, e))\n",
    "            raise e\n",
    "    \n",
    "    # sanity checks\n",
    "    assert len(handles) == len(tweets)\n",
    "    print(\"Found {} tweets in {} lines\".format(len(tweets), linenr + 1))\n",
    "    \n",
    "    # return data\n",
    "    return handles, tweets\n",
    "\n",
    "\n",
    "##########################################\n",
    "### coverting tweet strings to numbers ###\n",
    "\n",
    "# coverting labels to integers\n",
    "def int_labels(labels):\n",
    "    return list(map(lambda x: HANDLE_MAP[x], labels))\n",
    "\n",
    "#tokenizing\n",
    "_tokenizer = TweetTokenizer()\n",
    "_punctuation = set(string.punctuation)\n",
    "def tokenize(tweet, lowercase=True, strip_urls=True, strip_punctuation=True):\n",
    "    tokens = _tokenizer.tokenize(tweet)\n",
    "    if lowercase: tokens = list(map(lambda x: x.lower(), tokens))\n",
    "    if strip_urls: tokens = list(filter(lambda x: not x.startswith(\"http\"), tokens))\n",
    "    if strip_punctuation: #https://stackoverflow.com/questions/265960/best-way-to-strip-punctuation-from-a-string-in-python\n",
    "        tokens = list(filter(lambda x: x.startswith(u'@') or x.startswith(u'#') or x not in _punctuation and not re.match(u\"[^\\w\\d'\\s$]+\", x), tokens))\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# get all tweets\n",
    "def import_text(tweets):\n",
    "    tokenized_tweets = [tokenize(tweet) for tweet in tweets]\n",
    "    for tweet in tokenized_tweets:\n",
    "        add_bigrams_to_tweet(tweet)\n",
    "    w2i = enumerate_words(tokenized_tweets)\n",
    "    return [get_word_vector(w2i, tweet, enforce_length=50) for tweet in tokenized_tweets], tokenized_tweets, w2i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4743 tweets in 6251 lines\n",
      "\n",
      "The question in this election: Who can put the plans into action that will make your life better? https://t.co/XreEY9OicG\n",
      "['the', 'question', 'in', 'this', 'election', 'who', 'can', 'put', 'the', 'plans', 'into', 'action', 'that', 'will', 'make', 'your', 'life', 'better', 'the question', 'question in', 'in this', 'this election', 'election who', 'who can', 'can put', 'put the', 'the plans', 'plans into', 'into action', 'action that', 'that will', 'will make', 'make your', 'your life', 'life better']\n",
      "[2308, 13345, 14070, 9723, 48600, 28636, 44907, 20585, 2308, 522, 49040, 46708, 33819, 25170, 22851, 6680, 42400, 229, 36393, 35782, 24513, 7514, 40987, 27652, 36928, 16250, 34701, 38810, 32377, 13334, 45769, 25611, 16906, 48311, 3668, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "################################################\n",
    "# get raw test data\n",
    "################################################\n",
    "import random\n",
    "\n",
    "# init\n",
    "TEST_RATIO = 0.1\n",
    "assert TEST_RATIO > 0 and TEST_RATIO < 1\n",
    "\n",
    "# get data\n",
    "text_handles, raw_tweets = parse_tweet_csv(\"train.csv\")\n",
    "handles = int_labels(text_handles)\n",
    "tweets, tokenized_tweets, word_mapping = import_text(raw_tweets)   \n",
    "data_vector_size = len(tweets[0])\n",
    "\n",
    "### validation\n",
    "for i in range(1):\n",
    "    rand_i = random.randint(0, len(raw_tweets))\n",
    "    print()\n",
    "    print(raw_tweets[i].strip())\n",
    "    print(tokenized_tweets[i])\n",
    "    print(tweets[i])\n",
    "    print(handles[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Separated into 4299 train and 444 test (9%)\n",
      "\n",
      "   handle  length                                         tweet_data\n",
      "0       0      50  [13335, 11120, 5603, 20623, 4725, 34261, 8840,...\n",
      "1       0      50  [36111, 11676, 41442, 38895, 15867, 45445, 116...\n",
      "2       0      50  [22664, 2447, 35922, 31664, 49311, 6546, 44889...\n",
      "3       0      50  [32016, 21443, 20623, 21024, 30965, 12744, 430...\n",
      "4       0      50  [2308, 48600, 5688, 24037, 24738, 15231, 47013...\n"
     ]
    }
   ],
   "source": [
    "################################################\n",
    "# split test data into train and test\n",
    "################################################\n",
    "import pandas as pd\n",
    "\n",
    "LABEL = 'handle'\n",
    "DATA = 'tweet_data'\n",
    "LENGTH = 'length'\n",
    "\n",
    "VOCAB_SIZE = len(word_mapping) + ENUMERATION_BEGIN\n",
    "\n",
    "# split into test and train\n",
    "train_labels, train_data, test_labels, test_data = list(), list(), list(), list()\n",
    "for handle, tweet in zip(handles, tweets):\n",
    "    if random.random() < TEST_RATIO:\n",
    "        test_labels.append(handle)\n",
    "        test_data.append(tweet)\n",
    "    else:\n",
    "        train_labels.append(handle)\n",
    "        train_data.append(tweet)\n",
    "\n",
    "# document and validate\n",
    "print(\"Separated into {} train and {} test ({}%)\\n\".format(len(train_data), len(test_data), \n",
    "                                                         int(100.0 * len(test_data) / len(raw_tweets))))\n",
    "assert len(train_labels) == len(train_data) and len(train_data) > 0\n",
    "assert len(test_labels) == len(test_data) and len(test_data) > 0\n",
    "assert len(test_labels) > len(tweets) * (TEST_RATIO - .05)\n",
    "assert len(test_labels) < len(tweets) * (TEST_RATIO + .05) \n",
    "\n",
    "# save to dataframe\n",
    "train = pd.DataFrame({\n",
    "    LABEL: train_labels,\n",
    "    DATA: train_data,\n",
    "    LENGTH: [data_vector_size for _ in range(len(train_data))]\n",
    "})\n",
    "test = pd.DataFrame({\n",
    "    LABEL: test_labels,\n",
    "    DATA: test_data,\n",
    "    LENGTH: [data_vector_size for _ in range(len(test_data))]\n",
    "})\n",
    "print(train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequences:\n",
      " <class 'numpy.ndarray'>: \n",
      "[[20623 35112 30965 15010 19728 40297 49311 11158 10372  9192  1930 21949\n",
      "  13193  9901 15501 17827 43980 44478 29329 39239 25869 29772 22888 20218\n",
      "  27807 23207 30518 44069 24353 28818 31889 40107 40795  4962  5542 29571\n",
      "  15347 41399 30195 36733 18560 24436 36215 39084 35387 17288 27089     0\n",
      "      0     0]\n",
      " [ 2308 20623 24684 25621 43059  3069 19768 33859 35784  4594 40492 37634\n",
      "  47362     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0]]\n",
      " <class 'numpy.ndarray'>: \n",
      "[20623 35112 30965 15010 19728 40297 49311 11158 10372  9192  1930 21949\n",
      " 13193  9901 15501 17827 43980 44478 29329 39239 25869 29772 22888 20218\n",
      " 27807 23207 30518 44069 24353 28818 31889 40107 40795  4962  5542 29571\n",
      " 15347 41399 30195 36733 18560 24436 36215 39084 35387 17288 27089     0\n",
      "     0     0]\n",
      " <class 'numpy.int32'>: \n",
      "20623\n",
      "\n",
      "\n",
      "Target values\n",
      " <class 'numpy.ndarray'>: \n",
      "[1 1]\n",
      " <class 'numpy.int64'>: \n",
      "1\n",
      "\n",
      "\n",
      "Sequence lengths\n",
      " <class 'numpy.ndarray'>: \n",
      "[50 50]\n",
      " <class 'numpy.int64'>: \n",
      "50\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "################################################\n",
    "# initializing our tensor\n",
    "#\n",
    "# based off of blogpost david parks showed us:\n",
    "# https://r2rt.com/recurrent-neural-networks-in-tensorflow-iii-variable-length-sequences.html\n",
    "################################################\n",
    "import tensorflow as tf\n",
    " \n",
    "class DataIterator():\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.size = len(self.df)\n",
    "        self.epochs = 0\n",
    "        self.shuffle()\n",
    "\n",
    "    def shuffle(self):\n",
    "        self.df = self.df.sample(frac=1).reset_index(drop=True)\n",
    "        self.cursor = 0\n",
    "\n",
    "    def next_batch(self, n):\n",
    "        if self.cursor + n - 1 > self.size:\n",
    "            self.epochs += 1\n",
    "            self.shuffle()\n",
    "        res = self.df.ix[self.cursor:self.cursor + n - 1]\n",
    "        start_idx = self.cursor\n",
    "        self.cursor += n\n",
    "        # return res[DATA], res[LABEL], res[LENGTH]\n",
    "        # the above line fails.  an error is thrown when tf attempts to call np.asarray on this.\n",
    "        # what is different about how our data is organized compared to the blog post this came from?\n",
    "        # TODO \n",
    "        data = res[DATA]\n",
    "        labels = res[LABEL]\n",
    "        length = res[LENGTH]\n",
    "        return np.asarray([data[i] for i in range(start_idx, start_idx + len(data))]), \\\n",
    "               np.asarray([labels[i] for i in range(start_idx, start_idx + len(labels))]), \\\n",
    "               np.asarray([length[i] for i in range(start_idx, start_idx + len(length))])\n",
    "\n",
    "# validate data iterator\n",
    "d = DataIterator(test).next_batch(2)\n",
    "print('Input sequences:\\n', \n",
    "      \"{}: \\n{}\\n\".format(type(d[0]), d[0]), \n",
    "      \"{}: \\n{}\\n\".format(type(d[0][0]), d[0][0]), \n",
    "      \"{}: \\n{}\\n\".format(type(d[0][0][0]), d[0][0][0]), \n",
    "      end='\\n\\n')\n",
    "print('Target values\\n', \n",
    "      \"{}: \\n{}\\n\".format(type(d[1]), d[1]), \n",
    "      \"{}: \\n{}\\n\".format(type(d[1][0]), d[1][0]), \n",
    "      end='\\n\\n')\n",
    "print('Sequence lengths\\n', \n",
    "      \"{}: \\n{}\\n\".format(type(d[2]), d[2]), \n",
    "      \"{}: \\n{}\\n\".format(type(d[2][0]), d[2][0]), \n",
    "      end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "################################################\n",
    "# initializing our tensor\n",
    "#\n",
    "# based off of blogpost david parks showed us:\n",
    "# https://r2rt.com/recurrent-neural-networks-in-tensorflow-iii-variable-length-sequences.html\n",
    "################################################\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def reset_graph():\n",
    "    if 'sess' in globals() and sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "\n",
    "def build_graph(vocab_size = VOCAB_SIZE, state_size = 64, batch_size = 256, num_classes = 2):\n",
    "\n",
    "    reset_graph()\n",
    "\n",
    "    # Placeholders\n",
    "    x = tf.placeholder(tf.int32, [batch_size, None]) # [batch_size, num_steps]\n",
    "    seqlen = tf.placeholder(tf.int32, [batch_size])\n",
    "    y = tf.placeholder(tf.int32, [batch_size])\n",
    "    keep_prob = tf.placeholder_with_default(1.0, [])\n",
    "\n",
    "    # Embedding layer\n",
    "    embeddings = tf.get_variable('embedding_matrix', [vocab_size, state_size])\n",
    "    rnn_inputs = tf.nn.embedding_lookup(embeddings, x)\n",
    "\n",
    "    # RNN\n",
    "    cell = tf.nn.rnn_cell.GRUCell(state_size)\n",
    "    init_state = tf.get_variable('init_state', [1, state_size],\n",
    "                                 initializer=tf.constant_initializer(0.0))\n",
    "    init_state = tf.tile(init_state, [batch_size, 1])\n",
    "    rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, rnn_inputs, sequence_length=seqlen,\n",
    "                                                 initial_state=init_state)\n",
    "\n",
    "    # Add dropout, as the model otherwise quickly overfits\n",
    "    rnn_outputs = tf.nn.dropout(rnn_outputs, keep_prob)\n",
    "    idx = tf.range(batch_size)*tf.shape(rnn_outputs)[1] + (seqlen - 1)\n",
    "    # last_rnn_output = tf.gather(tf.reshape(rnn_outputs, [-1, state_size]), idx)\n",
    "    # last_rnn_output = tf.gather_nd(rnn_outputs, tf.pack([tf.range(batch_size), seqlen-1], axis=1))\n",
    "    last_rnn_output = tf.gather_nd(rnn_outputs, tf.stack([tf.range(batch_size), seqlen-1], axis=1))\n",
    "\n",
    "    # Softmax layer\n",
    "    with tf.variable_scope('softmax'):\n",
    "        W = tf.get_variable('W', [state_size, num_classes]) # weights?\n",
    "        b = tf.get_variable('b', [num_classes], initializer=tf.constant_initializer(0.0)) # bias?\n",
    "    logits = tf.matmul(last_rnn_output, W) + b\n",
    "    preds = tf.nn.softmax(logits)\n",
    "    correct = tf.equal(tf.cast(tf.argmax(preds,1),tf.int32), y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "\n",
    "    return {\n",
    "        'x': x,\n",
    "        'seqlen': seqlen,\n",
    "        'y': y,\n",
    "        'dropout': keep_prob,\n",
    "        'loss': loss,\n",
    "        'ts': train_step,\n",
    "        'preds': preds,\n",
    "        'accuracy': accuracy\n",
    "    }\n",
    "\n",
    "\n",
    "def train_graph(g, batch_size = 256, num_epochs = 10, iterator = DataIterator):\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        tr = iterator(train)\n",
    "        te = iterator(test)\n",
    "\n",
    "        step, accuracy = 0, 0\n",
    "        tr_losses, te_losses = [], []\n",
    "        current_epoch = 0\n",
    "        while current_epoch < num_epochs:\n",
    "            step += 1\n",
    "            batch = tr.next_batch(batch_size)\n",
    "            feed = {g['x']: batch[0], g['y']: batch[1], g['seqlen']: batch[2], g['dropout']: 0.6}\n",
    "            accuracy_, _ = sess.run([g['accuracy'], g['ts']], feed_dict=feed)\n",
    "            accuracy += accuracy_\n",
    "\n",
    "            if tr.epochs > current_epoch:\n",
    "                current_epoch += 1\n",
    "                tr_losses.append(accuracy / step)\n",
    "                step, accuracy = 0, 0\n",
    "\n",
    "                #eval test set\n",
    "                te_epoch = te.epochs\n",
    "                while te.epochs == te_epoch:\n",
    "                    step += 1\n",
    "                    batch = te.next_batch(batch_size)\n",
    "                    feed = {g['x']: batch[0], g['y']: batch[1], g['seqlen']: batch[2]}\n",
    "                    accuracy_ = sess.run([g['accuracy']], feed_dict=feed)[0]\n",
    "                    accuracy += accuracy_\n",
    "\n",
    "                te_losses.append(accuracy / step)\n",
    "                step, accuracy = 0,0\n",
    "                print(\"Accuracy after epoch\", current_epoch, \" - tr:\", tr_losses[-1], \"- te:\", te_losses[-1])\n",
    "\n",
    "    return tr_losses, te_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after epoch 1  - tr: 0.496323529412 - te: 0.5\n",
      "Accuracy after epoch 2  - tr: 0.543701171875 - te: 0.50390625\n",
      "Accuracy after epoch 3  - tr: 0.588623046875 - te: 0.61328125\n",
      "Accuracy after epoch 4  - tr: 0.613525390625 - te: 0.609375\n",
      "Accuracy after epoch 5  - tr: 0.718994140625 - te: 0.82421875\n",
      "Accuracy after epoch 6  - tr: 0.9169921875 - te: 0.81640625\n",
      "Accuracy after epoch 7  - tr: 0.940673828125 - te: 0.91015625\n",
      "Accuracy after epoch 8  - tr: 0.954345703125 - te: 0.91796875\n",
      "Accuracy after epoch 9  - tr: 0.967041015625 - te: 0.91015625\n",
      "Accuracy after epoch 10  - tr: 0.9814453125 - te: 0.921875\n",
      "Accuracy after epoch 11  - tr: 0.98681640625 - te: 0.953125\n",
      "Accuracy after epoch 12  - tr: 0.9931640625 - te: 0.9453125\n",
      "Accuracy after epoch 13  - tr: 0.995849609375 - te: 0.921875\n",
      "Accuracy after epoch 14  - tr: 0.996826171875 - te: 0.93359375\n",
      "Accuracy after epoch 15  - tr: 0.99755859375 - te: 0.9453125\n",
      "Accuracy after epoch 16  - tr: 0.997314453125 - te: 0.94140625\n"
     ]
    }
   ],
   "source": [
    "################################################\n",
    "# run it!\n",
    "################################################\n",
    "\n",
    "# this fails, just like us\n",
    "g = build_graph()\n",
    "tr_losses, te_losses = train_graph(g, num_epochs=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nWhat we tried:\\n\\nglove library - broke even\\n    overfit and data was varied\\nglove \"average tweet value\"\\n    overfit and test data was varied\\nglove ATV with variance \\n    this worked worse than ATV\\ntwitter glove worked worse (but need better tokenizing)\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TODO: we're overfitting, why?\n",
    "\n",
    "#TODO: try something better than averaging the word vec values\n",
    "# maybe we could do three-d arrays?  encode each word and pad the data\n",
    "\n",
    "#TODO: we strip a lot away (ie punctuation, smilies) and lose other\n",
    "# data to glove (#hashtags, @handles). how can we keep this?\n",
    "\n",
    "#TODO: how do we run this on the test dataset?\n",
    "\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "\"\"\"\n",
    "What we tried:\n",
    "\n",
    "glove library - broke even\n",
    "    overfit and data was varied\n",
    "glove \"average tweet value\"\n",
    "    overfit and test data was varied\n",
    "glove ATV with variance \n",
    "    this worked worse than ATV\n",
    "twitter glove worked worse (but need better tokenizing)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
