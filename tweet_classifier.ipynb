{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('tests', 0.9164356610046378), ('testing', 0.81992214225170001), ('tested', 0.74428756870386181), ('final', 0.69102279899050822), ('taking', 0.68790020450861178), ('results', 0.68468909013686297), ('match', 0.67696818430673733), ('determine', 0.67679768970507126), ('challenge', 0.6747705556743957)]\n"
     ]
    }
   ],
   "source": [
    "################################################\n",
    "# intialize glove\n",
    "################################################\n",
    "# >pip install glove_python\n",
    "# >wget http://www.google.com/search?q=glove+word+embeddings+download+those+dope+pre+trained+vectors\n",
    "# >unzip dope_glove_shit.zip\n",
    "# >echo \"set GLOVE_LOCATION to one of those files\"\n",
    "################################################\n",
    "\n",
    "from glove import Glove\n",
    "\n",
    "GLOVE_LOCATION = \"../glove.6B/glove.6B.50d.txt\"\n",
    "\n",
    "glove_data = Glove.load_stanford(GLOVE_LOCATION)\n",
    "\n",
    "print(glove_data.most_similar('test', number=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4743 tweets in 6251 lines\n['the', 'question', 'in', 'this', 'election', 'who', 'can', 'put', 'the', 'plans', 'into', 'action', 'that', 'will', 'make', 'your', 'life', 'better']\n[-0.02025492  0.17447466  0.1113957   0.10850239 -0.15766437 -0.00880653\n  0.08078878 -0.18767856 -0.01001138 -0.02274603  0.06846685 -0.0201696\n  0.18227073  0.02041927 -0.00273995  0.05156851  0.02180317  0.07600065\n  0.23698627  0.1188742   0.27986816 -0.04159186  0.19940441 -0.03171541\n  0.02606152  0.07798278 -0.05114254  0.11331858 -0.01473179  0.02611706\n  1.34425389  0.31747192 -0.00419791 -0.00589631 -0.20707862 -0.20163681\n -0.00490502 -0.09823382 -0.32663391  0.01303974  0.17537923 -0.26139659\n -0.03909616 -0.06620144  0.26834694 -0.19813136 -0.38873964  0.01047187\n -0.03145515  0.24641894]\n\n['last', 'night', 'donald', 'trump', 'said', 'not', 'paying', 'taxes', 'was', 'smart', 'you', 'know', 'what', 'i', 'call', 'it', 'unpatriotic']\n[ -8.85836716e-02   2.28314284e-01   1.39221142e-01   1.33450377e-01\n  -8.12677151e-02  -6.84589110e-02   7.59751507e-02  -2.43248040e-01\n   2.53259904e-02  -6.62242575e-02  -1.08987966e-02  -1.22199839e-01\n   1.18392747e-01   1.09304588e-01  -2.41040602e-02   1.79898184e-02\n   7.72063009e-02  -3.83449744e-03   1.31803853e-01   1.32417211e-01\n   2.20828378e-01  -1.25391405e-01   1.38858919e-01  -1.25620565e-01\n  -9.76423645e-03  -8.20200134e-02  -1.92449128e-01   2.25286843e-01\n  -1.46304962e-02  -6.33386645e-02   1.20392438e+00   1.95321043e-01\n  -3.62321097e-02   3.43120541e-02  -1.40669193e-01  -2.17452694e-01\n  -2.95414819e-02  -8.29407558e-02  -1.91875877e-01  -4.52255003e-02\n   4.88284624e-02  -1.08684925e-01   4.23665929e-03  -1.63452205e-01\n   1.92575530e-01  -1.15830448e-01  -2.82427192e-01   8.38952439e-02\n   2.78987092e-04   2.80635083e-01]\n\n['if', 'we', 'stand', 'together', \"there's\", 'nothing', 'we', \"can't\", 'do', 'make', 'sure', \"you're\", 'ready', 'to', 'vote']\n[ 0.03867076  0.26797841  0.11200483  0.18279485 -0.10663236 -0.05469228\n -0.02363164 -0.03154969  0.28602778 -0.02290537 -0.00469233  0.03981343\n -0.0018809  -0.04519832 -0.01514916 -0.03322704  0.04913362 -0.09195056\n  0.22143108  0.10115181  0.04430486 -0.10496725  0.13109599  0.00986941\n  0.07457041 -0.09940892 -0.02755067  0.22017904 -0.04354279 -0.01207813\n  0.94403718 -0.0601169  -0.03199206  0.10538942 -0.21772039 -0.22373909\n  0.00377013 -0.1217366  -0.33986355 -0.02989552  0.24070431 -0.04940143\n -0.13518975 -0.01941866 -0.0373745   0.02149706 -0.17609204 -0.06975219\n  0.08374166  0.35940076]\n\n['both', 'candidates', 'were', 'asked', 'about', 'how', \"they'd\", 'confront', 'racial', 'injustice', 'only', 'one', 'had', 'a', 'real', 'answer']\n[ 0.01508353  0.15614483  0.06831048  0.12451919 -0.1924814  -0.08411121\n  0.07804554 -0.13832694  0.07187414 -0.16082124 -0.06015119 -0.06176643\n  0.23458061  0.04140594 -0.01485973 -0.02386378  0.20671611  0.10609507\n  0.14100508  0.14184797  0.20498616 -0.07814837  0.19436367 -0.11147123\n  0.04365252  0.04994126  0.04602413  0.09015078 -0.08951025  0.07863789\n  1.36145519 -0.11616279  0.03261358 -0.00669526 -0.238255   -0.2356493\n -0.03142999 -0.12485351 -0.23485339  0.09447534  0.16183428 -0.24279425\n -0.0376352  -0.09981503  0.11413043 -0.02269338 -0.29547245  0.00297051\n -0.03318194  0.33764036]\n\n['join', 'me', 'for', 'a', '3pm', 'rally', 'tomorrow', 'at', 'the', 'mid-america', 'center', 'in', 'council', 'bluffs', 'iowa', 'tickets']\n[  8.77362121e-02   1.42880728e-01   1.06519591e-02   1.41661162e-01\n  -1.47427875e-01  -8.07986507e-03   6.21808280e-02  -1.94851458e-01\n   5.50258600e-02   2.17700281e-02  -8.52259937e-04  -9.74694190e-03\n   2.35096536e-01  -1.04652498e-01  -1.58729604e-02   1.12918594e-01\n   4.67140738e-02   1.06028367e-01   8.37237174e-02  -6.55596018e-02\n   5.62623967e-02   6.36254591e-02   1.39638601e-01  -3.91233067e-02\n   1.43119807e-01   1.11695893e-01  -1.64866512e-02   1.99654504e-01\n   5.42201183e-02  -2.37050306e-02   1.15925521e+00   4.23705899e-02\n   4.92519252e-02  -1.73580599e-02  -1.59097148e-01  -2.12405991e-01\n   1.07062731e-01  -1.04040721e-01  -1.71319628e-01   3.64605717e-02\n   1.74228112e-01  -1.82332318e-01  -1.27758869e-01   6.68957353e-02\n  -7.78972902e-03  -1.01393317e-01  -2.67319238e-01   1.58895414e-02\n  -1.29973468e-01   3.38806011e-01]\n\n['when', 'donald', 'trump', 'goes', 'low', 'register', 'to', 'vote']\n[ 0.03020908  0.2481194   0.04302483  0.13679192 -0.29620558 -0.10062628\n  0.02861014 -0.18617272  0.09996888  0.12313189  0.19413751 -0.0319601\n  0.24662438  0.10623685 -0.08353029  0.0349418   0.1393973  -0.06972098\n  0.14851424  0.11992671  0.124238    0.05465534  0.10639392 -0.09253715\n  0.00291078  0.13915794  0.16948437  0.03233772 -0.12284548 -0.02564575\n  1.19960025  0.17440962  0.11611842 -0.01142584 -0.08425875 -0.21426669\n -0.04924243 -0.06975193 -0.18946606  0.13898946  0.16742507 -0.03044012\n  0.06297097 -0.06742217  0.17601856  0.01194886 -0.39229323  0.03423332\n -0.04289904  0.35040967]\n\n['3', 'has', 'trump', 'offered', 'a', 'single', 'proposal', 'to', 'reduce', 'the', 'friction', 'of', 'starting', 'a', 'business', '@hillaryclinton', 'has']\n[-0.16780968  0.22751935  0.10301363  0.15109803 -0.11557776 -0.09045454\n  0.18733599 -0.17420313  0.05099141  0.063916    0.02517793  0.00393994\n  0.09909077  0.13795505 -0.00614481 -0.01875243  0.13221177  0.14946574\n  0.13823954  0.08281077  0.17001487 -0.08083467  0.13951591 -0.02782538\n  0.07788694 -0.11940703 -0.08979798  0.1323763  -0.05104221 -0.07356948\n  1.16087434  0.17524336  0.08351766  0.11944273 -0.20609495 -0.2017003\n -0.06158125 -0.28387475 -0.21353026  0.02422047  0.10322676  0.0410976\n -0.06899588 -0.07394952  0.17984459 -0.07963606 -0.11990409  0.10197897\n -0.08953107  0.37710184]\n\n['the', 'election', 'is', 'just', 'weeks', 'away', 'check', 'if', \"you're\", 'registered', 'to', 'vote', 'at', 'only', 'takes', 'a', 'few', 'cl']\n[  2.38838011e-01   1.52809614e-01   4.68762839e-02   3.14619859e-01\n  -4.52859383e-02  -9.87790072e-02  -7.38534460e-03  -2.10581643e-01\n   9.86587004e-02   1.15076646e-02   5.02074630e-02   7.00240017e-02\n   5.43867112e-02  -1.80850829e-04  -2.27457898e-02   1.12089331e-02\n  -7.44345480e-02   3.92166173e-02   1.04499636e-01   1.00338620e-01\n   1.13851211e-01   2.48540638e-02   1.02033939e-01  -7.28187742e-02\n  -1.19972774e-01   5.30412957e-02  -2.03135535e-02   1.67803510e-01\n   5.11129123e-03  -5.50486833e-02   1.10212641e+00  -1.27018590e-02\n   2.52494859e-02   2.58109291e-02  -2.21843345e-01  -1.06721270e-01\n  -1.48531588e-02  -1.90769422e-01  -1.26171798e-01  -1.25485932e-01\n   2.44137070e-01  -4.68253775e-02   1.15827255e-01   3.64370054e-02\n   1.31842705e-01  -2.02835919e-02  -2.13760084e-01   2.12329049e-02\n   1.36498857e-02   2.91404255e-01]\n\n['hillary', \"clinton's\", 'campaign', 'continues', 'to', 'make', 'false', 'claims', 'about', 'foundation', 'disclosure']\n[ -3.73726004e-02   2.04028989e-01   8.58527173e-02   1.05333511e-01\n  -8.77876172e-02  -1.04069257e-01   2.23376896e-03  -1.65624314e-01\n  -9.99099268e-02  -3.65018469e-02   1.88063889e-01  -3.99709188e-02\n   3.12801796e-01   5.72884011e-02  -1.78720612e-02   2.32299472e-04\n   1.15511547e-01   1.57912189e-01   1.86085283e-01   1.49260161e-01\n   2.85904891e-01  -4.28509562e-02   1.91672434e-01  -9.34069162e-02\n   9.82120365e-02   1.62415414e-02   1.40727681e-01   6.72354158e-02\n  -8.91645421e-02  -6.04706205e-02   1.39527578e+00   1.52576058e-01\n   1.15161721e-01   3.90056302e-02  -1.60565294e-01  -2.16191307e-01\n  -5.98543193e-02  -1.65522963e-01  -2.98379422e-01   1.57720452e-02\n  -3.85968632e-03  -1.91761289e-01   3.69405387e-02  -2.72536206e-01\n   9.09206252e-02  -8.64607291e-02  -2.22332218e-01   9.95912444e-02\n  -9.93773280e-02   2.80932591e-01]\n\n['cnbc', 'time', 'magazine', 'online', 'polls', 'say', 'donald', 'trump', 'won', 'the', 'first', 'presidential', 'debate', 'via', '@washtimes', '#maga']\n[ 0.07977725  0.32503022  0.30300948  0.09298268  0.02068206 -0.00121307\n  0.10759809 -0.17745853  0.07828976  0.05111254 -0.04624836  0.01128174\n  0.09223599 -0.11570541  0.20109912 -0.04871388  0.15117521  0.17588891\n -0.05141551  0.03505887  0.13740279  0.02015784  0.32004931 -0.04953061\n  0.06581527 -0.0286369  -0.03936566  0.21566476  0.01177065  0.02387794\n  0.98960732  0.06792902 -0.2313796   0.03518043 -0.15114923 -0.16674323\n -0.04918211 -0.07880306 -0.07350951 -0.29447341  0.17267548  0.07000275\n -0.13302437 -0.08948321  0.29698185 -0.08359834 -0.19106939  0.12627586\n  0.08078251  0.31332943]\n\n['donald', 'trump', 'lied', 'to', 'the', 'american', 'people', 'at', 'least', '58', 'times', 'during', 'the', 'first', 'presidential', 'debate', 'we', 'counted']\n[ 0.04344205  0.18091715  0.17921524  0.13786291 -0.27115315 -0.04765215\n  0.0833431  -0.16363892  0.07929387  0.01837252  0.00183348  0.11293115\n  0.2322857   0.02508349 -0.27035225  0.13118281  0.08941392  0.08742038\n  0.2259453  -0.06453732 -0.01521202 -0.06942227  0.17547754 -0.04525309\n  0.02959825  0.14826776 -0.20083398  0.20470413 -0.16757027  0.08842652\n  1.40890001  0.19564518 -0.00245631 -0.12147135 -0.24951308 -0.18785229\n -0.17560196 -0.093696   -0.18143207 -0.04722909  0.04078317  0.06058919\n -0.14431392  0.07379611  0.3539096  -0.19079097 -0.24984663  0.22410198\n -0.08589396  0.33841012]\n\n['in', 'the', 'last', '24', 'hrs', 'we', 'have', 'raised', 'over', '13m', 'from', 'online', 'donations', 'and', 'national', 'call', 'day', 'and', 'we', 're', 'still', 'going', 'thank', 'you', 'america', '#maga']\n[ 0.28839915  0.22490124 -0.06041439  0.29544363  0.05904967 -0.11226756\n  0.0086063  -0.2158453   0.04606334  0.06935187 -0.06597188  0.14414069\n -0.05325201 -0.01666654 -0.16455838 -0.0033978   0.19116781  0.12607075\n -0.07481658 -0.0457796  -0.05102601 -0.08630197  0.14392523  0.06400415\n -0.17930136  0.14443594 -0.07712831  0.36830304 -0.0368682  -0.00172004\n  1.11996036 -0.0989341  -0.15605651 -0.04522687 -0.2309214  -0.10340263\n  0.09961785 -0.24876787 -0.08814415 -0.10688789  0.00512066  0.13358472\n -0.35118739  0.12124681  0.35249177 -0.14112678 -0.34357587 -0.03995123\n  0.14312569  0.27170368]\n\n['she', 'gained', 'about', '55', 'pounds', 'in', '9', 'months', 'she', 'was', 'like', 'an', 'eating', 'machine', 'trump', 'a', 'man', 'who', 'wants', 'to', 'be', 'president']\n[ -8.58473503e-03   2.05088355e-01   1.55044632e-01   1.36321661e-01\n  -1.77134033e-01  -1.58842975e-01   1.63415374e-01  -7.76174860e-02\n   1.09766077e-01  -8.08125212e-02   4.01591041e-02  -5.09655494e-02\n   1.80325766e-01   1.08687856e-01  -2.24323335e-01   2.03372719e-01\n   2.36199085e-01  -9.82172210e-02   1.52044807e-01   7.25767091e-02\n   1.93558753e-01  -4.94087916e-02   1.37038405e-01  -5.77981438e-02\n  -5.76028622e-03   1.28525425e-01  -2.57555346e-01   2.24620198e-01\n   3.78665949e-02   1.04723342e-01   1.44644295e+00   7.13959120e-02\n   4.88334837e-03  -1.20342314e-01  -2.43334215e-01  -1.84853283e-01\n  -8.72403429e-02  -1.38220111e-01  -3.86334871e-01   1.02173223e-01\n   1.21287015e-01  -5.85267559e-03  -1.35915264e-02  -7.60834138e-04\n   2.22723994e-01  -1.83026031e-01  -3.04856258e-01   7.89834458e-02\n  -5.91168648e-02   2.62856371e-01]\n\n['we', 'don', 't', 'want', 'to', 'turn', 'against', 'each', 'other', 'we', 'want', 'to', 'work', 'with', 'one', 'another', 'we', 'want', 'to', 'set', 'big', 'goals', 'in', 'this', 'country', '#strongertogether']\n[ 0.11141274  0.22875291  0.02458783  0.36120335 -0.0322259  -0.11651738\n -0.03110629 -0.32466548  0.0877969  -0.11299561 -0.04943447  0.01569982\n -0.00814844  0.08427325 -0.0791345   0.07625188  0.08234016  0.0164455\n  0.05850702  0.02513727  0.15801381 -0.07494183  0.16988296 -0.05203404\n -0.21863455  0.05084761 -0.15732797  0.33305074  0.10285252 -0.12184747\n  1.1121371   0.03907427 -0.06676421  0.03832995 -0.24111408 -0.15853965\n  0.06501485 -0.3238325  -0.11954695 -0.04136457  0.15083107 -0.12632193\n -0.00252674  0.0811723   0.13491326 -0.10090105 -0.36772188 -0.05530934\n  0.07309289  0.34863386]\n\n['what', 'we', 'hear', 'from', 'my', 'opponent', 'is', 'dangerously', 'incoherent', \"it's\", 'unclear', 'what', \"he's\", 'saying', 'but', 'words', 'matter', 'hillary']\n[ 0.00125113  0.20233592  0.12113938  0.22004832 -0.05040096 -0.05897326\n  0.17734231 -0.08650655  0.06371336 -0.15688364  0.01286222 -0.12390586\n  0.12512938  0.00800331 -0.11150432  0.11265767  0.04644681 -0.0262135\n  0.17145746  0.08795327  0.16975484 -0.07642076  0.1024629  -0.04972619\n  0.01765936 -0.06903762 -0.06402993  0.10867415  0.00429798 -0.08885525\n  1.17549338 -0.09542131  0.02555036  0.01014214 -0.29247644 -0.27236783\n -0.06411046 -0.15763151 -0.28203827 -0.02413121  0.16660745 -0.06097416\n -0.06678263 -0.14543519  0.08948277 -0.11435489 -0.18011525 -0.08456172\n -0.05254102  0.3688157 ]\n\n['one', 'candidate', 'made', 'it', 'clear', 'he', 'wasn', 't', 'prepared', 'for', 'last', 'night', 's', 'debate', 'the', 'other', 'made', 'it', 'clear', 'she', 's', 'prepared', 'to', 'b']\n[ 0.22911183  0.13137895  0.0541441   0.33436879 -0.06786561 -0.02272159\n -0.07385715 -0.29168121  0.09960347  0.0040855  -0.06632771 -0.03946435\n  0.01572088  0.01420869 -0.10346338 -0.08984854  0.07133377  0.00817684\n  0.18428301  0.08772841  0.13362359 -0.1410931   0.13310033 -0.05807193\n -0.15565126  0.04622243 -0.06790567  0.14756683  0.00758309 -0.06042977\n  1.21536155  0.0367004  -0.06846464 -0.07979642 -0.10234783 -0.20982951\n -0.04151788 -0.14791151 -0.09441805 -0.14603821  0.10271439  0.1094797\n  0.0769528  -0.04754136  0.2058877  -0.22006922 -0.29593666  0.05782536\n  0.03544143  0.36953729]\n\n"
     ]
    }
   ],
   "source": [
    "################################################\n",
    "# file parsing functions\n",
    "################################################\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import string, re\n",
    "\n",
    "# definitions\n",
    "HC=\"HillaryClinton\"\n",
    "DT=\"realDonaldTrump\"\n",
    "NA=\"none\"\n",
    "HANDLES = [HC,DT,NA]\n",
    "HANDLE_MAP = {NA:0, HC:1, DT:2}\n",
    "\n",
    "# read csv file, return handles and tweets\n",
    "def parse_tweet_csv(file, file_encoding=\"utf8\"):\n",
    "    # init\n",
    "    handles, tweets = [], []\n",
    "    \n",
    "    # read file\n",
    "    linenr = -1\n",
    "    with open(file, encoding=file_encoding) as input:\n",
    "        try:\n",
    "            for line in input:\n",
    "                linenr += 1\n",
    "                if linenr == 0: continue\n",
    "                \n",
    "                # get contents\n",
    "                line = line.split(\",\")\n",
    "                if line[0] in HANDLES: #label and irst line of tweet\n",
    "                    handles.append(line[0])\n",
    "                    tweet = ','.join(line[1:])\n",
    "                    tweets.append(tweet)\n",
    "                else: #second+ line of tweet\n",
    "                    tweet = tweets.pop()\n",
    "                    tweet += ','.join(line)\n",
    "                    tweets.append(tweet)\n",
    "        except Exception as e:\n",
    "            print(\"Exception at line {}: {}\".format(linenr, e))\n",
    "            raise e\n",
    "    \n",
    "    # sanity checks\n",
    "    assert len(handles) == len(tweets)\n",
    "    print(\"Found {} tweets in {} lines\".format(len(tweets), linenr + 1))\n",
    "    \n",
    "    # return data\n",
    "    return handles, tweets\n",
    "\n",
    "\n",
    "##########################################\n",
    "### coverting tweet strings to numbers ###\n",
    "#tokenize\n",
    "_tokenizer = TweetTokenizer()\n",
    "_punctuation = set(string.punctuation)\n",
    "def tokenize(tweet, lowercase=True, strip_urls=True, strip_punctuation=True):\n",
    "    tokens = _tokenizer.tokenize(tweet)\n",
    "    if lowercase: tokens = list(map(lambda x: x.lower(), tokens))\n",
    "    if strip_urls: tokens = list(filter(lambda x: not x.startswith(\"http\"), tokens))\n",
    "    if strip_punctuation: #https://stackoverflow.com/questions/265960/best-way-to-strip-punctuation-from-a-string-in-python\n",
    "        tokens = list(filter(lambda x: x.startswith(u'@') or x.startswith(u'#') or x not in _punctuation and not re.match(u\"[^\\w\\d'\\s$]+\", x), tokens))\n",
    "    return tokens\n",
    "# glove\n",
    "def get_glove_vector(tokens):\n",
    "    def transform_paragraph(self, paragraph, epochs=50, ignore_missing=False):\n",
    "        \"\"\"\n",
    "        Transform an iterable of tokens into its vector representation\n",
    "        (a paragraph vector).\n",
    "    \n",
    "        Experimental. This will return something close to a tf-idf\n",
    "        weighted average of constituent token vectors by fitting\n",
    "        rare words (with low word bias values) more closely.\n",
    "        \"\"\"\n",
    "        import collections\n",
    "        import numpy as np\n",
    "        import glove\n",
    "        from glove.glove_cython import fit_vectors, transform_paragraph\n",
    "        \n",
    "    \n",
    "        if self.word_vectors is None:\n",
    "            raise Exception('Model must be fit to transform paragraphs')\n",
    "    \n",
    "        if self.dictionary is None:\n",
    "            raise Exception('Dictionary must be provided to '\n",
    "                            'transform paragraphs')\n",
    "    \n",
    "        cooccurrence = collections.defaultdict(lambda: 0.0)\n",
    "    \n",
    "        for token in paragraph:\n",
    "            try:\n",
    "                cooccurrence[self.dictionary[token]] += self.max_count / 10.0\n",
    "            except KeyError:\n",
    "                if not ignore_missing:\n",
    "                    raise\n",
    "    \n",
    "        random_state = glove.glove.check_random_state(self.random_state)\n",
    "    \n",
    "        word_ids = np.array(list(cooccurrence.keys()), dtype=np.int32)\n",
    "        values = np.array(list(cooccurrence.values()), dtype=np.float64)\n",
    "        shuffle_indices = np.arange(len(word_ids), dtype=np.int32)\n",
    "    \n",
    "        # Initialize the vector to mean of constituent word vectors\n",
    "        paragraph_vector = np.mean(self.word_vectors[word_ids], axis=0)\n",
    "        sum_gradients = np.ones_like(paragraph_vector)\n",
    "    \n",
    "        # Shuffle the coocurrence matrix\n",
    "        random_state.shuffle(shuffle_indices)\n",
    "        transform_paragraph(self.word_vectors,\n",
    "                            self.word_biases,\n",
    "                            paragraph_vector,\n",
    "                            sum_gradients,\n",
    "                            word_ids,\n",
    "                            values,\n",
    "                            shuffle_indices,\n",
    "                            self.learning_rate,\n",
    "                            self.max_count,\n",
    "                            self.alpha,\n",
    "                            epochs)\n",
    "    \n",
    "        return paragraph_vector\n",
    "    \n",
    "    return transform_paragraph(glove_data, \" \".join(tokens), ignore_missing=True)\n",
    "# handle\n",
    "def import_text(tweets):\n",
    "    return [tokenize(tweet) for tweet in tweets]\n",
    "\n",
    "\n",
    "def int_labels(labels):\n",
    "    return list(map(lambda x: HANDLE_MAP[x], labels))\n",
    "    \n",
    "\n",
    "handles, tweets = parse_tweet_csv(\"train.csv\")\n",
    "handles = int_labels(handles)\n",
    "# tweets = import_text(tweets)\n",
    "\n",
    "### for validation\n",
    "for tweet in tweets[0:16]:\n",
    "    print(tokenize(tweet))\n",
    "    print(get_glove_vector(tokenize(tweet)))\n",
    "    print()\n",
    "# for handle in int_labels(handles[0:7]):\n",
    "#     print(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}