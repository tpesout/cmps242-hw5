{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('tests', 0.9164356610046378), ('testing', 0.81992214225169979), ('tested', 0.74428756870386203), ('final', 0.69102279899050834), ('taking', 0.68790020450861178), ('results', 0.68468909013686308), ('match', 0.67696818430673733), ('determine', 0.67679768970507126), ('challenge', 0.67477055567439548)]\n"
     ]
    }
   ],
   "source": [
    "################################################\n",
    "# intialize glove\n",
    "################################################\n",
    "# >pip install glove_python\n",
    "# >wget http://www.google.com/search?q=glove+word+embeddings+download+those+dope+pre+trained+vectors\n",
    "# >unzip dope_glove_shit.zip\n",
    "# >cp cmps242_hw5_config.py.example cmps242_hw5_config.py\n",
    "# >echo \"set GLOVE_LOCATION in cmps242_hw5_config.py to one of those files\"\n",
    "################################################\n",
    "from cmps242_hw5_config import *\n",
    "from glove import Glove\n",
    "\n",
    "glove_data = Glove.load_stanford(GLOVE_LOCATION)\n",
    "\n",
    "print(glove_data.most_similar('test', number=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "################################################\n",
    "# file parsing functions\n",
    "################################################\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import string, re\n",
    "import collections\n",
    "import numpy as np\n",
    "import glove\n",
    "from glove.glove_cython import fit_vectors, transform_paragraph\n",
    "\n",
    "# definitions\n",
    "HC=\"HillaryClinton\"\n",
    "DT=\"realDonaldTrump\"\n",
    "NA=\"none\"\n",
    "HANDLES = [HC,DT,NA]\n",
    "HANDLE_MAP = {NA:-1, HC:0, DT:1}\n",
    "\n",
    "# read csv file, return handles and tweets\n",
    "def parse_tweet_csv(file, file_encoding=\"utf8\"):\n",
    "    # init\n",
    "    handles, tweets = [], []\n",
    "    \n",
    "    # read file\n",
    "    linenr = -1\n",
    "    with open(file, encoding=file_encoding) as input:\n",
    "        try:\n",
    "            for line in input:\n",
    "                linenr += 1\n",
    "                if linenr == 0: continue\n",
    "                \n",
    "                # get contents\n",
    "                line = line.split(\",\")\n",
    "                if line[0] in HANDLES: #label and irst line of tweet\n",
    "                    handles.append(line[0])\n",
    "                    tweet = ','.join(line[1:])\n",
    "                    tweets.append(tweet)\n",
    "                else: #second+ line of tweet\n",
    "                    tweet = tweets.pop()\n",
    "                    tweet += ','.join(line)\n",
    "                    tweets.append(tweet)\n",
    "        except Exception as e:\n",
    "            print(\"Exception at line {}: {}\".format(linenr, e))\n",
    "            raise e\n",
    "    \n",
    "    # sanity checks\n",
    "    assert len(handles) == len(tweets)\n",
    "    print(\"Found {} tweets in {} lines\".format(len(tweets), linenr + 1))\n",
    "    \n",
    "    # return data\n",
    "    return handles, tweets\n",
    "\n",
    "\n",
    "##########################################\n",
    "### coverting tweet strings to numbers ###\n",
    "\n",
    "# coverting labels to integers\n",
    "def int_labels(labels):\n",
    "    return list(map(lambda x: HANDLE_MAP[x], labels))\n",
    "\n",
    "#tokenizing\n",
    "_tokenizer = TweetTokenizer()\n",
    "_punctuation = set(string.punctuation)\n",
    "def tokenize(tweet, lowercase=True, strip_urls=True, strip_punctuation=True):\n",
    "    tokens = _tokenizer.tokenize(tweet)\n",
    "    if lowercase: tokens = list(map(lambda x: x.lower(), tokens))\n",
    "    if strip_urls: tokens = list(filter(lambda x: not x.startswith(\"http\"), tokens))\n",
    "    if strip_punctuation: #https://stackoverflow.com/questions/265960/best-way-to-strip-punctuation-from-a-string-in-python\n",
    "        tokens = list(filter(lambda x: x.startswith(u'@') or x.startswith(u'#') or x not in _punctuation and not re.match(u\"[^\\w\\d'\\s$]+\", x), tokens))\n",
    "    return tokens\n",
    "\n",
    "# glove information\n",
    "def get_glove_vector(glove_data, tokens, epochs=50, ignore_missing=True):\n",
    "    \"\"\"\n",
    "    tpesout: This code came from the 'glove' repo I'm using (but had a bug, so I needed to slighly modify it)\n",
    "    https://github.com/maciejkula/glove-python/blob/master/glove/glove.py\n",
    "    \n",
    "    Transform an iterable of tokens into its vector representation\n",
    "    (a paragraph vector).\n",
    "\n",
    "    Experimental. This will return something close to a tf-idf\n",
    "    weighted average of constituent token vectors by fitting\n",
    "    rare words (with low word bias values) more closely.\n",
    "    \"\"\"\n",
    "\n",
    "    if glove_data.word_vectors is None:\n",
    "        raise Exception('Model must be fit to transform paragraphs')\n",
    "\n",
    "    if glove_data.dictionary is None:\n",
    "        raise Exception('Dictionary must be provided to '\n",
    "                        'transform paragraphs')\n",
    "\n",
    "    cooccurrence = collections.defaultdict(lambda: 0.0)\n",
    "\n",
    "    for token in tokens:\n",
    "        try:\n",
    "            cooccurrence[glove_data.dictionary[token]] += glove_data.max_count / 10.0\n",
    "        except KeyError:\n",
    "            if not ignore_missing:\n",
    "                raise\n",
    "\n",
    "    random_state = glove.glove.check_random_state(glove_data.random_state)\n",
    "\n",
    "    word_ids = np.array(list(cooccurrence.keys()), dtype=np.int32)\n",
    "    values = np.array(list(cooccurrence.values()), dtype=np.float64)\n",
    "    shuffle_indices = np.arange(len(word_ids), dtype=np.int32)\n",
    "\n",
    "    # Initialize the vector to mean of constituent word vectors\n",
    "    paragraph_vector = np.mean(glove_data.word_vectors[word_ids], axis=0)\n",
    "    sum_gradients = np.ones_like(paragraph_vector)\n",
    "\n",
    "    # Shuffle the coocurrence matrix\n",
    "    random_state.shuffle(shuffle_indices)\n",
    "    transform_paragraph(glove_data.word_vectors,\n",
    "                        glove_data.word_biases,\n",
    "                        paragraph_vector,\n",
    "                        sum_gradients,\n",
    "                        word_ids,\n",
    "                        values,\n",
    "                        shuffle_indices,\n",
    "                        glove_data.learning_rate,\n",
    "                        glove_data.max_count,\n",
    "                        glove_data.alpha,\n",
    "                        epochs)\n",
    "\n",
    "    return paragraph_vector\n",
    "    \n",
    "    \n",
    "# get all tweets\n",
    "def import_text(tweets):\n",
    "    return [get_glove_vector(glove_data, tokenize(tweet)) for tweet in tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4743 tweets in 6251 lines\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2909: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:73: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['many', 'people', 'fight', 'for', 'change', 'in', 'dc', '@realdonaldtrump', 'is', 'a', 'leader', 'with', 'an', 'outsider', 's', 'perspective', 'the', 'vision', 'guts', 'energy', 'to', 'get', 'it', 'done']\n",
      "[-0.82055499 -0.49558452  0.21535475 -0.12559149 -0.56620563 -0.60028881\n",
      "  0.66788046  0.41637388  0.11512276  0.21439958 -0.19909359  0.16621002\n",
      "  0.38901462  0.14045758 -0.57522895 -0.26700974 -0.32056153 -0.03883236\n",
      "  0.66012547  0.42367811 -0.1288033  -0.14599726 -0.11942878  0.18576679\n",
      "  0.03294466 -0.14145208  0.13096304  0.07177186 -0.08580724  0.08425607\n",
      "  1.36460511  0.03456175  0.43834515  0.33595587 -0.25108404  0.31985529\n",
      " -0.31622282 -0.16803377 -0.1952103   0.14844919  0.58688626 -0.5395061\n",
      " -0.01686696 -0.09407228  0.13058426 -0.27060855  0.35660119 -0.07820809\n",
      "  0.03800161  0.02317981]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "################################################\n",
    "# get raw test data\n",
    "################################################\n",
    "import random\n",
    "\n",
    "# init\n",
    "TEST_RATIO = 0.1\n",
    "assert TEST_RATIO > 0 and TEST_RATIO < 1\n",
    "\n",
    "# get data\n",
    "text_handles, raw_tweets = parse_tweet_csv(\"train.csv\")\n",
    "handles = int_labels(text_handles)\n",
    "tweets = import_text(raw_tweets)   \n",
    "data_vector_size = len(tweets[0])\n",
    "\n",
    "### validation\n",
    "for i in range(1):\n",
    "    tweet = raw_tweets[random.randint(0, len(raw_tweets))]\n",
    "    print(tokenize(tweet))\n",
    "    print(get_glove_vector(glove_data, tokenize(tweet)))\n",
    "    print()\n",
    "# for handle in int_labels(handles[0:7]):\n",
    "#     print(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Separated into 4208 train and 474 test (9%)\n",
      "\n",
      "   handle  length                                         tweet_data\n",
      "0       0      50  [47053, 61529, 70695, 72153, 53029, 56239, 805...\n",
      "1       0      50  [49798, 74523, 52005, 79781, 55897, 71292, 796...\n",
      "2       0      50  [53003, 58349, 64302, 69018, 50697, 54404, 784...\n",
      "3       0      50  [59000, 67573, 57807, 79910, 56636, 60197, 727...\n",
      "4       0      50  [51051, 53072, 66876, 69405, 52779, 54383, 796...\n"
     ]
    }
   ],
   "source": [
    "################################################\n",
    "# split test data into train and test\n",
    "################################################\n",
    "import pandas as pd\n",
    "\n",
    "LABEL = 'handle'\n",
    "DATA = 'tweet_data'\n",
    "LENGTH = 'length'\n",
    "\n",
    "# we get floats from glove, so we need to convert them to integers.\n",
    "# the glove floats are both positive and negative, and I'm not sure\n",
    "# what the min/max are, so we normalize over that value, and then\n",
    "# scale them to a particular granularity\n",
    "# TODO how do we give floats to tensorflow?\n",
    "# TODO what's the min/max we'll get from this glove library?\n",
    "\n",
    "# this is so the glove output is in integer form (maybe not necessary)\n",
    "FLOAT_GRANULARITY = (1 << 16)\n",
    "VOCAB_SIZE = 2 * FLOAT_GRANULARITY + 1 # +/- and exclusive\n",
    "\n",
    "# this is so we can define a range for the values\n",
    "value_max = 0\n",
    "for tweet in tweets:\n",
    "    for x in tweet:\n",
    "        if abs(x) > value_max: value_max = abs(x)\n",
    "\n",
    "# split into test and train\n",
    "train_labels, train_data, test_labels, test_data = list(), list(), list(), list()\n",
    "for handle, tweet in zip(handles, tweets):\n",
    "    if np.isnan(tweet[0]): continue #a row of all nan's happens with data that glove can't understand (like, all hashtags)\n",
    "    tweet = list(map(lambda x: int(x / value_max * FLOAT_GRANULARITY + FLOAT_GRANULARITY), tweet))\n",
    "    if random.random() < TEST_RATIO:\n",
    "        test_labels.append(handle)\n",
    "        test_data.append(tweet)\n",
    "    else:\n",
    "        train_labels.append(handle)\n",
    "        train_data.append(tweet)\n",
    "\n",
    "# document and validate\n",
    "print(\"Separated into {} train and {} test ({}%)\\n\".format(len(train_data), len(test_data), \n",
    "                                                         int(100.0 * len(test_data) / len(raw_tweets))))\n",
    "assert len(train_labels) == len(train_data) and len(train_data) > 0\n",
    "assert len(test_labels) == len(test_data) and len(test_data) > 0\n",
    "assert len(test_labels) > len(tweets) * (TEST_RATIO - .05)\n",
    "assert len(test_labels) < len(tweets) * (TEST_RATIO + .05) \n",
    "\n",
    "# save to dataframe\n",
    "train = pd.DataFrame({\n",
    "    LABEL: train_labels,\n",
    "    DATA: train_data,\n",
    "    LENGTH: [data_vector_size for _ in range(len(train_data))]\n",
    "})\n",
    "test = pd.DataFrame({\n",
    "    LABEL: test_labels,\n",
    "    DATA: test_data,\n",
    "    LENGTH: [data_vector_size for _ in range(len(test_data))]\n",
    "})\n",
    "print(train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "################################################\n",
    "# initializing our tensor\n",
    "#\n",
    "# based off of blogpost david parks showed us:\n",
    "# https://r2rt.com/recurrent-neural-networks-in-tensorflow-iii-variable-length-sequences.html\n",
    "################################################\n",
    "import tensorflow as tf\n",
    " \n",
    " \n",
    "class DataIterator():\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.size = len(self.df)\n",
    "        self.epochs = 0\n",
    "        self.shuffle()\n",
    "\n",
    "    def shuffle(self):\n",
    "        self.df = self.df.sample(frac=1).reset_index(drop=True)\n",
    "        self.cursor = 0\n",
    "\n",
    "    def next_batch(self, n):\n",
    "        if self.cursor + n - 1 > self.size:\n",
    "            self.epochs += 1\n",
    "            self.shuffle()\n",
    "        res = self.df.ix[self.cursor:self.cursor + n - 1]\n",
    "        start_idx = self.cursor\n",
    "        self.cursor += n\n",
    "        # return res[DATA], res[LABEL], res[LENGTH]\n",
    "        # the above line fails.  an error is thrown when tf attempts to call np.asarray on this\n",
    "        # what is different about how our data is organized compared to the blog post this came from?\n",
    "        # TODO \n",
    "        data = res[DATA]\n",
    "        labels = res[LABEL]\n",
    "        length = res[LENGTH]\n",
    "        return np.asarray([data[i] for i in range(start_idx, start_idx + len(data))]), \\\n",
    "               np.asarray([labels[i] for i in range(start_idx, start_idx + len(labels))]), \\\n",
    "               np.asarray([length[i] for i in range(start_idx, start_idx + len(length))])\n",
    "\n",
    "\n",
    "def reset_graph():\n",
    "    if 'sess' in globals() and sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "\n",
    "def build_graph(vocab_size = VOCAB_SIZE, state_size = 64, batch_size = 256, num_classes = 2):\n",
    "\n",
    "    reset_graph()\n",
    "\n",
    "    # Placeholders\n",
    "    x = tf.placeholder(tf.int32, [batch_size, None]) # [batch_size, num_steps]\n",
    "    seqlen = tf.placeholder(tf.int32, [batch_size])\n",
    "    y = tf.placeholder(tf.int32, [batch_size])\n",
    "    keep_prob = tf.placeholder_with_default(1.0, [])\n",
    "\n",
    "    # Embedding layer\n",
    "    embeddings = tf.get_variable('embedding_matrix', [vocab_size, state_size])\n",
    "    rnn_inputs = tf.nn.embedding_lookup(embeddings, x)\n",
    "\n",
    "    # RNN\n",
    "    cell = tf.nn.rnn_cell.GRUCell(state_size)\n",
    "    init_state = tf.get_variable('init_state', [1, state_size],\n",
    "                                 initializer=tf.constant_initializer(0.0))\n",
    "    init_state = tf.tile(init_state, [batch_size, 1])\n",
    "    rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, rnn_inputs, sequence_length=seqlen,\n",
    "                                                 initial_state=init_state)\n",
    "\n",
    "    # Add dropout, as the model otherwise quickly overfits\n",
    "    rnn_outputs = tf.nn.dropout(rnn_outputs, keep_prob)\n",
    "    idx = tf.range(batch_size)*tf.shape(rnn_outputs)[1] + (seqlen - 1)\n",
    "    last_rnn_output = tf.gather(tf.reshape(rnn_outputs, [-1, state_size]), idx)\n",
    "\n",
    "    # Softmax layer\n",
    "    with tf.variable_scope('softmax'):\n",
    "        W = tf.get_variable('W', [state_size, num_classes])\n",
    "        b = tf.get_variable('b', [num_classes], initializer=tf.constant_initializer(0.0))\n",
    "    logits = tf.matmul(last_rnn_output, W) + b\n",
    "    preds = tf.nn.softmax(logits)\n",
    "    correct = tf.equal(tf.cast(tf.argmax(preds,1),tf.int32), y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "\n",
    "    return {\n",
    "        'x': x,\n",
    "        'seqlen': seqlen,\n",
    "        'y': y,\n",
    "        'dropout': keep_prob,\n",
    "        'loss': loss,\n",
    "        'ts': train_step,\n",
    "        'preds': preds,\n",
    "        'accuracy': accuracy\n",
    "    }\n",
    "\n",
    "\n",
    "def train_graph(g, batch_size = 256, num_epochs = 10, iterator = DataIterator):\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        tr = iterator(train)\n",
    "        te = iterator(test)\n",
    "\n",
    "        step, accuracy = 0, 0\n",
    "        tr_losses, te_losses = [], []\n",
    "        current_epoch = 0\n",
    "        while current_epoch < num_epochs:\n",
    "            step += 1\n",
    "            batch = tr.next_batch(batch_size)\n",
    "            feed = {g['x']: batch[0], g['y']: batch[1], g['seqlen']: batch[2], g['dropout']: 0.6}\n",
    "            accuracy_, _ = sess.run([g['accuracy'], g['ts']], feed_dict=feed)\n",
    "            accuracy += accuracy_\n",
    "\n",
    "            if tr.epochs > current_epoch:\n",
    "                current_epoch += 1\n",
    "                tr_losses.append(accuracy / step)\n",
    "                step, accuracy = 0, 0\n",
    "\n",
    "                #eval test set\n",
    "                te_epoch = te.epochs\n",
    "                while te.epochs == te_epoch:\n",
    "                    step += 1\n",
    "                    batch = te.next_batch(batch_size)\n",
    "                    feed = {g['x']: batch[0], g['y']: batch[1], g['seqlen']: batch[2]}\n",
    "                    accuracy_ = sess.run([g['accuracy']], feed_dict=feed)[0]\n",
    "                    accuracy += accuracy_\n",
    "\n",
    "                te_losses.append(accuracy / step)\n",
    "                step, accuracy = 0,0\n",
    "                print(\"Accuracy after epoch\", current_epoch, \" - tr:\", tr_losses[-1], \"- te:\", te_losses[-1])\n",
    "\n",
    "    return tr_losses, te_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequences:\n",
      " <class 'numpy.ndarray'>: \n",
      "[[ 59307  67832  69894  71811  55194  61203  71748  63810  61938  67070\n",
      "   55971  73352  73552  63955  56481  70901  57663  59868  74552  77380\n",
      "   67239  70059  57878  62387  65811  59532  76462  60725  63047  68059\n",
      "   97946  67823  65116  69679  65184  62384  67223  66628  57911  67103\n",
      "   74532  61186  69464  76125  64127  55791  68399  59934  72789  75496]\n",
      " [ 47812  58227  70627  70276  52605  58742  85230  71227  79415  69355\n",
      "   63672  64762  81166  74002  52488  56576  59570  66428  74224  73253\n",
      "   60287  64864  56538  64835  63677  54904  80036  60564  57238  76013\n",
      "  106152  65548  78869  76452  53142  70358  56696  51331  52847  74627\n",
      "   77331  55113  70225  60478  69339  63784  75125  69297  64880  64063]\n",
      " [ 51126  68779  63033  79120  50224  62527  81236  78994  66734  67554\n",
      "   53237  68688  78858  77732  52395  59008  61834  66042  73977  75371\n",
      "   69564  72262  55745  75978  58142  53854  79121  57730  57703  67146\n",
      "  106723  61506  72561  78052  74638  70850  64271  57468  52367  72386\n",
      "   74997  66483  61410  69894  77136  56901  66144  62218  73012  69874]]\n",
      " <class 'numpy.ndarray'>: \n",
      "[59307 67832 69894 71811 55194 61203 71748 63810 61938 67070 55971 73352\n",
      " 73552 63955 56481 70901 57663 59868 74552 77380 67239 70059 57878 62387\n",
      " 65811 59532 76462 60725 63047 68059 97946 67823 65116 69679 65184 62384\n",
      " 67223 66628 57911 67103 74532 61186 69464 76125 64127 55791 68399 59934\n",
      " 72789 75496]\n",
      " <class 'numpy.int32'>: \n",
      "59307\n",
      "\n",
      "\n",
      "Target values\n",
      " <class 'numpy.ndarray'>: \n",
      "[0 0 0]\n",
      " <class 'numpy.int64'>: \n",
      "0\n",
      "\n",
      "\n",
      "Sequence lengths\n",
      " <class 'numpy.ndarray'>: \n",
      "[50 50 50]\n",
      " <class 'numpy.int64'>: \n",
      "50\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "################################################\n",
    "# explore our data iterator\n",
    "################################################\n",
    "\n",
    "# validate data iterator\n",
    "d = DataIterator(test).next_batch(3)\n",
    "print('Input sequences:\\n', \n",
    "      \"{}: \\n{}\\n\".format(type(d[0]), d[0]), \n",
    "      \"{}: \\n{}\\n\".format(type(d[0][0]), d[0][0]), \n",
    "      \"{}: \\n{}\\n\".format(type(d[0][0][0]), d[0][0][0]), \n",
    "      end='\\n\\n')\n",
    "print('Target values\\n', \n",
    "      \"{}: \\n{}\\n\".format(type(d[1]), d[1]), \n",
    "      \"{}: \\n{}\\n\".format(type(d[1][0]), d[1][0]), \n",
    "      end='\\n\\n')\n",
    "print('Sequence lengths\\n', \n",
    "      \"{}: \\n{}\\n\".format(type(d[2]), d[2]), \n",
    "      \"{}: \\n{}\\n\".format(type(d[2][0]), d[2][0]), \n",
    "      end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after epoch 1  - tr: 0.506663602941 - te: 0.51953125\n",
      "Accuracy after epoch 2  - tr: 0.728515625 - te: 0.45703125\n",
      "Accuracy after epoch 3  - tr: 0.74072265625 - te: 0.48828125\n",
      "Accuracy after epoch 4  - tr: 0.796142578125 - te: 0.47265625\n",
      "Accuracy after epoch 5  - tr: 0.906005859375 - te: 0.4609375\n",
      "Accuracy after epoch 6  - tr: 0.965576171875 - te: 0.5\n",
      "Accuracy after epoch 7  - tr: 0.962890625 - te: 0.48828125\n",
      "Accuracy after epoch 8  - tr: 0.994140625 - te: 0.44921875\n",
      "Accuracy after epoch 9  - tr: 0.95166015625 - te: 0.5234375\n",
      "Accuracy after epoch 10  - tr: 0.904052734375 - te: 0.515625\n"
     ]
    }
   ],
   "source": [
    "################################################\n",
    "# run it!\n",
    "################################################\n",
    "\n",
    "# this fails, just like us\n",
    "g = build_graph()\n",
    "tr_losses, te_losses = train_graph(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: we're overfitting, why?\n",
    "\n",
    "#TODO: try something better than averaging the word vec values\n",
    "# maybe we could do three-d arrays?  encode each word and pad the data\n",
    "\n",
    "#TODO: we strip a lot away (ie punctuation, smilies) and lose other\n",
    "# data to glove (#hashtags, @handles). how can we keep this?\n",
    "\n",
    "#TODO: how do we run this on the test dataset?"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
