{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('tests', 0.9164356610046378), ('testing', 0.81992214225169979), ('tested', 0.74428756870386203), ('final', 0.69102279899050834), ('taking', 0.68790020450861178), ('results', 0.68468909013686308), ('match', 0.67696818430673733), ('determine', 0.67679768970507126), ('challenge', 0.67477055567439548)]\n"
     ]
    }
   ],
   "source": [
    "################################################\n",
    "# intialize glove\n",
    "################################################\n",
    "# >pip install glove_python\n",
    "# >wget http://www.google.com/search?q=glove+word+embeddings+download+those+dope+pre+trained+vectors\n",
    "# >unzip dope_glove_shit.zip\n",
    "# >cp cmps242_hw5_config.py.example cmps242_hw5_config.py\n",
    "# >echo \"set GLOVE_LOCATION in cmps242_hw5_config.py to one of those files\"\n",
    "################################################\n",
    "from cmps242_hw5_config import *\n",
    "from glove import Glove\n",
    "\n",
    "glove_data = Glove.load_stanford(GLOVE_LOCATION)\n",
    "\n",
    "print(glove_data.most_similar('test', number=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "################################################\n",
    "# file parsing functions\n",
    "################################################\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import string, re\n",
    "import collections\n",
    "import numpy as np\n",
    "import glove\n",
    "from glove.glove_cython import fit_vectors, transform_paragraph\n",
    "\n",
    "# definitions\n",
    "HC=\"HillaryClinton\"\n",
    "DT=\"realDonaldTrump\"\n",
    "NA=\"none\"\n",
    "HANDLES = [HC,DT,NA]\n",
    "HANDLE_MAP = {NA:-1, HC:0, DT:1}\n",
    "\n",
    "# read csv file, return handles and tweets\n",
    "def parse_tweet_csv(file, file_encoding=\"utf8\"):\n",
    "    # init\n",
    "    handles, tweets = [], []\n",
    "    \n",
    "    # read file\n",
    "    linenr = -1\n",
    "    with open(file, encoding=file_encoding) as input:\n",
    "        try:\n",
    "            for line in input:\n",
    "                linenr += 1\n",
    "                if linenr == 0: continue\n",
    "                \n",
    "                # get contents\n",
    "                line = line.split(\",\")\n",
    "                if line[0] in HANDLES: #label and irst line of tweet\n",
    "                    handles.append(line[0])\n",
    "                    tweet = ','.join(line[1:])\n",
    "                    tweets.append(tweet)\n",
    "                else: #second+ line of tweet\n",
    "                    tweet = tweets.pop()\n",
    "                    tweet += ','.join(line)\n",
    "                    tweets.append(tweet)\n",
    "        except Exception as e:\n",
    "            print(\"Exception at line {}: {}\".format(linenr, e))\n",
    "            raise e\n",
    "    \n",
    "    # sanity checks\n",
    "    assert len(handles) == len(tweets)\n",
    "    print(\"Found {} tweets in {} lines\".format(len(tweets), linenr + 1))\n",
    "    \n",
    "    # return data\n",
    "    return handles, tweets\n",
    "\n",
    "\n",
    "##########################################\n",
    "### coverting tweet strings to numbers ###\n",
    "\n",
    "# coverting labels to integers\n",
    "def int_labels(labels):\n",
    "    return list(map(lambda x: HANDLE_MAP[x], labels))\n",
    "\n",
    "#tokenizing\n",
    "_tokenizer = TweetTokenizer()\n",
    "_punctuation = set(string.punctuation)\n",
    "def tokenize(tweet, lowercase=True, strip_urls=True, strip_punctuation=True):\n",
    "    tokens = _tokenizer.tokenize(tweet)\n",
    "    if lowercase: tokens = list(map(lambda x: x.lower(), tokens))\n",
    "    if strip_urls: tokens = list(filter(lambda x: not x.startswith(\"http\"), tokens))\n",
    "    if strip_punctuation: #https://stackoverflow.com/questions/265960/best-way-to-strip-punctuation-from-a-string-in-python\n",
    "        tokens = list(filter(lambda x: x.startswith(u'@') or x.startswith(u'#') or x not in _punctuation and not re.match(u\"[^\\w\\d'\\s$]+\", x), tokens))\n",
    "    return tokens\n",
    "\n",
    "# glove information\n",
    "def get_glove_vector(glove_data, tokens, epochs=50, ignore_missing=True):\n",
    "    \"\"\"\n",
    "    tpesout: This code came from the 'glove' repo I'm using (but had a bug, so I needed to slighly modify it)\n",
    "    https://github.com/maciejkula/glove-python/blob/master/glove/glove.py\n",
    "    \n",
    "    Transform an iterable of tokens into its vector representation\n",
    "    (a paragraph vector).\n",
    "\n",
    "    Experimental. This will return something close to a tf-idf\n",
    "    weighted average of constituent token vectors by fitting\n",
    "    rare words (with low word bias values) more closely.\n",
    "    \"\"\"\n",
    "\n",
    "    if glove_data.word_vectors is None:\n",
    "        raise Exception('Model must be fit to transform paragraphs')\n",
    "\n",
    "    if glove_data.dictionary is None:\n",
    "        raise Exception('Dictionary must be provided to '\n",
    "                        'transform paragraphs')\n",
    "\n",
    "    cooccurrence = collections.defaultdict(lambda: 0.0)\n",
    "\n",
    "    for token in tokens:\n",
    "        try:\n",
    "            cooccurrence[glove_data.dictionary[token]] += glove_data.max_count / 10.0\n",
    "        except KeyError:\n",
    "            if not ignore_missing:\n",
    "                raise\n",
    "\n",
    "    random_state = glove.glove.check_random_state(glove_data.random_state)\n",
    "\n",
    "    word_ids = np.array(list(cooccurrence.keys()), dtype=np.int32)\n",
    "    values = np.array(list(cooccurrence.values()), dtype=np.float64)\n",
    "    shuffle_indices = np.arange(len(word_ids), dtype=np.int32)\n",
    "\n",
    "    # Initialize the vector to mean of constituent word vectors\n",
    "    paragraph_vector = np.mean(glove_data.word_vectors[word_ids], axis=0)\n",
    "    sum_gradients = np.ones_like(paragraph_vector)\n",
    "\n",
    "    # Shuffle the coocurrence matrix\n",
    "    random_state.shuffle(shuffle_indices)\n",
    "    transform_paragraph(glove_data.word_vectors,\n",
    "                        glove_data.word_biases,\n",
    "                        paragraph_vector,\n",
    "                        sum_gradients,\n",
    "                        word_ids,\n",
    "                        values,\n",
    "                        shuffle_indices,\n",
    "                        glove_data.learning_rate,\n",
    "                        glove_data.max_count,\n",
    "                        glove_data.alpha,\n",
    "                        epochs)\n",
    "\n",
    "    return paragraph_vector\n",
    "    \n",
    "    \n",
    "# get all tweets\n",
    "def import_text(tweets):\n",
    "    return [get_glove_vector(glove_data, tokenize(tweet)) for tweet in tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4743 tweets in 6251 lines\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2909: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:73: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ted', 'cruz', 'attacked', 'new', 'yorkers', 'and', 'new', 'york', 'values', 'we', \"don't\", 'forget']\n",
      "[-0.24999201 -0.04778582  0.29153717  0.04027289 -0.18036391  0.12771958\n",
      " -0.03715405  0.34251726  0.2599375   0.24550271 -0.0627659  -0.27238753\n",
      "  0.49486947  0.14592573 -0.01369054 -0.24762663 -0.09457744 -0.00466164\n",
      "  0.22792134  0.29722554 -0.32670197 -0.37924141 -0.02137749 -0.06018623\n",
      "  0.23723187 -0.45057317  0.19107633  0.01940349 -0.065164    0.10058307\n",
      "  1.1994636   0.03369799  0.2032412   0.21097079 -0.21799111  0.10030391\n",
      "  0.10109712 -0.05194849 -0.05891673 -0.02537707  0.21913774  0.05949629\n",
      " -0.40151552  0.15616248  0.02312383 -0.26925013  0.16723349 -0.03448194\n",
      "  0.18492417  0.02967186]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "################################################\n",
    "# get raw test data\n",
    "################################################\n",
    "import random\n",
    "\n",
    "# init\n",
    "TEST_RATIO = 0.1\n",
    "assert TEST_RATIO > 0 and TEST_RATIO < 1\n",
    "\n",
    "# get data\n",
    "text_handles, raw_tweets = parse_tweet_csv(\"train.csv\")\n",
    "handles = int_labels(text_handles)\n",
    "tweets = import_text(raw_tweets)   \n",
    "data_vector_size = len(tweets[0])\n",
    "\n",
    "### validation\n",
    "for i in range(1):\n",
    "    tweet = raw_tweets[random.randint(0, len(raw_tweets))]\n",
    "    print(tokenize(tweet))\n",
    "    print(get_glove_vector(glove_data, tokenize(tweet)))\n",
    "    print()\n",
    "# for handle in int_labels(handles[0:7]):\n",
    "#     print(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Separated into 4207 train and 475 test (10%)\n",
      "\n",
      "   handle  length                                         tweet_data\n",
      "0       0      50  [60877, 65038, 66615, 67433, 62477, 63097, 690...\n",
      "1       0      50  [62564, 64808, 63911, 67661, 63559, 64651, 685...\n",
      "2       0      50  [61830, 67720, 62425, 69089, 63148, 66673, 688...\n",
      "3       0      50  [63450, 63290, 65418, 66544, 62071, 62876, 688...\n",
      "4       1      50  [62199, 63505, 66809, 65194, 61459, 63717, 670...\n"
     ]
    }
   ],
   "source": [
    "################################################\n",
    "# split test data into train and test\n",
    "################################################\n",
    "import pandas as pd\n",
    "\n",
    "LABEL = 'handle'\n",
    "DATA = 'tweet_data'\n",
    "LENGTH = 'length'\n",
    "\n",
    "# we get floats from glove, so we need to convert them to integers.\n",
    "# the glove floats are both positive and negative, and I'm not sure\n",
    "# what the min/max are, so we normalize over that value, and then\n",
    "# scale them to a particular granularity\n",
    "# TODO how do we give floats to tensorflow?\n",
    "# TODO what's the min/max we'll get from this glove library?\n",
    "\n",
    "# this is so the glove output is in integer form (maybe not necessary)\n",
    "FLOAT_GRANULARITY = (1 << 16)\n",
    "VOCAB_SIZE = 2 * FLOAT_GRANULARITY + 1 # +/- and exclusive\n",
    "\n",
    "# this is so we can define a range for the values\n",
    "# value_max = 0\n",
    "# for tweet in tweets:\n",
    "#     for x in tweet:\n",
    "#         if abs(x) > value_max: value_max = abs(x)\n",
    "# print(\"Got max value of {}\".format(value_max))\n",
    "value_max = 10 # so we're not dependent on what the data is\n",
    "\n",
    "# split into test and train\n",
    "train_labels, train_data, test_labels, test_data = list(), list(), list(), list()\n",
    "for handle, tweet in zip(handles, tweets):\n",
    "    if np.isnan(tweet[0]): continue #a row of all nan's happens with data that glove can't understand (like, all hashtags)\n",
    "    tweet = list(map(lambda x: int(x / value_max * FLOAT_GRANULARITY + FLOAT_GRANULARITY), tweet))\n",
    "    if random.random() < TEST_RATIO:\n",
    "        test_labels.append(handle)\n",
    "        test_data.append(tweet)\n",
    "    else:\n",
    "        train_labels.append(handle)\n",
    "        train_data.append(tweet)\n",
    "\n",
    "# document and validate\n",
    "print(\"Separated into {} train and {} test ({}%)\\n\".format(len(train_data), len(test_data), \n",
    "                                                         int(100.0 * len(test_data) / len(raw_tweets))))\n",
    "assert len(train_labels) == len(train_data) and len(train_data) > 0\n",
    "assert len(test_labels) == len(test_data) and len(test_data) > 0\n",
    "assert len(test_labels) > len(tweets) * (TEST_RATIO - .05)\n",
    "assert len(test_labels) < len(tweets) * (TEST_RATIO + .05) \n",
    "\n",
    "# save to dataframe\n",
    "train = pd.DataFrame({\n",
    "    LABEL: train_labels,\n",
    "    DATA: train_data,\n",
    "    LENGTH: [data_vector_size for _ in range(len(train_data))]\n",
    "})\n",
    "test = pd.DataFrame({\n",
    "    LABEL: test_labels,\n",
    "    DATA: test_data,\n",
    "    LENGTH: [data_vector_size for _ in range(len(test_data))]\n",
    "})\n",
    "print(train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "################################################\n",
    "# initializing our tensor\n",
    "#\n",
    "# based off of blogpost david parks showed us:\n",
    "# https://r2rt.com/recurrent-neural-networks-in-tensorflow-iii-variable-length-sequences.html\n",
    "################################################\n",
    "import tensorflow as tf\n",
    " \n",
    " \n",
    "class DataIterator():\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.size = len(self.df)\n",
    "        self.epochs = 0\n",
    "        self.shuffle()\n",
    "\n",
    "    def shuffle(self):\n",
    "        self.df = self.df.sample(frac=1).reset_index(drop=True)\n",
    "        self.cursor = 0\n",
    "\n",
    "    def next_batch(self, n):\n",
    "        if self.cursor + n - 1 > self.size:\n",
    "            self.epochs += 1\n",
    "            self.shuffle()\n",
    "        res = self.df.ix[self.cursor:self.cursor + n - 1]\n",
    "        start_idx = self.cursor\n",
    "        self.cursor += n\n",
    "        # return res[DATA], res[LABEL], res[LENGTH]\n",
    "        # the above line fails.  an error is thrown when tf attempts to call np.asarray on this\n",
    "        # what is different about how our data is organized compared to the blog post this came from?\n",
    "        # TODO \n",
    "        data = res[DATA]\n",
    "        labels = res[LABEL]\n",
    "        length = res[LENGTH]\n",
    "        return np.asarray([data[i] for i in range(start_idx, start_idx + len(data))]), \\\n",
    "               np.asarray([labels[i] for i in range(start_idx, start_idx + len(labels))]), \\\n",
    "               np.asarray([length[i] for i in range(start_idx, start_idx + len(length))])\n",
    "\n",
    "\n",
    "def reset_graph():\n",
    "    if 'sess' in globals() and sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "\n",
    "def build_graph(vocab_size = VOCAB_SIZE, state_size = 64, batch_size = 256, num_classes = 2):\n",
    "\n",
    "    reset_graph()\n",
    "\n",
    "    # Placeholders\n",
    "    x = tf.placeholder(tf.int32, [batch_size, None]) # [batch_size, num_steps]\n",
    "    seqlen = tf.placeholder(tf.int32, [batch_size])\n",
    "    y = tf.placeholder(tf.int32, [batch_size])\n",
    "    keep_prob = tf.placeholder_with_default(1.0, [])\n",
    "\n",
    "    # Embedding layer\n",
    "    embeddings = tf.get_variable('embedding_matrix', [vocab_size, state_size])\n",
    "    rnn_inputs = tf.nn.embedding_lookup(embeddings, x)\n",
    "\n",
    "    # RNN\n",
    "    cell = tf.nn.rnn_cell.GRUCell(state_size)\n",
    "    init_state = tf.get_variable('init_state', [1, state_size],\n",
    "                                 initializer=tf.constant_initializer(0.0))\n",
    "    init_state = tf.tile(init_state, [batch_size, 1])\n",
    "    rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, rnn_inputs, sequence_length=seqlen,\n",
    "                                                 initial_state=init_state)\n",
    "\n",
    "    # Add dropout, as the model otherwise quickly overfits\n",
    "    rnn_outputs = tf.nn.dropout(rnn_outputs, keep_prob)\n",
    "    idx = tf.range(batch_size)*tf.shape(rnn_outputs)[1] + (seqlen - 1)\n",
    "    # last_rnn_output = tf.gather(tf.reshape(rnn_outputs, [-1, state_size]), idx)\n",
    "    # last_rnn_output = tf.gather_nd(rnn_outputs, tf.pack([tf.range(batch_size), seqlen-1], axis=1))\n",
    "    last_rnn_output = tf.gather_nd(rnn_outputs, tf.stack([tf.range(batch_size), seqlen-1], axis=1))\n",
    "\n",
    "    # Softmax layer\n",
    "    with tf.variable_scope('softmax'):\n",
    "        W = tf.get_variable('W', [state_size, num_classes]) # weights?\n",
    "        b = tf.get_variable('b', [num_classes], initializer=tf.constant_initializer(0.0)) # bias?\n",
    "    logits = tf.matmul(last_rnn_output, W) + b\n",
    "    preds = tf.nn.softmax(logits)\n",
    "    correct = tf.equal(tf.cast(tf.argmax(preds,1),tf.int32), y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "\n",
    "    return {\n",
    "        'x': x,\n",
    "        'seqlen': seqlen,\n",
    "        'y': y,\n",
    "        'dropout': keep_prob,\n",
    "        'loss': loss,\n",
    "        'ts': train_step,\n",
    "        'preds': preds,\n",
    "        'accuracy': accuracy\n",
    "    }\n",
    "\n",
    "\n",
    "def train_graph(g, batch_size = 256, num_epochs = 10, iterator = DataIterator):\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        tr = iterator(train)\n",
    "        te = iterator(test)\n",
    "\n",
    "        step, accuracy = 0, 0\n",
    "        tr_losses, te_losses = [], []\n",
    "        current_epoch = 0\n",
    "        while current_epoch < num_epochs:\n",
    "            step += 1\n",
    "            batch = tr.next_batch(batch_size)\n",
    "            feed = {g['x']: batch[0], g['y']: batch[1], g['seqlen']: batch[2], g['dropout']: 0.6}\n",
    "            accuracy_, _ = sess.run([g['accuracy'], g['ts']], feed_dict=feed)\n",
    "            accuracy += accuracy_\n",
    "\n",
    "            if tr.epochs > current_epoch:\n",
    "                current_epoch += 1\n",
    "                tr_losses.append(accuracy / step)\n",
    "                step, accuracy = 0, 0\n",
    "\n",
    "                #eval test set\n",
    "                te_epoch = te.epochs\n",
    "                while te.epochs == te_epoch:\n",
    "                    step += 1\n",
    "                    batch = te.next_batch(batch_size)\n",
    "                    feed = {g['x']: batch[0], g['y']: batch[1], g['seqlen']: batch[2]}\n",
    "                    accuracy_ = sess.run([g['accuracy']], feed_dict=feed)[0]\n",
    "                    accuracy += accuracy_\n",
    "\n",
    "                te_losses.append(accuracy / step)\n",
    "                step, accuracy = 0,0\n",
    "                print(\"Accuracy after epoch\", current_epoch, \" - tr:\", tr_losses[-1], \"- te:\", te_losses[-1])\n",
    "\n",
    "    return tr_losses, te_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequences:\n",
      " <class 'numpy.ndarray'>: \n",
      "[[63083 62809 63238 65074 63259 65533 69595 68583 64593 64495 63189 63332\n",
      "  68032 67229 63046 63006 63707 65324 65314 68144 64139 62602 64148 64270\n",
      "  63712 64170 67183 67964 63575 68440 77464 63374 68288 70191 66224 67506\n",
      "  65609 62057 67385 67881 66614 63921 62023 63400 65382 63321 69458 64712\n",
      "  68493 64078]\n",
      " [66072 64795 68072 67466 63942 64318 66623 66561 64551 67390 64989 65861\n",
      "  67720 64664 64519 65487 64927 67535 66372 66259 63219 64680 65344 65783\n",
      "  66540 64483 66071 65946 64181 64482 72260 65807 64619 66030 65222 64664\n",
      "  66230 65206 65117 64878 68380 63717 64459 66224 65907 64114 65361 65657\n",
      "  65651 68956]\n",
      " [63525 66199 66836 65704 62121 64784 67612 66526 65712 67381 63721 66742\n",
      "  67800 67985 63032 65029 63823 64643 68472 69657 62934 65142 64658 67257\n",
      "  66305 64745 65390 64643 62513 67685 74517 65927 68455 67524 66925 65351\n",
      "  63771 63723 64279 69233 66907 63974 65994 67117 69135 62915 67221 67017\n",
      "  68150 66349]]\n",
      " <class 'numpy.ndarray'>: \n",
      "[63083 62809 63238 65074 63259 65533 69595 68583 64593 64495 63189 63332\n",
      " 68032 67229 63046 63006 63707 65324 65314 68144 64139 62602 64148 64270\n",
      " 63712 64170 67183 67964 63575 68440 77464 63374 68288 70191 66224 67506\n",
      " 65609 62057 67385 67881 66614 63921 62023 63400 65382 63321 69458 64712\n",
      " 68493 64078]\n",
      " <class 'numpy.int32'>: \n",
      "63083\n",
      "\n",
      "\n",
      "Target values\n",
      " <class 'numpy.ndarray'>: \n",
      "[0 1 1]\n",
      " <class 'numpy.int64'>: \n",
      "0\n",
      "\n",
      "\n",
      "Sequence lengths\n",
      " <class 'numpy.ndarray'>: \n",
      "[50 50 50]\n",
      " <class 'numpy.int64'>: \n",
      "50\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "################################################\n",
    "# explore our data iterator\n",
    "################################################\n",
    "\n",
    "# validate data iterator\n",
    "d = DataIterator(test).next_batch(3)\n",
    "print('Input sequences:\\n', \n",
    "      \"{}: \\n{}\\n\".format(type(d[0]), d[0]), \n",
    "      \"{}: \\n{}\\n\".format(type(d[0][0]), d[0][0]), \n",
    "      \"{}: \\n{}\\n\".format(type(d[0][0][0]), d[0][0][0]), \n",
    "      end='\\n\\n')\n",
    "print('Target values\\n', \n",
    "      \"{}: \\n{}\\n\".format(type(d[1]), d[1]), \n",
    "      \"{}: \\n{}\\n\".format(type(d[1][0]), d[1][0]), \n",
    "      end='\\n\\n')\n",
    "print('Sequence lengths\\n', \n",
    "      \"{}: \\n{}\\n\".format(type(d[2]), d[2]), \n",
    "      \"{}: \\n{}\\n\".format(type(d[2][0]), d[2][0]), \n",
    "      end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after epoch 1  - tr: 0.506893382353 - te: 0.517578125\n",
      "Accuracy after epoch 2  - tr: 0.608642578125 - te: 0.5390625\n",
      "Accuracy after epoch 3  - tr: 0.6923828125 - te: 0.53515625\n",
      "Accuracy after epoch 4  - tr: 0.760498046875 - te: 0.53125\n",
      "Accuracy after epoch 5  - tr: 0.81591796875 - te: 0.51171875\n",
      "Accuracy after epoch 6  - tr: 0.87939453125 - te: 0.52734375\n",
      "Accuracy after epoch 7  - tr: 0.920166015625 - te: 0.51171875\n",
      "Accuracy after epoch 8  - tr: 0.92529296875 - te: 0.5\n",
      "Accuracy after epoch 9  - tr: 0.91259765625 - te: 0.48828125\n",
      "Accuracy after epoch 10  - tr: 0.87451171875 - te: 0.50390625\n"
     ]
    }
   ],
   "source": [
    "################################################\n",
    "# run it!\n",
    "################################################\n",
    "\n",
    "# this fails, just like us\n",
    "g = build_graph()\n",
    "tr_losses, te_losses = train_graph(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: we're overfitting, why?\n",
    "\n",
    "#TODO: try something better than averaging the word vec values\n",
    "# maybe we could do three-d arrays?  encode each word and pad the data\n",
    "\n",
    "#TODO: we strip a lot away (ie punctuation, smilies) and lose other\n",
    "# data to glove (#hashtags, @handles). how can we keep this?\n",
    "\n",
    "#TODO: how do we run this on the test dataset?"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
