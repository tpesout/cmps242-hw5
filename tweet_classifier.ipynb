{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manually importing glove vectors\n",
      "Imported 400000 words, with max glove val 5.4593\n"
     ]
    }
   ],
   "source": [
    "################################################\n",
    "# intialize glove\n",
    "################################################\n",
    "# >pip install glove_python\n",
    "# >wget http://www.google.com/search?q=glove+word+embeddings+download+those+dope+pre+trained+vectors\n",
    "# >unzip dope_glove_shit.zip\n",
    "# >cp cmps242_hw5_config.py.example cmps242_hw5_config.py\n",
    "# >echo \"set GLOVE_LOCATION in cmps242_hw5_config.py to one of those files\"\n",
    "################################################\n",
    "from cmps242_hw5_config import *\n",
    "import numpy as np\n",
    "\n",
    "print(\"Manually importing glove vectors\")\n",
    "\n",
    "# glove data\n",
    "glove_data = dict()\n",
    "max_glove_val = 0\n",
    "with open(GLOVE_LOCATION, encoding='utf8') as glovein:\n",
    "    for line in glovein:\n",
    "        line = line.split()\n",
    "        word = line[0]\n",
    "        vec = [float(line[i]) for i in range(1,len(line))]\n",
    "        for g in vec:\n",
    "            if abs(g) > max_glove_val: max_glove_val = abs(g)\n",
    "        glove_data[word] = vec\n",
    "    print(\"Imported {} words, with max glove val {}\".format(len(glove_data), max_glove_val))\n",
    "\n",
    "glove_dict_length = len(vec)\n",
    "def get_glove_vector(tokens, glove_information=glove_data, glove_vec_length=glove_dict_length):\n",
    "    temp_vec = [[0.0] for _ in range(glove_vec_length)]\n",
    "\n",
    "    for word in tokens:\n",
    "        if word not in glove_information: continue\n",
    "        glove_vec = glove_information[word]\n",
    "        for i in range(glove_vec_length):\n",
    "            temp_vec[i].append(glove_vec[i])\n",
    "    \n",
    "    vec = []\n",
    "    for i in range(glove_vec_length):\n",
    "        vec.append(np.mean(temp_vec[i]))\n",
    "        vec.append(np.std(temp_vec[i]))\n",
    "\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "################################################\n",
    "# file parsing functions\n",
    "################################################\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import string, re\n",
    "import collections\n",
    "import numpy as np\n",
    "import glove\n",
    "from glove.glove_cython import fit_vectors, transform_paragraph\n",
    "\n",
    "# definitions\n",
    "HC=\"HillaryClinton\"\n",
    "DT=\"realDonaldTrump\"\n",
    "NA=\"none\"\n",
    "HANDLES = [HC,DT,NA]\n",
    "HANDLE_MAP = {NA:-1, HC:0, DT:1}\n",
    "\n",
    "# read csv file, return handles and tweets\n",
    "def parse_tweet_csv(file, file_encoding=\"utf8\"):\n",
    "    # init\n",
    "    handles, tweets = [], []\n",
    "    \n",
    "    # read file\n",
    "    linenr = -1\n",
    "    with open(file, encoding=file_encoding) as input:\n",
    "        try:\n",
    "            for line in input:\n",
    "                linenr += 1\n",
    "                if linenr == 0: continue\n",
    "                \n",
    "                # get contents\n",
    "                line = line.split(\",\")\n",
    "                if line[0] in HANDLES: #label and irst line of tweet\n",
    "                    handles.append(line[0])\n",
    "                    tweet = ','.join(line[1:])\n",
    "                    tweets.append(tweet)\n",
    "                else: #second+ line of tweet\n",
    "                    tweet = tweets.pop()\n",
    "                    tweet += ','.join(line)\n",
    "                    tweets.append(tweet)\n",
    "        except Exception as e:\n",
    "            print(\"Exception at line {}: {}\".format(linenr, e))\n",
    "            raise e\n",
    "    \n",
    "    # sanity checks\n",
    "    assert len(handles) == len(tweets)\n",
    "    print(\"Found {} tweets in {} lines\".format(len(tweets), linenr + 1))\n",
    "    \n",
    "    # return data\n",
    "    return handles, tweets\n",
    "\n",
    "\n",
    "##########################################\n",
    "### coverting tweet strings to numbers ###\n",
    "\n",
    "# coverting labels to integers\n",
    "def int_labels(labels):\n",
    "    return list(map(lambda x: HANDLE_MAP[x], labels))\n",
    "\n",
    "#tokenizing\n",
    "_tokenizer = TweetTokenizer()\n",
    "_punctuation = set(string.punctuation)\n",
    "def tokenize(tweet, lowercase=True, strip_urls=True, strip_punctuation=True):\n",
    "    tokens = _tokenizer.tokenize(tweet)\n",
    "    if lowercase: tokens = list(map(lambda x: x.lower(), tokens))\n",
    "    if strip_urls: tokens = list(filter(lambda x: not x.startswith(\"http\"), tokens))\n",
    "    if strip_punctuation: #https://stackoverflow.com/questions/265960/best-way-to-strip-punctuation-from-a-string-in-python\n",
    "        tokens = list(filter(lambda x: x.startswith(u'@') or x.startswith(u'#') or x not in _punctuation and not re.match(u\"[^\\w\\d'\\s$]+\", x), tokens))\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# get all tweets\n",
    "def import_text(tweets):\n",
    "    return [get_glove_vector(tokenize(tweet)) for tweet in tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4743 tweets in 6251 lines\n",
      "['the', 'first', 'in', 'a', 'new', 'series', 'a', 'message', 'from', 'your', 'potential', 'next', 'president', 'on', 'pregnancy']\n",
      "[0.2992816875, 0.31553719851067452, 0.39200687499999998, 0.20576131146667095, -0.078937818749999986, 0.43497225748163271, 0.036343374999999997, 0.51416561809485017, 0.29410268750000002, 0.50567872089211929, 0.28678837500000004, 0.71390184460872097, -0.58061412499999998, 0.43955127129933247, -0.16351031249999998, 0.30235273835305321, 0.096402960624999992, 0.61435490196876263, -0.047386687500000003, 0.34502558174027448, -0.13158525000000004, 0.32708791771622731, -0.040319874999999998, 0.5057580890309461, -0.35178218750000001, 0.46987363940082588, 0.13452987500000002, 0.37077039688024499, 0.40985453125000004, 0.41723479237832428, 0.0028652499999999963, 0.4251665935958957, -0.25630468750000002, 0.46280584956095772, -0.17510508125000002, 0.38136119423626341, -0.49419000000000002, 0.74748610662673864, -0.16181806249999997, 0.42043256863474382, 0.23107999999999998, 0.32434362980564302, 0.044574374999999999, 0.37316919039774621, 0.045128143750000002, 0.41805842946811572, -0.019822212499999999, 0.41060906484864329, 0.030481543750000013, 0.39312426009915741, -1.6301100000000002, 0.65105316258543744, -0.22053387499999999, 0.45854353352515437, 0.0071105624999999902, 0.37477642269808153, 0.06580575000000001, 0.5846418668698492, -0.060255187500000001, 0.3879265548401557, 3.1629437500000002, 1.0617615404180629, 0.179160125, 0.54859220893140137, -0.46367912500000003, 0.52021383883022987, -0.31661800000000001, 0.51847081825788421, 0.065237308125000004, 0.34855793425650899, -0.043385381250000001, 0.38514701331777879, 0.118521375, 0.37437058077523183, 0.033509812500000007, 0.35064659592551639, -0.027936874999999996, 0.54871888402428748, -0.35806956249999999, 0.3450708121936657, -0.16652887500000002, 0.49251552826939821, 0.078606799999999991, 0.38647147585284353, -0.31962708750000002, 0.35791199825353692, -0.090147062500000014, 0.49690849193154629, -0.066837187500000006, 0.32402405454653721, 0.1697318125, 0.45997997921665279, 0.07991680625, 0.50746814102234594, -0.024763000000000011, 0.43305753096961602, 0.095947050000000006, 0.27298175192192337, 0.014443937500000004, 0.45751119352433184]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "################################################\n",
    "# get raw test data\n",
    "################################################\n",
    "import random\n",
    "\n",
    "# init\n",
    "TEST_RATIO = 0.1\n",
    "assert TEST_RATIO > 0 and TEST_RATIO < 1\n",
    "\n",
    "# get data\n",
    "text_handles, raw_tweets = parse_tweet_csv(\"train.csv\")\n",
    "handles = int_labels(text_handles)\n",
    "tweets = import_text(raw_tweets)   \n",
    "data_vector_size = len(tweets[0])\n",
    "\n",
    "### validation\n",
    "for i in range(1):\n",
    "    tweet = raw_tweets[random.randint(0, len(raw_tweets))]\n",
    "    print(tokenize(tweet))\n",
    "    print(get_glove_vector(tokenize(tweet)))\n",
    "    print()\n",
    "# for handle in int_labels(handles[0:7]):\n",
    "#     print(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Separated into 4263 train and 480 test (10%)\n",
      "\n",
      "   handle  length                                         tweet_data\n",
      "0       0     100  [67949, 68325, 66056, 68118, 65716, 68268, 649...\n",
      "1       0     100  [66689, 67325, 65331, 68355, 66513, 67626, 648...\n",
      "2       0     100  [67627, 67406, 64699, 66789, 66828, 67378, 641...\n",
      "3       0     100  [67154, 69252, 65796, 67757, 66097, 68693, 646...\n",
      "4       1     100  [64800, 69204, 67689, 67783, 64180, 68449, 658...\n"
     ]
    }
   ],
   "source": [
    "################################################\n",
    "# split test data into train and test\n",
    "################################################\n",
    "import pandas as pd\n",
    "\n",
    "LABEL = 'handle'\n",
    "DATA = 'tweet_data'\n",
    "LENGTH = 'length'\n",
    "\n",
    "# we get floats from glove, so we need to convert them to integers.\n",
    "# the glove floats are both positive and negative, and I'm not sure\n",
    "# what the min/max are, so we normalize over that value, and then\n",
    "# scale them to a particular granularity\n",
    "# TODO how do we give floats to tensorflow?\n",
    "# TODO what's the min/max we'll get from this glove library?\n",
    "\n",
    "# this is so the glove output is in integer form (maybe not necessary)\n",
    "FLOAT_GRANULARITY = (1 << 16)\n",
    "VOCAB_SIZE = 2 * FLOAT_GRANULARITY + 1 # +/- and exclusive\n",
    "\n",
    "# this is so we can define a range for the values\n",
    "# value_max = 0\n",
    "# for tweet in tweets:\n",
    "#     for x in tweet:\n",
    "#         if abs(x) > value_max: value_max = abs(x)\n",
    "# print(\"Got max value of {}\".format(value_max))\n",
    "value_max = 10 # so we're not dependent on what the data is\n",
    "\n",
    "# split into test and train\n",
    "train_labels, train_data, test_labels, test_data = list(), list(), list(), list()\n",
    "for handle, tweet in zip(handles, tweets):\n",
    "#     if np.isnan(tweet[0]): continue #a row of all nan's happens with data that glove can't understand (like, all hashtags)\n",
    "#     tweet = [list(map(lambda x: int(x / value_max * FLOAT_GRANULARITY + FLOAT_GRANULARITY), word)) for word in tweet]\n",
    "    tweet = list(map(lambda x: int(x / value_max * FLOAT_GRANULARITY + FLOAT_GRANULARITY), tweet))\n",
    "    if random.random() < TEST_RATIO:\n",
    "        test_labels.append(handle)\n",
    "        test_data.append(tweet)\n",
    "    else:\n",
    "        train_labels.append(handle)\n",
    "        train_data.append(tweet)\n",
    "\n",
    "# document and validate\n",
    "print(\"Separated into {} train and {} test ({}%)\\n\".format(len(train_data), len(test_data), \n",
    "                                                         int(100.0 * len(test_data) / len(raw_tweets))))\n",
    "assert len(train_labels) == len(train_data) and len(train_data) > 0\n",
    "assert len(test_labels) == len(test_data) and len(test_data) > 0\n",
    "assert len(test_labels) > len(tweets) * (TEST_RATIO - .05)\n",
    "assert len(test_labels) < len(tweets) * (TEST_RATIO + .05) \n",
    "\n",
    "# save to dataframe\n",
    "train = pd.DataFrame({\n",
    "    LABEL: train_labels,\n",
    "    DATA: train_data,\n",
    "    LENGTH: [data_vector_size for _ in range(len(train_data))]\n",
    "})\n",
    "test = pd.DataFrame({\n",
    "    LABEL: test_labels,\n",
    "    DATA: test_data,\n",
    "    LENGTH: [data_vector_size for _ in range(len(test_data))]\n",
    "})\n",
    "print(train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "################################################\n",
    "# initializing our tensor\n",
    "#\n",
    "# based off of blogpost david parks showed us:\n",
    "# https://r2rt.com/recurrent-neural-networks-in-tensorflow-iii-variable-length-sequences.html\n",
    "################################################\n",
    "import tensorflow as tf\n",
    " \n",
    " \n",
    "class DataIterator():\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.size = len(self.df)\n",
    "        self.epochs = 0\n",
    "        self.shuffle()\n",
    "\n",
    "    def shuffle(self):\n",
    "        self.df = self.df.sample(frac=1).reset_index(drop=True)\n",
    "        self.cursor = 0\n",
    "\n",
    "    def next_batch(self, n):\n",
    "        if self.cursor + n - 1 > self.size:\n",
    "            self.epochs += 1\n",
    "            self.shuffle()\n",
    "        res = self.df.ix[self.cursor:self.cursor + n - 1]\n",
    "        start_idx = self.cursor\n",
    "        self.cursor += n\n",
    "        # return res[DATA], res[LABEL], res[LENGTH]\n",
    "        # the above line fails.  an error is thrown when tf attempts to call np.asarray on this.\n",
    "        # what is different about how our data is organized compared to the blog post this came from?\n",
    "        # TODO \n",
    "        data = res[DATA]\n",
    "        labels = res[LABEL]\n",
    "        length = res[LENGTH]\n",
    "        return np.asarray([data[i] for i in range(start_idx, start_idx + len(data))]), \\\n",
    "               np.asarray([labels[i] for i in range(start_idx, start_idx + len(labels))]), \\\n",
    "               np.asarray([length[i] for i in range(start_idx, start_idx + len(length))])\n",
    "\n",
    "\n",
    "def reset_graph():\n",
    "    if 'sess' in globals() and sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "\n",
    "def build_graph(vocab_size = VOCAB_SIZE, state_size = 64, batch_size = 256, num_classes = 2):\n",
    "\n",
    "    reset_graph()\n",
    "\n",
    "    # Placeholders\n",
    "    x = tf.placeholder(tf.int32, [batch_size, None]) # [batch_size, num_steps]\n",
    "    seqlen = tf.placeholder(tf.int32, [batch_size])\n",
    "    y = tf.placeholder(tf.int32, [batch_size])\n",
    "    keep_prob = tf.placeholder_with_default(1.0, [])\n",
    "\n",
    "    # Embedding layer\n",
    "    embeddings = tf.get_variable('embedding_matrix', [vocab_size, state_size])\n",
    "    rnn_inputs = tf.nn.embedding_lookup(embeddings, x)\n",
    "\n",
    "    # RNN\n",
    "    cell = tf.nn.rnn_cell.GRUCell(state_size)\n",
    "    init_state = tf.get_variable('init_state', [1, state_size],\n",
    "                                 initializer=tf.constant_initializer(0.0))\n",
    "    init_state = tf.tile(init_state, [batch_size, 1])\n",
    "    rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, rnn_inputs, sequence_length=seqlen,\n",
    "                                                 initial_state=init_state)\n",
    "\n",
    "    # Add dropout, as the model otherwise quickly overfits\n",
    "    rnn_outputs = tf.nn.dropout(rnn_outputs, keep_prob)\n",
    "    idx = tf.range(batch_size)*tf.shape(rnn_outputs)[1] + (seqlen - 1)\n",
    "    # last_rnn_output = tf.gather(tf.reshape(rnn_outputs, [-1, state_size]), idx)\n",
    "    # last_rnn_output = tf.gather_nd(rnn_outputs, tf.pack([tf.range(batch_size), seqlen-1], axis=1))\n",
    "    last_rnn_output = tf.gather_nd(rnn_outputs, tf.stack([tf.range(batch_size), seqlen-1], axis=1))\n",
    "\n",
    "    # Softmax layer\n",
    "    with tf.variable_scope('softmax'):\n",
    "        W = tf.get_variable('W', [state_size, num_classes]) # weights?\n",
    "        b = tf.get_variable('b', [num_classes], initializer=tf.constant_initializer(0.0)) # bias?\n",
    "    logits = tf.matmul(last_rnn_output, W) + b\n",
    "    preds = tf.nn.softmax(logits)\n",
    "    correct = tf.equal(tf.cast(tf.argmax(preds,1),tf.int32), y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "\n",
    "    return {\n",
    "        'x': x,\n",
    "        'seqlen': seqlen,\n",
    "        'y': y,\n",
    "        'dropout': keep_prob,\n",
    "        'loss': loss,\n",
    "        'ts': train_step,\n",
    "        'preds': preds,\n",
    "        'accuracy': accuracy\n",
    "    }\n",
    "\n",
    "\n",
    "def train_graph(g, batch_size = 256, num_epochs = 10, iterator = DataIterator):\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        tr = iterator(train)\n",
    "        te = iterator(test)\n",
    "\n",
    "        step, accuracy = 0, 0\n",
    "        tr_losses, te_losses = [], []\n",
    "        current_epoch = 0\n",
    "        while current_epoch < num_epochs:\n",
    "            step += 1\n",
    "            batch = tr.next_batch(batch_size)\n",
    "            feed = {g['x']: batch[0], g['y']: batch[1], g['seqlen']: batch[2], g['dropout']: 0.6}\n",
    "            accuracy_, _ = sess.run([g['accuracy'], g['ts']], feed_dict=feed)\n",
    "            accuracy += accuracy_\n",
    "\n",
    "            if tr.epochs > current_epoch:\n",
    "                current_epoch += 1\n",
    "                tr_losses.append(accuracy / step)\n",
    "                step, accuracy = 0, 0\n",
    "\n",
    "                #eval test set\n",
    "                te_epoch = te.epochs\n",
    "                while te.epochs == te_epoch:\n",
    "                    step += 1\n",
    "                    batch = te.next_batch(batch_size)\n",
    "                    feed = {g['x']: batch[0], g['y']: batch[1], g['seqlen']: batch[2]}\n",
    "                    accuracy_ = sess.run([g['accuracy']], feed_dict=feed)[0]\n",
    "                    accuracy += accuracy_\n",
    "\n",
    "                te_losses.append(accuracy / step)\n",
    "                step, accuracy = 0,0\n",
    "                print(\"Accuracy after epoch\", current_epoch, \" - tr:\", tr_losses[-1], \"- te:\", te_losses[-1])\n",
    "\n",
    "    return tr_losses, te_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequences:\n",
      " <class 'numpy.ndarray'>: \n",
      "[[66390 69131 64966 69233 65793 69143 63732 67488 68058 68137 65824 69315\n",
      "  64480 67910 65208 68601 65901 68737 63126 69413 65431 67197 65555 70369\n",
      "  63442 68068 63911 67308 67688 69270 65319 69450 65732 68883 64773 67937\n",
      "  64342 70660 62962 68460 66118 68047 67701 68850 65890 68623 64129 67787\n",
      "  64520 69453 54661 69881 64148 69142 63278 68227 66913 68666 64407 68610\n",
      "  85149 71597 67158 69010 62752 69216 61195 69882 66518 68091 66591 68988\n",
      "  66198 68159 64202 68892 66105 67391 66564 68652 62665 68051 66413 68137\n",
      "  68748 68884 66601 68067 65115 69194 65265 69465 63262 67922 65482 68022\n",
      "  65720 67351 64676 68335]\n",
      " [67339 68446 65015 68451 66220 68868 65765 68795 68674 67771 64848 67980\n",
      "  62950 67469 65744 68951 64185 67757 64658 69829 65567 68524 67901 69259\n",
      "  62854 67333 65437 67360 68206 69978 69344 67992 66833 68117 65248 67550\n",
      "  64034 68992 63029 69029 66107 67844 65763 68201 66179 68132 65013 67889\n",
      "  67293 68394 55246 69388 63875 68814 65580 67833 67982 68905 63576 68978\n",
      "  82927 75141 67119 69239 63260 67811 64097 68752 66299 67620 64231 66960\n",
      "  65913 67423 67602 67320 65235 68488 63482 67610 63956 67545 65958 68158\n",
      "  65406 66927 66743 69024 64595 69308 67015 66961 64672 68636 65098 67636\n",
      "  65336 67225 67406 68479]\n",
      " [67334 68178 66874 68565 64849 68728 64825 67471 69193 68739 67505 69288\n",
      "  64493 68725 64516 67953 64280 67613 65384 68052 65336 68649 67114 67900\n",
      "  64585 68430 64711 67806 66742 68359 66684 68252 66685 67951 65127 66763\n",
      "  65134 68725 61349 68333 65610 68336 66787 68606 67743 68621 65444 68154\n",
      "  66915 69087 56103 68993 63411 68458 64966 67878 67515 68705 63363 68726\n",
      "  87007 72694 68085 68507 63330 68798 62246 68132 65839 67611 64814 68227\n",
      "  65373 68295 65483 68147 64301 68207 64916 68539 64614 67840 65333 68136\n",
      "  66090 68083 67337 67907 65660 68528 66338 66904 64068 68089 65750 67968\n",
      "  64618 67761 65747 68485]]\n",
      " <class 'numpy.ndarray'>: \n",
      "[66390 69131 64966 69233 65793 69143 63732 67488 68058 68137 65824 69315\n",
      " 64480 67910 65208 68601 65901 68737 63126 69413 65431 67197 65555 70369\n",
      " 63442 68068 63911 67308 67688 69270 65319 69450 65732 68883 64773 67937\n",
      " 64342 70660 62962 68460 66118 68047 67701 68850 65890 68623 64129 67787\n",
      " 64520 69453 54661 69881 64148 69142 63278 68227 66913 68666 64407 68610\n",
      " 85149 71597 67158 69010 62752 69216 61195 69882 66518 68091 66591 68988\n",
      " 66198 68159 64202 68892 66105 67391 66564 68652 62665 68051 66413 68137\n",
      " 68748 68884 66601 68067 65115 69194 65265 69465 63262 67922 65482 68022\n",
      " 65720 67351 64676 68335]\n",
      " <class 'numpy.int32'>: \n",
      "66390\n",
      "\n",
      "\n",
      "Target values\n",
      " <class 'numpy.ndarray'>: \n",
      "[1 0 0]\n",
      " <class 'numpy.int64'>: \n",
      "1\n",
      "\n",
      "\n",
      "Sequence lengths\n",
      " <class 'numpy.ndarray'>: \n",
      "[100 100 100]\n",
      " <class 'numpy.int64'>: \n",
      "100\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "################################################\n",
    "# explore our data iterator\n",
    "################################################\n",
    "\n",
    "# validate data iterator\n",
    "d = DataIterator(test).next_batch(3)\n",
    "print('Input sequences:\\n', \n",
    "      \"{}: \\n{}\\n\".format(type(d[0]), d[0]), \n",
    "      \"{}: \\n{}\\n\".format(type(d[0][0]), d[0][0]), \n",
    "      \"{}: \\n{}\\n\".format(type(d[0][0][0]), d[0][0][0]), \n",
    "      end='\\n\\n')\n",
    "print('Target values\\n', \n",
    "      \"{}: \\n{}\\n\".format(type(d[1]), d[1]), \n",
    "      \"{}: \\n{}\\n\".format(type(d[1][0]), d[1][0]), \n",
    "      end='\\n\\n')\n",
    "print('Sequence lengths\\n', \n",
    "      \"{}: \\n{}\\n\".format(type(d[2]), d[2]), \n",
    "      \"{}: \\n{}\\n\".format(type(d[2][0]), d[2][0]), \n",
    "      end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after epoch 1  - tr: 0.513097426471 - te: 0.490234375\n",
      "Accuracy after epoch 2  - tr: 0.63427734375 - te: 0.4921875\n",
      "Accuracy after epoch 3  - tr: 0.718017578125 - te: 0.46875\n",
      "Accuracy after epoch 4  - tr: 0.84228515625 - te: 0.5234375\n",
      "Accuracy after epoch 5  - tr: 0.84912109375 - te: 0.5078125\n",
      "Accuracy after epoch 6  - tr: 0.88427734375 - te: 0.578125\n",
      "Accuracy after epoch 7  - tr: 0.84912109375 - te: 0.546875\n",
      "Accuracy after epoch 8  - tr: 0.74853515625 - te: 0.4921875\n",
      "Accuracy after epoch 9  - tr: 0.85498046875 - te: 0.5234375\n",
      "Accuracy after epoch 10  - tr: 0.92041015625 - te: 0.56640625\n"
     ]
    }
   ],
   "source": [
    "################################################\n",
    "# run it!\n",
    "################################################\n",
    "\n",
    "# this fails, just like us\n",
    "g = build_graph()\n",
    "tr_losses, te_losses = train_graph(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: we're overfitting, why?\n",
    "\n",
    "#TODO: try something better than averaging the word vec values\n",
    "# maybe we could do three-d arrays?  encode each word and pad the data\n",
    "\n",
    "#TODO: we strip a lot away (ie punctuation, smilies) and lose other\n",
    "# data to glove (#hashtags, @handles). how can we keep this?\n",
    "\n",
    "#TODO: how do we run this on the test dataset?"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
