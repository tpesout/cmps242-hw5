{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('tests', 0.9164356610046378), ('testing', 0.81992214225169979), ('tested', 0.74428756870386203), ('final', 0.69102279899050834), ('taking', 0.68790020450861178), ('results', 0.68468909013686308), ('match', 0.67696818430673733), ('determine', 0.67679768970507126), ('challenge', 0.67477055567439548)]\n"
     ]
    }
   ],
   "source": [
    "################################################\n",
    "# intialize glove\n",
    "################################################\n",
    "# >pip install glove_python\n",
    "# >wget http://www.google.com/search?q=glove+word+embeddings+download+those+dope+pre+trained+vectors\n",
    "# >unzip dope_glove_shit.zip\n",
    "# >cp cmps242_hw5_config.py.example cmps242_hw5_config.py\n",
    "# >echo \"set GLOVE_LOCATION in cmps242_hw5_config.py to one of those files\"\n",
    "################################################\n",
    "from cmps242_hw5_config import *\n",
    "from glove import Glove\n",
    "\n",
    "glove_data = Glove.load_stanford(GLOVE_LOCATION)\n",
    "\n",
    "print(glove_data.most_similar('test', number=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "################################################\n",
    "# file parsing functions\n",
    "################################################\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import string, re\n",
    "import collections\n",
    "import numpy as np\n",
    "import glove\n",
    "from glove.glove_cython import fit_vectors, transform_paragraph\n",
    "\n",
    "# definitions\n",
    "HC=\"HillaryClinton\"\n",
    "DT=\"realDonaldTrump\"\n",
    "NA=\"none\"\n",
    "HANDLES = [HC,DT,NA]\n",
    "HANDLE_MAP = {NA:0, HC:1, DT:2}\n",
    "\n",
    "# read csv file, return handles and tweets\n",
    "def parse_tweet_csv(file, file_encoding=\"utf8\"):\n",
    "    # init\n",
    "    handles, tweets = [], []\n",
    "    \n",
    "    # read file\n",
    "    linenr = -1\n",
    "    with open(file, encoding=file_encoding) as input:\n",
    "        try:\n",
    "            for line in input:\n",
    "                linenr += 1\n",
    "                if linenr == 0: continue\n",
    "                \n",
    "                # get contents\n",
    "                line = line.split(\",\")\n",
    "                if line[0] in HANDLES: #label and irst line of tweet\n",
    "                    handles.append(line[0])\n",
    "                    tweet = ','.join(line[1:])\n",
    "                    tweets.append(tweet)\n",
    "                else: #second+ line of tweet\n",
    "                    tweet = tweets.pop()\n",
    "                    tweet += ','.join(line)\n",
    "                    tweets.append(tweet)\n",
    "        except Exception as e:\n",
    "            print(\"Exception at line {}: {}\".format(linenr, e))\n",
    "            raise e\n",
    "    \n",
    "    # sanity checks\n",
    "    assert len(handles) == len(tweets)\n",
    "    print(\"Found {} tweets in {} lines\".format(len(tweets), linenr + 1))\n",
    "    \n",
    "    # return data\n",
    "    return handles, tweets\n",
    "\n",
    "\n",
    "##########################################\n",
    "### coverting tweet strings to numbers ###\n",
    "\n",
    "# coverting labels to integers\n",
    "def int_labels(labels):\n",
    "    return list(map(lambda x: HANDLE_MAP[x], labels))\n",
    "\n",
    "#tokenizing\n",
    "_tokenizer = TweetTokenizer()\n",
    "_punctuation = set(string.punctuation)\n",
    "def tokenize(tweet, lowercase=True, strip_urls=True, strip_punctuation=True):\n",
    "    tokens = _tokenizer.tokenize(tweet)\n",
    "    if lowercase: tokens = list(map(lambda x: x.lower(), tokens))\n",
    "    if strip_urls: tokens = list(filter(lambda x: not x.startswith(\"http\"), tokens))\n",
    "    if strip_punctuation: #https://stackoverflow.com/questions/265960/best-way-to-strip-punctuation-from-a-string-in-python\n",
    "        tokens = list(filter(lambda x: x.startswith(u'@') or x.startswith(u'#') or x not in _punctuation and not re.match(u\"[^\\w\\d'\\s$]+\", x), tokens))\n",
    "    return tokens\n",
    "\n",
    "# glove information\n",
    "def get_glove_vector(glove_data, tokens, epochs=50, ignore_missing=True):\n",
    "    \"\"\"\n",
    "    tpesout: This code came from the 'glove' repo I'm using (but had a bug, so I needed to slighly modify it)\n",
    "    https://github.com/maciejkula/glove-python/blob/master/glove/glove.py\n",
    "    \n",
    "    Transform an iterable of tokens into its vector representation\n",
    "    (a paragraph vector).\n",
    "\n",
    "    Experimental. This will return something close to a tf-idf\n",
    "    weighted average of constituent token vectors by fitting\n",
    "    rare words (with low word bias values) more closely.\n",
    "    \"\"\"\n",
    "\n",
    "    if glove_data.word_vectors is None:\n",
    "        raise Exception('Model must be fit to transform paragraphs')\n",
    "\n",
    "    if glove_data.dictionary is None:\n",
    "        raise Exception('Dictionary must be provided to '\n",
    "                        'transform paragraphs')\n",
    "\n",
    "    cooccurrence = collections.defaultdict(lambda: 0.0)\n",
    "\n",
    "    for token in tokens:\n",
    "        try:\n",
    "            cooccurrence[glove_data.dictionary[token]] += glove_data.max_count / 10.0\n",
    "        except KeyError:\n",
    "            if not ignore_missing:\n",
    "                raise\n",
    "\n",
    "    random_state = glove.glove.check_random_state(glove_data.random_state)\n",
    "\n",
    "    word_ids = np.array(list(cooccurrence.keys()), dtype=np.int32)\n",
    "    values = np.array(list(cooccurrence.values()), dtype=np.float64)\n",
    "    shuffle_indices = np.arange(len(word_ids), dtype=np.int32)\n",
    "\n",
    "    # Initialize the vector to mean of constituent word vectors\n",
    "    paragraph_vector = np.mean(glove_data.word_vectors[word_ids], axis=0)\n",
    "    sum_gradients = np.ones_like(paragraph_vector)\n",
    "\n",
    "    # Shuffle the coocurrence matrix\n",
    "    random_state.shuffle(shuffle_indices)\n",
    "    transform_paragraph(glove_data.word_vectors,\n",
    "                        glove_data.word_biases,\n",
    "                        paragraph_vector,\n",
    "                        sum_gradients,\n",
    "                        word_ids,\n",
    "                        values,\n",
    "                        shuffle_indices,\n",
    "                        glove_data.learning_rate,\n",
    "                        glove_data.max_count,\n",
    "                        glove_data.alpha,\n",
    "                        epochs)\n",
    "\n",
    "    return paragraph_vector\n",
    "    \n",
    "    \n",
    "# get all tweets\n",
    "def import_text(tweets):\n",
    "    return [get_glove_vector(glove_data, tokenize(tweet)) for tweet in tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4743 tweets in 6251 lines\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2909: RuntimeWarning: Mean of empty slice.\n  out=out, **kwargs)\nC:\\Program Files\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:73: RuntimeWarning: invalid value encountered in true_divide\n  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#makeamericagreatagain', '#trump2016']\n[ nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n  nan  nan  nan  nan  nan]\n\n"
     ]
    }
   ],
   "source": [
    "################################################\n",
    "# get raw test data\n",
    "################################################\n",
    "import random\n",
    "\n",
    "# init\n",
    "TEST_RATIO = 0.1\n",
    "assert TEST_RATIO > 0 and TEST_RATIO < 1\n",
    "\n",
    "# get data\n",
    "text_handles, raw_tweets = parse_tweet_csv(\"train.csv\")\n",
    "handles = int_labels(text_handles)\n",
    "tweets = import_text(raw_tweets)   \n",
    "data_vector_size = len(tweets[0])\n",
    "\n",
    "### validation\n",
    "for i in range(1):\n",
    "    tweet = raw_tweets[random.randint(0, len(raw_tweets))]\n",
    "    print(tokenize(tweet))\n",
    "    print(get_glove_vector(glove_data, tokenize(tweet)))\n",
    "    print()\n",
    "# for handle in int_labels(handles[0:7]):\n",
    "#     print(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Separated into 4229 train and 453 test (9%)\n\n   handle  length                                         tweet_data\n0       1      50  [-42109, -10098, 4462, 15191, -31174, -22236, ...\n1       1      50  [-30288, -6132, -17246, 20218, -20686, -8332, ...\n2       1      50  [-36519, 21300, -29862, 33744, -21948, 13709, ...\n3       1      50  [-22020, -24918, -1488, 9447, -33670, -27225, ...\n4       2      50  [-31240, -19811, 12546, -4235, -42001, -16107,...\n"
     ]
    }
   ],
   "source": [
    "################################################\n",
    "# split test data into train and test\n",
    "################################################\n",
    "import pandas as pd\n",
    "\n",
    "LABEL = 'handle'\n",
    "DATA = 'tweet_data'\n",
    "LENGTH = 'length'\n",
    "\n",
    "# this is so the glove output is in integer form (maybe not necessary)\n",
    "FLOAT_GRANULARITY = (1 << 16)\n",
    "VOCAB_SIZE = 2 * FLOAT_GRANULARITY # for positive and negative\n",
    "\n",
    "# split into test and train\n",
    "train_labels, train_data, test_labels, test_data = list(), list(), list(), list()\n",
    "for handle, tweet in zip(handles, tweets):\n",
    "    if np.isnan(tweet[0]): continue #a row of all nan's happens with data that glove can't understand (like, all hashtags)\n",
    "    tweet = list(map(lambda x: int(x * FLOAT_GRANULARITY), tweet))\n",
    "    if random.random() < TEST_RATIO:\n",
    "        test_labels.append(handle)\n",
    "        test_data.append(tweet)\n",
    "    else:\n",
    "        train_labels.append(handle)\n",
    "        train_data.append(tweet)\n",
    "\n",
    "# document and validate\n",
    "print(\"Separated into {} train and {} test ({}%)\\n\".format(len(train_data), len(test_data), \n",
    "                                                         int(100.0 * len(test_data) / len(raw_tweets))))\n",
    "assert len(train_labels) == len(train_data) and len(train_data) > 0\n",
    "assert len(test_labels) == len(test_data) and len(test_data) > 0\n",
    "assert len(test_labels) > len(tweets) * (TEST_RATIO - .05)\n",
    "assert len(test_labels) < len(tweets) * (TEST_RATIO + .05) \n",
    "\n",
    "# save to dataframe\n",
    "train = pd.DataFrame({\n",
    "    LABEL: train_labels,\n",
    "    DATA: train_data,\n",
    "    LENGTH: [data_vector_size for _ in range(len(train_data))]\n",
    "})\n",
    "test = pd.DataFrame({\n",
    "    LABEL: test_labels,\n",
    "    DATA: test_data,\n",
    "    LENGTH: [data_vector_size for _ in range(len(test_data))]\n",
    "})\n",
    "print(train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "################################################\n",
    "# initializing our tensor\n",
    "#\n",
    "# based off of blogpost david parks showed us:\n",
    "# https://r2rt.com/recurrent-neural-networks-in-tensorflow-iii-variable-length-sequences.html\n",
    "################################################\n",
    "import tensorflow as tf\n",
    " \n",
    " \n",
    "class DataIterator():\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.size = len(self.df)\n",
    "        self.epochs = 0\n",
    "        self.shuffle()\n",
    "\n",
    "    def shuffle(self):\n",
    "        self.df = self.df.sample(frac=1).reset_index(drop=True)\n",
    "        self.cursor = 0\n",
    "\n",
    "    def next_batch(self, n):\n",
    "        if self.cursor+n-1 > self.size:\n",
    "            self.epochs += 1\n",
    "            self.shuffle()\n",
    "        res = self.df.ix[self.cursor:self.cursor+n-1]\n",
    "        self.cursor += n\n",
    "        # return res['as_numbers'], res['gender']*3 + res['age_bracket'], res['length']\n",
    "        # return res[DATA], res[LABEL], res[LENGTH]\n",
    "        return res[DATA], res[LABEL], res[LENGTH]\n",
    "\n",
    "\n",
    "def reset_graph():\n",
    "    if 'sess' in globals() and sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "\n",
    "def build_graph(vocab_size = VOCAB_SIZE, state_size = 64, batch_size = 16, num_classes = 2):\n",
    "\n",
    "    reset_graph()\n",
    "\n",
    "    # Placeholders\n",
    "    x = tf.placeholder(tf.int32, [batch_size, None]) # [batch_size, num_steps]\n",
    "    seqlen = tf.placeholder(tf.int32, [batch_size])\n",
    "    y = tf.placeholder(tf.int32, [batch_size])\n",
    "    keep_prob = tf.constant(1.0)\n",
    "\n",
    "    # Embedding layer\n",
    "    embeddings = tf.get_variable('embedding_matrix', [vocab_size, state_size])\n",
    "    rnn_inputs = tf.nn.embedding_lookup(embeddings, x)\n",
    "\n",
    "    # RNN\n",
    "    cell = tf.nn.rnn_cell.GRUCell(state_size)\n",
    "    init_state = tf.get_variable('init_state', [1, state_size],\n",
    "                                 initializer=tf.constant_initializer(0.0))\n",
    "    init_state = tf.tile(init_state, [batch_size, 1])\n",
    "    rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, rnn_inputs, sequence_length=seqlen,\n",
    "                                                 initial_state=init_state)\n",
    "\n",
    "    # Add dropout, as the model otherwise quickly overfits\n",
    "    rnn_outputs = tf.nn.dropout(rnn_outputs, keep_prob)\n",
    "    idx = tf.range(batch_size)*tf.shape(rnn_outputs)[1] + (seqlen - 1)\n",
    "    last_rnn_output = tf.gather(tf.reshape(rnn_outputs, [-1, state_size]), idx)\n",
    "\n",
    "    # Softmax layer\n",
    "    with tf.variable_scope('softmax'):\n",
    "        W = tf.get_variable('W', [state_size, num_classes])\n",
    "        b = tf.get_variable('b', [num_classes], initializer=tf.constant_initializer(0.0))\n",
    "    logits = tf.matmul(last_rnn_output, W) + b\n",
    "    preds = tf.nn.softmax(logits)\n",
    "    correct = tf.equal(tf.cast(tf.argmax(preds,1),tf.int32), y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "\n",
    "    return {\n",
    "        'x': x,\n",
    "        'seqlen': seqlen,\n",
    "        'y': y,\n",
    "        'dropout': keep_prob,\n",
    "        'loss': loss,\n",
    "        'ts': train_step,\n",
    "        'preds': preds,\n",
    "        'accuracy': accuracy\n",
    "    }\n",
    "\n",
    "\n",
    "def train_graph(g, batch_size = 256, num_epochs = 10, iterator = DataIterator):\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        tr = iterator(train)\n",
    "        te = iterator(test)\n",
    "\n",
    "        step, accuracy = 0, 0\n",
    "        tr_losses, te_losses = [], []\n",
    "        current_epoch = 0\n",
    "        while current_epoch < num_epochs:\n",
    "            step += 1\n",
    "            batch = tr.next_batch(batch_size)\n",
    "            feed = {g['x']: batch[0], g['y']: batch[1], g['seqlen']: batch[2], g['dropout']: 0.6}\n",
    "            print(feed)\n",
    "            accuracy_, _ = sess.run([g['accuracy'], g['ts']], feed_dict=feed)\n",
    "            accuracy += accuracy_\n",
    "\n",
    "            if tr.epochs > current_epoch:\n",
    "                current_epoch += 1\n",
    "                tr_losses.append(accuracy / step)\n",
    "                step, accuracy = 0, 0\n",
    "\n",
    "                #eval test set\n",
    "                te_epoch = te.epochs\n",
    "                while te.epochs == te_epoch:\n",
    "                    step += 1\n",
    "                    batch = te.next_batch(batch_size)\n",
    "                    feed = {g['x']: batch[0], g['y']: batch[1], g['seqlen']: batch[2]}\n",
    "                    accuracy_ = sess.run([g['accuracy']], feed_dict=feed)[0]\n",
    "                    accuracy += accuracy_\n",
    "\n",
    "                te_losses.append(accuracy / step)\n",
    "                step, accuracy = 0,0\n",
    "                print(\"Accuracy after epoch\", current_epoch, \" - tr:\", tr_losses[-1], \"- te:\", te_losses[-1])\n",
    "\n",
    "    return tr_losses, te_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequences:\n <class 'pandas.core.series.Series'>: \n0    [-23220, -16038, 21501, 19702, -29209, -18200,...\n1    [-29662, -30123, 9171, -12641, -43482, -34101,...\n2    [-40992, -34769, 8202, 4369, -38950, -23321, 3...\nName: tweet_data, dtype: object\n <class 'list'>: \n[-23220, -16038, 21501, 19702, -29209, -18200, 20775, 15318, 113, 679, -8878, 25544, 18791, 22421, -12248, 5307, 6147, -705, 29726, 27388, -18505, 314, -11106, 15874, -17429, -19174, 21538, -24105, -32373, -15124, 84529, 18018, 24098, 33385, 13006, 9015, -820, -15473, -24512, 2098, 8037, -11588, 24353, 12246, 10620, -1562, -4760, -18723, -12746, 14142]\n <class 'int'>: \n-23220\n\n\nTarget values\n <class 'pandas.core.series.Series'>: \n0    1\n1    1\n2    1\nName: handle, dtype: int64\n <class 'numpy.int64'>: \n1\n\n\nSequence lengths\n <class 'pandas.core.series.Series'>: \n0    50\n1    50\n2    50\nName: length, dtype: int64\n <class 'numpy.int64'>: \n50\n\n\n"
     ]
    }
   ],
   "source": [
    "################################################\n",
    "# explore our data iterator\n",
    "################################################\n",
    "\n",
    "# validate data iterator\n",
    "d = DataIterator(test).next_batch(3)\n",
    "print('Input sequences:\\n', \n",
    "      \"{}: \\n{}\\n\".format(type(d[0]), d[0]), \n",
    "      \"{}: \\n{}\\n\".format(type(d[0][0]), d[0][0]), \n",
    "      \"{}: \\n{}\\n\".format(type(d[0][0][0]), d[0][0][0]), \n",
    "      end='\\n\\n')\n",
    "print('Target values\\n', \n",
    "      \"{}: \\n{}\\n\".format(type(d[1]), d[1]), \n",
    "      \"{}: \\n{}\\n\".format(type(d[1][0]), d[1][0]), \n",
    "      end='\\n\\n')\n",
    "print('Sequence lengths\\n', \n",
    "      \"{}: \\n{}\\n\".format(type(d[2]), d[2]), \n",
    "      \"{}: \\n{}\\n\".format(type(d[2][0]), d[2][0]), \n",
    "      end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{<tf.Tensor 'Placeholder:0' shape=(16, ?) dtype=int32>: 0      [-19597, 3807, -8248, 11081, -36083, 16265, 17...\n1      [-7391, -22530, -6053, 31666, -11602, -8017, 1...\n2      [-24991, -20675, -9795, 33102, -30620, -30586,...\n3      [-29375, -18296, 33733, -2698, -32659, -27741,...\n4      [-37291, -26696, -12705, 16690, -34839, -26794...\n5      [-11776, 10437, -20617, 16236, -5312, -9818, 1...\n6      [11274, -25112, -15371, 15847, -7950, 13641, 7...\n7      [-27797, -31908, 18046, 10629, -33857, 6124, 4...\n8      [-11224, 6346, 2639, 30194, -17485, -31192, 30...\n9      [-33930, -16429, -12657, 18986, -25771, -17956...\n10     [4769, -17286, -15732, -8530, 31991, -5158, 11...\n11     [-31725, 3431, -3099, 8552, -39946, -11157, 26...\n12     [-28251, -13058, -10185, 12182, -17267, -13589...\n13     [-19279, 3354, 13119, 14376, -22925, -16620, 5...\n14     [-19903, -29001, -19340, 11088, -20989, 21317,...\n15     [-39233, -21299, -8505, 11710, -23503, -27121,...\n16     [-31199, -19418, 4889, -5871, -23627, -5208, 2...\n17     [-2191, -13508, 15317, 32604, -23220, -21515, ...\n18     [-56487, -35813, 30981, 6957, -3654, -19415, 2...\n19     [11106, -14116, -26600, 5787, -2727, 12498, 72...\n20     [-14445, -4363, 15397, 21029, -27689, 17749, 1...\n21     [-30288, -14950, -203, 18712, -12966, -17155, ...\n22     [-24440, 4020, 2092, 13353, -27183, -2487, 330...\n23     [-27037, -23962, 36118, -13515, -35907, -16706...\n24     [-38712, -36963, 11930, -15765, -26979, -21149...\n25     [-46766, -18447, -1057, 16238, -25571, -27990,...\n26     [-19522, -8241, 33816, 20530, -12287, -23770, ...\n27     [-46927, -7584, 17160, 15612, -23251, -21123, ...\n28     [-36688, -19712, -2310, -2650, -39608, -34186,...\n29     [-23967, -4020, 2662, 5355, -36637, 19243, 273...\n                             ...                        \n226    [-39459, -9372, 6696, 6726, -15416, -8515, 129...\n227    [8144, -13596, -18652, 3217, 10648, -7174, 180...\n228    [-9263, -17860, -2173, 9206, -34051, -4363, 23...\n229    [-22159, -18135, 11152, 11784, -11308, -28480,...\n230    [-37560, -15183, -3333, 6987, -18389, 20529, 2...\n231    [-41148, -16015, 11388, 1604, -17189, -27666, ...\n232    [-30679, -7355, 17072, 30231, -25432, -20043, ...\n233    [-20327, -20736, -8461, 24118, -28478, -35376,...\n234    [-34970, -41773, -17241, -8543, -30575, -11248...\n235    [-35103, 4679, -17597, 8679, -34096, -22590, 3...\n236    [-41509, -23991, 15299, 5196, -50858, -32055, ...\n237    [-23856, -7830, 17955, -18647, -8562, -2814, 3...\n238    [-51184, -9537, -3631, 620, -42994, -25685, 46...\n239    [-19096, -13997, 6801, 5041, -39849, -28019, 3...\n240    [-27796, -28621, 32510, -27348, -19591, -1279,...\n241    [-35407, -30834, -17997, 24753, -24410, -18745...\n242    [-23908, 4404, -11160, 193, -25021, -8080, 289...\n243    [-8650, -22965, -16329, 4914, -14087, -8923, 5...\n244    [-24064, -16418, -15560, 20940, -26535, -7762,...\n245    [-32612, -13820, -1568, 12600, -30815, -26066,...\n246    [-28100, -3664, -2114, 12213, -27898, -16942, ...\n247    [-32991, -42006, 2062, -8305, -38922, -18711, ...\n248    [-12887, -13854, 18165, 24466, -6647, 5093, 82...\n249    [-34073, -27137, -8429, -6139, -25375, -16540,...\n250    [-14858, 16669, 15153, 11527, -37053, -18761, ...\n251    [-27333, -12576, 9696, -22891, -24967, -30286,...\n252    [-20092, -13838, 6713, 5442, -4631, -12608, 25...\n253    [-25870, -26585, -3209, 16386, -21833, -34489,...\n254    [-41798, -21408, -26583, -5990, -28621, 7890, ...\n255    [-27119, -42321, -2060, 4188, -27632, -27238, ...\nName: tweet_data, dtype: object, <tf.Tensor 'Placeholder_2:0' shape=(16,) dtype=int32>: 0      2\n1      1\n2      1\n3      1\n4      1\n5      2\n6      2\n7      2\n8      1\n9      1\n10     1\n11     1\n12     1\n13     2\n14     2\n15     1\n16     2\n17     1\n18     2\n19     1\n20     2\n21     2\n22     2\n23     1\n24     2\n25     2\n26     1\n27     2\n28     1\n29     1\n      ..\n226    2\n227    2\n228    2\n229    1\n230    2\n231    2\n232    1\n233    1\n234    2\n235    2\n236    2\n237    1\n238    1\n239    1\n240    1\n241    2\n242    2\n243    2\n244    1\n245    2\n246    1\n247    2\n248    2\n249    1\n250    2\n251    2\n252    2\n253    1\n254    2\n255    1\nName: handle, dtype: int64, <tf.Tensor 'Const:0' shape=() dtype=float32>: 0.6, <tf.Tensor 'Placeholder_1:0' shape=(16,) dtype=int32>: 0      50\n1      50\n2      50\n3      50\n4      50\n5      50\n6      50\n7      50\n8      50\n9      50\n10     50\n11     50\n12     50\n13     50\n14     50\n15     50\n16     50\n17     50\n18     50\n19     50\n20     50\n21     50\n22     50\n23     50\n24     50\n25     50\n26     50\n27     50\n28     50\n29     50\n       ..\n226    50\n227    50\n228    50\n229    50\n230    50\n231    50\n232    50\n233    50\n234    50\n235    50\n236    50\n237    50\n238    50\n239    50\n240    50\n241    50\n242    50\n243    50\n244    50\n245    50\n246    50\n247    50\n248    50\n249    50\n250    50\n251    50\n252    50\n253    50\n254    50\n255    50\nName: length, dtype: int64}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-3429a29418ed>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# this fails, just like us\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mtr_losses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mte_losses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-42-9f4337a8d059>\u001b[0m in \u001b[0;36mtrain_graph\u001b[1;34m(g, batch_size, num_epochs, iterator)\u001b[0m\n\u001b[0;32m     99\u001b[0m             \u001b[0mfeed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'x'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'y'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'seqlen'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'dropout'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0.6\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 101\u001b[1;33m             \u001b[0maccuracy_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ts'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    102\u001b[0m             \u001b[0maccuracy\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0maccuracy_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    887\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 889\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    890\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1087\u001b[0m             \u001b[0mfeed_handles\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubfeed_val\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1088\u001b[0m           \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1089\u001b[1;33m             \u001b[0mnp_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubfeed_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1090\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1091\u001b[0m           if (not is_tensor_handle_feed and\n",
      "\u001b[1;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m    529\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m     \"\"\"\n\u001b[1;32m--> 531\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    532\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "################################################\n",
    "# run it!\n",
    "################################################\n",
    "\n",
    "# this fails, just like us\n",
    "g = build_graph()\n",
    "tr_losses, te_losses = train_graph(g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}