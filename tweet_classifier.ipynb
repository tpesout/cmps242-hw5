{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('tests', 0.9164356610046378), ('testing', 0.81992214225170001), ('tested', 0.74428756870386181), ('final', 0.69102279899050822), ('taking', 0.68790020450861178), ('results', 0.68468909013686297), ('match', 0.67696818430673733), ('determine', 0.67679768970507126), ('challenge', 0.6747705556743957)]\n"
     ]
    }
   ],
   "source": [
    "################################################\n",
    "# intialize glove\n",
    "################################################\n",
    "# >pip install glove_python\n",
    "# >wget http://www.google.com/search?q=glove+word+embeddings+download+those+dope+pre+trained+vectors\n",
    "# >unzip dope_glove_shit.zip\n",
    "# >echo \"set GLOVE_LOCATION to one of those files\"\n",
    "################################################\n",
    "\n",
    "from glove import Glove\n",
    "\n",
    "GLOVE_LOCATION = \"../glove.6B/glove.6B.50d.txt\"\n",
    "\n",
    "glove_data = Glove.load_stanford(GLOVE_LOCATION)\n",
    "\n",
    "print(glove_data.most_similar('test', number=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################\n",
    "# file parsing functions\n",
    "################################################\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import string, re\n",
    "\n",
    "\n",
    "# definitions\n",
    "HC=\"HillaryClinton\"\n",
    "DT=\"realDonaldTrump\"\n",
    "NA=\"none\"\n",
    "HANDLES = [HC,DT,NA]\n",
    "HANDLE_MAP = {NA:0, HC:1, DT:2}\n",
    "\n",
    "# read csv file, return handles and tweets\n",
    "def parse_tweet_csv(file, file_encoding=\"utf8\"):\n",
    "    # init\n",
    "    handles, tweets = [], []\n",
    "    \n",
    "    # read file\n",
    "    linenr = -1\n",
    "    with open(file, encoding=file_encoding) as input:\n",
    "        try:\n",
    "            for line in input:\n",
    "                linenr += 1\n",
    "                if linenr == 0: continue\n",
    "                \n",
    "                # get contents\n",
    "                line = line.split(\",\")\n",
    "                if line[0] in HANDLES: #label and irst line of tweet\n",
    "                    handles.append(line[0])\n",
    "                    tweet = ','.join(line[1:])\n",
    "                    tweets.append(tweet)\n",
    "                else: #second+ line of tweet\n",
    "                    tweet = tweets.pop()\n",
    "                    tweet += ','.join(line)\n",
    "                    tweets.append(tweet)\n",
    "        except Exception as e:\n",
    "            print(\"Exception at line {}: {}\".format(linenr, e))\n",
    "            raise e\n",
    "    \n",
    "    # sanity checks\n",
    "    assert len(handles) == len(tweets)\n",
    "    print(\"Found {} tweets in {} lines\".format(len(tweets), linenr + 1))\n",
    "    \n",
    "    # return data\n",
    "    return handles, tweets\n",
    "\n",
    "\n",
    "##########################################\n",
    "### coverting tweet strings to numbers ###\n",
    "#tokenize\n",
    "_tokenizer = TweetTokenizer()\n",
    "_punctuation = set(string.punctuation)\n",
    "def tokenize(tweet, lowercase=True, strip_urls=True, strip_punctuation=True):\n",
    "    tokens = _tokenizer.tokenize(tweet)\n",
    "    if lowercase: tokens = list(map(lambda x: x.lower(), tokens))\n",
    "    if strip_urls: tokens = list(filter(lambda x: not x.startswith(\"http\"), tokens))\n",
    "    if strip_punctuation: #https://stackoverflow.com/questions/265960/best-way-to-strip-punctuation-from-a-string-in-python\n",
    "        tokens = list(filter(lambda x: x.startswith(u'@') or x.startswith(u'#') or x not in _punctuation and not re.match(u\"[^\\w\\d'\\s$]+\", x), tokens))\n",
    "    return tokens\n",
    "# glove\n",
    "def get_glove_vector(tokens):\n",
    "    def transform_paragraph(self, paragraph, epochs=50, ignore_missing=False):\n",
    "        \"\"\"\n",
    "        Transform an iterable of tokens into its vector representation\n",
    "        (a paragraph vector).\n",
    "    \n",
    "        Experimental. This will return something close to a tf-idf\n",
    "        weighted average of constituent token vectors by fitting\n",
    "        rare words (with low word bias values) more closely.\n",
    "        \"\"\"\n",
    "        \n",
    "    \n",
    "        if self.word_vectors is None:\n",
    "            raise Exception('Model must be fit to transform paragraphs')\n",
    "    \n",
    "        if self.dictionary is None:\n",
    "            raise Exception('Dictionary must be provided to '\n",
    "                            'transform paragraphs')\n",
    "    \n",
    "        cooccurrence = collections.defaultdict(lambda: 0.0)\n",
    "    \n",
    "        for token in paragraph:\n",
    "            try:\n",
    "                cooccurrence[self.dictionary[token]] += self.max_count / 10.0\n",
    "            except KeyError:\n",
    "                if not ignore_missing:\n",
    "                    raise\n",
    "    \n",
    "        random_state = glove.glove.check_random_state(self.random_state)\n",
    "    \n",
    "        word_ids = np.array(list(cooccurrence.keys()), dtype=np.int32)\n",
    "        values = np.array(list(cooccurrence.values()), dtype=np.float64)\n",
    "        shuffle_indices = np.arange(len(word_ids), dtype=np.int32)\n",
    "    \n",
    "        # Initialize the vector to mean of constituent word vectors\n",
    "        paragraph_vector = np.mean(self.word_vectors[word_ids], axis=0)\n",
    "        sum_gradients = np.ones_like(paragraph_vector)\n",
    "    \n",
    "        # Shuffle the coocurrence matrix\n",
    "        random_state.shuffle(shuffle_indices)\n",
    "        transform_paragraph(self.word_vectors,\n",
    "                            self.word_biases,\n",
    "                            paragraph_vector,\n",
    "                            sum_gradients,\n",
    "                            word_ids,\n",
    "                            values,\n",
    "                            shuffle_indices,\n",
    "                            self.learning_rate,\n",
    "                            self.max_count,\n",
    "                            self.alpha,\n",
    "                            epochs)\n",
    "    \n",
    "        return paragraph_vector\n",
    "    \n",
    "    return transform_paragraph(glove_data, \" \".join(tokens), ignore_missing=True)\n",
    "# handle\n",
    "def import_text(tweets):\n",
    "    return [tokenize(tweet) for tweet in tweets]\n",
    "\n",
    "\n",
    "def int_labels(labels):\n",
    "    return list(map(lambda x: HANDLE_MAP[x], labels))\n",
    "    \n",
    "\n",
    "handles, tweets = parse_tweet_csv(\"train.csv\")\n",
    "handles = int_labels(handles)\n",
    "# tweets = import_text(tweets)\n",
    "\n",
    "### for validation\n",
    "for tweet in tweets[0:16]:\n",
    "    print(tokenize(tweet))\n",
    "    print(get_glove_vector(tokenize(tweet)))\n",
    "    print()\n",
    "# for handle in int_labels(handles[0:7]):\n",
    "#     print(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}