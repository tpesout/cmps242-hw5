{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported 1193514 words, with max glove val 6.9739\n"
     ]
    }
   ],
   "source": [
    "################################################\n",
    "# intialize glove\n",
    "################################################\n",
    "# >pip install glove_python\n",
    "# >wget http://www.google.com/search?q=glove+word+embeddings+download+those+dope+pre+trained+vectors\n",
    "# >unzip dope_glove_shit.zip\n",
    "# >cp cmps242_hw5_config.py.example cmps242_hw5_config.py\n",
    "# >echo \"set GLOVE_LOCATION in cmps242_hw5_config.py to one of those files\"\n",
    "################################################\n",
    "from cmps242_hw5_config import *\n",
    "import numpy as np\n",
    "\n",
    "# config\n",
    "INCLUDE_VARIANCE = False\n",
    "\n",
    "# glove data\n",
    "glove_data = dict()\n",
    "max_glove_val = 0\n",
    "with open(GLOVE_LOCATION, encoding='utf8') as glovein:\n",
    "    for line in glovein:\n",
    "        line = line.split()\n",
    "        word = line[0]\n",
    "        vec = [float(line[i]) for i in range(1,len(line))]\n",
    "        for g in vec:\n",
    "            if abs(g) > max_glove_val: max_glove_val = abs(g)\n",
    "        glove_data[word] = vec\n",
    "    print(\"Imported {} words, with max glove val {}\".format(len(glove_data), max_glove_val))\n",
    "\n",
    "glove_dict_length = len(vec)\n",
    "def get_glove_vector(tokens, glove_information=glove_data, glove_vec_length=glove_dict_length):\n",
    "    temp_vec = [[0.0] for _ in range(glove_vec_length)]\n",
    "\n",
    "    for word in tokens:\n",
    "        if word not in glove_information: continue\n",
    "        glove_vec = glove_information[word]\n",
    "        for i in range(glove_vec_length):\n",
    "            temp_vec[i].append(glove_vec[i])\n",
    "    \n",
    "    vec = []\n",
    "    for i in range(glove_vec_length):\n",
    "        vec.append(np.mean(temp_vec[i]))\n",
    "        if INCLUDE_VARIANCE: vec.append(np.std(temp_vec[i])) #this makes us perform worse\n",
    "\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "################################################\n",
    "# file parsing functions\n",
    "################################################\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import string, re\n",
    "import collections\n",
    "import numpy as np\n",
    "import glove\n",
    "from glove.glove_cython import fit_vectors, transform_paragraph\n",
    "\n",
    "# definitions\n",
    "HC=\"HillaryClinton\"\n",
    "DT=\"realDonaldTrump\"\n",
    "NA=\"none\"\n",
    "HANDLES = [HC,DT,NA]\n",
    "HANDLE_MAP = {NA:-1, HC:0, DT:1}\n",
    "\n",
    "# read csv file, return handles and tweets\n",
    "def parse_tweet_csv(file, file_encoding=\"utf8\"):\n",
    "    # init\n",
    "    handles, tweets = [], []\n",
    "    \n",
    "    # read file\n",
    "    linenr = -1\n",
    "    with open(file, encoding=file_encoding) as input:\n",
    "        try:\n",
    "            for line in input:\n",
    "                linenr += 1\n",
    "                if linenr == 0: continue\n",
    "                \n",
    "                # get contents\n",
    "                line = line.split(\",\")\n",
    "                if line[0] in HANDLES: #label and irst line of tweet\n",
    "                    handles.append(line[0])\n",
    "                    tweet = ','.join(line[1:])\n",
    "                    tweets.append(tweet)\n",
    "                else: #second+ line of tweet\n",
    "                    tweet = tweets.pop()\n",
    "                    tweet += ','.join(line)\n",
    "                    tweets.append(tweet)\n",
    "        except Exception as e:\n",
    "            print(\"Exception at line {}: {}\".format(linenr, e))\n",
    "            raise e\n",
    "    \n",
    "    # sanity checks\n",
    "    assert len(handles) == len(tweets)\n",
    "    print(\"Found {} tweets in {} lines\".format(len(tweets), linenr + 1))\n",
    "    \n",
    "    # return data\n",
    "    return handles, tweets\n",
    "\n",
    "\n",
    "##########################################\n",
    "### coverting tweet strings to numbers ###\n",
    "\n",
    "# coverting labels to integers\n",
    "def int_labels(labels):\n",
    "    return list(map(lambda x: HANDLE_MAP[x], labels))\n",
    "\n",
    "#tokenizing\n",
    "_tokenizer = TweetTokenizer()\n",
    "_punctuation = set(string.punctuation)\n",
    "def tokenize(tweet, lowercase=True, strip_urls=True, strip_punctuation=True):\n",
    "    tokens = _tokenizer.tokenize(tweet)\n",
    "    if lowercase: tokens = list(map(lambda x: x.lower(), tokens))\n",
    "    if strip_urls: tokens = list(filter(lambda x: not x.startswith(\"http\"), tokens))\n",
    "    if strip_punctuation: #https://stackoverflow.com/questions/265960/best-way-to-strip-punctuation-from-a-string-in-python\n",
    "        tokens = list(filter(lambda x: x.startswith(u'@') or x.startswith(u'#') or x not in _punctuation and not re.match(u\"[^\\w\\d'\\s$]+\", x), tokens))\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# get all tweets\n",
    "def import_text(tweets):\n",
    "    return [get_glove_vector(tokenize(tweet)) for tweet in tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4743 tweets in 6251 lines\n",
      "['america', 'needs', 'leadership', 'in', 'the', 'white', 'house', 'not', 'a', 'liability']\n",
      "[0.14379363636363635, 0.0032391818181818151, 0.17495569999999996, 0.093365636363636373, 0.12517408181818182, 0.12194663636363635, 0.3631844545454545, -0.022450181818181824, 0.098126363636363637, -0.26408872727272725, 0.063339090909090914, -0.16446799999999998, -4.0688272727272734, 0.10241981818181821, 0.50555009090909087, -0.22288363636363639, -0.10493672727272725, 0.12781509090909088, 0.094303090909090906, 0.049515181818181819, -0.035895454545454536, -0.17864363636363637, -0.071469090909090899, 0.10648199999999998, 0.10824227272727273, 0.25315545454545457, 0.11925154545454547, 0.063975090909090912, 0.0196441090909091, 0.0084427272727272806, 0.13786818181818183, -0.17963563636363633, -0.042298327272727282, -0.21707090909090906, 0.51508518181818186, 0.28755669090909086, -0.026367781818181821, 0.30249754545454549, -0.22844952999999998, -0.14388132727272726, -0.76833272727272728, 0.097191818181818157, 0.032289090909090906, -0.011395109090909098, 0.41092999999999996, -0.0088563636363636275, 0.31872595454545455, 0.18600090909090908, -0.1025878181818182, -0.22289318181818182]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "################################################\n",
    "# get raw test data\n",
    "################################################\n",
    "import random\n",
    "\n",
    "# init\n",
    "TEST_RATIO = 0.1\n",
    "assert TEST_RATIO > 0 and TEST_RATIO < 1\n",
    "\n",
    "# get data\n",
    "text_handles, raw_tweets = parse_tweet_csv(\"train.csv\")\n",
    "handles = int_labels(text_handles)\n",
    "tweets = import_text(raw_tweets)   \n",
    "data_vector_size = len(tweets[0])\n",
    "\n",
    "### validation\n",
    "for i in range(1):\n",
    "    tweet = raw_tweets[random.randint(0, len(raw_tweets))]\n",
    "    print(tokenize(tweet))\n",
    "    print(get_glove_vector(tokenize(tweet)))\n",
    "    print()\n",
    "# for handle in int_labels(handles[0:7]):\n",
    "#     print(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Separated into 4322 train and 421 test (8%)\n",
      "\n",
      "   handle  length                                         tweet_data\n",
      "0       0      50  [67083, 66651, 65713, 66366, 66684, 66870, 704...\n",
      "1       0      50  [66591, 68016, 66283, 65513, 64586, 65835, 683...\n",
      "2       0      50  [67598, 67903, 66837, 65325, 65379, 66284, 704...\n",
      "3       0      50  [67843, 69111, 66023, 67281, 65863, 66120, 682...\n",
      "4       1      50  [67888, 66314, 67082, 62778, 66086, 65688, 690...\n"
     ]
    }
   ],
   "source": [
    "################################################\n",
    "# split test data into train and test\n",
    "################################################\n",
    "import pandas as pd\n",
    "\n",
    "LABEL = 'handle'\n",
    "DATA = 'tweet_data'\n",
    "LENGTH = 'length'\n",
    "\n",
    "# we get floats from glove, so we need to convert them to integers.\n",
    "# the glove floats are both positive and negative, and I'm not sure\n",
    "# what the min/max are, so we normalize over that value, and then\n",
    "# scale them to a particular granularity\n",
    "# TODO how do we give floats to tensorflow?\n",
    "# TODO what's the min/max we'll get from this glove library?\n",
    "\n",
    "# this is so the glove output is in integer form (maybe not necessary)\n",
    "FLOAT_GRANULARITY = (1 << 16)\n",
    "VOCAB_SIZE = 2 * FLOAT_GRANULARITY + 1 # +/- and exclusive\n",
    "\n",
    "# this is so we can define a range for the values\n",
    "# value_max = 0\n",
    "# for tweet in tweets:\n",
    "#     for x in tweet:\n",
    "#         if abs(x) > value_max: value_max = abs(x)\n",
    "# print(\"Got max value of {}\".format(value_max))\n",
    "value_max = 10 # so we're not dependent on what the data is\n",
    "\n",
    "# split into test and train\n",
    "train_labels, train_data, test_labels, test_data = list(), list(), list(), list()\n",
    "for handle, tweet in zip(handles, tweets):\n",
    "#     if np.isnan(tweet[0]): continue #a row of all nan's happens with data that glove can't understand (like, all hashtags)\n",
    "#     tweet = [list(map(lambda x: int(x / value_max * FLOAT_GRANULARITY + FLOAT_GRANULARITY), word)) for word in tweet]\n",
    "    tweet = list(map(lambda x: int(x / value_max * FLOAT_GRANULARITY + FLOAT_GRANULARITY), tweet))\n",
    "    if random.random() < TEST_RATIO:\n",
    "        test_labels.append(handle)\n",
    "        test_data.append(tweet)\n",
    "    else:\n",
    "        train_labels.append(handle)\n",
    "        train_data.append(tweet)\n",
    "\n",
    "# document and validate\n",
    "print(\"Separated into {} train and {} test ({}%)\\n\".format(len(train_data), len(test_data), \n",
    "                                                         int(100.0 * len(test_data) / len(raw_tweets))))\n",
    "assert len(train_labels) == len(train_data) and len(train_data) > 0\n",
    "assert len(test_labels) == len(test_data) and len(test_data) > 0\n",
    "assert len(test_labels) > len(tweets) * (TEST_RATIO - .05)\n",
    "assert len(test_labels) < len(tweets) * (TEST_RATIO + .05) \n",
    "\n",
    "# save to dataframe\n",
    "train = pd.DataFrame({\n",
    "    LABEL: train_labels,\n",
    "    DATA: train_data,\n",
    "    LENGTH: [data_vector_size for _ in range(len(train_data))]\n",
    "})\n",
    "test = pd.DataFrame({\n",
    "    LABEL: test_labels,\n",
    "    DATA: test_data,\n",
    "    LENGTH: [data_vector_size for _ in range(len(test_data))]\n",
    "})\n",
    "print(train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequences:\n",
      " <class 'numpy.ndarray'>: \n",
      "[[65238 67236 67903 65160 65404 66382 70329 65335 64896 61693 64953 63832\n",
      "  38185 65877 65391 65533 65362 65759 63559 63627 65222 64546 65037 65957\n",
      "  65845 69034 66683 66426 67024 66235 67660 65220 65056 66287 70290 67269\n",
      "  66142 65217 64760 65971 61415 66596 66346 66987 67186 66745 64254 64269\n",
      "  67315 66451]\n",
      " [70029 66674 67728 64646 68629 66745 69239 66520 65146 65263 67618 64263\n",
      "  52351 69142 66498 63635 66369 68084 65817 66143 65092 67799 66270 63969\n",
      "  64544 65862 64925 63437 66675 66259 66505 63384 65963 64627 67645 64984\n",
      "  65991 65012 68394 63084 65005 64236 66555 64735 66069 65594 64956 65252\n",
      "  63968 64808]\n",
      " [66041 65724 67042 63663 64809 67065 69080 67450 64856 63974 66104 65256\n",
      "  38807 64996 67398 64393 64577 65475 64388 65176 66799 65080 65192 64835\n",
      "  65918 68746 67057 66312 65138 64654 66289 66215 65094 66027 68735 66797\n",
      "  65223 66777 65263 65341 61158 65163 67039 67178 67053 63666 66745 65018\n",
      "  64793 66961]]\n",
      " <class 'numpy.ndarray'>: \n",
      "[65238 67236 67903 65160 65404 66382 70329 65335 64896 61693 64953 63832\n",
      " 38185 65877 65391 65533 65362 65759 63559 63627 65222 64546 65037 65957\n",
      " 65845 69034 66683 66426 67024 66235 67660 65220 65056 66287 70290 67269\n",
      " 66142 65217 64760 65971 61415 66596 66346 66987 67186 66745 64254 64269\n",
      " 67315 66451]\n",
      " <class 'numpy.int32'>: \n",
      "65238\n",
      "\n",
      "\n",
      "Target values\n",
      " <class 'numpy.ndarray'>: \n",
      "[0 0 1]\n",
      " <class 'numpy.int64'>: \n",
      "0\n",
      "\n",
      "\n",
      "Sequence lengths\n",
      " <class 'numpy.ndarray'>: \n",
      "[50 50 50]\n",
      " <class 'numpy.int64'>: \n",
      "50\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "################################################\n",
    "# initializing our tensor\n",
    "#\n",
    "# based off of blogpost david parks showed us:\n",
    "# https://r2rt.com/recurrent-neural-networks-in-tensorflow-iii-variable-length-sequences.html\n",
    "################################################\n",
    "import tensorflow as tf\n",
    " \n",
    "class DataIterator():\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.size = len(self.df)\n",
    "        self.epochs = 0\n",
    "        self.shuffle()\n",
    "\n",
    "    def shuffle(self):\n",
    "        self.df = self.df.sample(frac=1).reset_index(drop=True)\n",
    "        self.cursor = 0\n",
    "\n",
    "    def next_batch(self, n):\n",
    "        if self.cursor + n - 1 > self.size:\n",
    "            self.epochs += 1\n",
    "            self.shuffle()\n",
    "        res = self.df.ix[self.cursor:self.cursor + n - 1]\n",
    "        start_idx = self.cursor\n",
    "        self.cursor += n\n",
    "        # return res[DATA], res[LABEL], res[LENGTH]\n",
    "        # the above line fails.  an error is thrown when tf attempts to call np.asarray on this.\n",
    "        # what is different about how our data is organized compared to the blog post this came from?\n",
    "        # TODO \n",
    "        data = res[DATA]\n",
    "        labels = res[LABEL]\n",
    "        length = res[LENGTH]\n",
    "        return np.asarray([data[i] for i in range(start_idx, start_idx + len(data))]), \\\n",
    "               np.asarray([labels[i] for i in range(start_idx, start_idx + len(labels))]), \\\n",
    "               np.asarray([length[i] for i in range(start_idx, start_idx + len(length))])\n",
    "\n",
    "# validate data iterator\n",
    "d = DataIterator(test).next_batch(3)\n",
    "print('Input sequences:\\n', \n",
    "      \"{}: \\n{}\\n\".format(type(d[0]), d[0]), \n",
    "      \"{}: \\n{}\\n\".format(type(d[0][0]), d[0][0]), \n",
    "      \"{}: \\n{}\\n\".format(type(d[0][0][0]), d[0][0][0]), \n",
    "      end='\\n\\n')\n",
    "print('Target values\\n', \n",
    "      \"{}: \\n{}\\n\".format(type(d[1]), d[1]), \n",
    "      \"{}: \\n{}\\n\".format(type(d[1][0]), d[1][0]), \n",
    "      end='\\n\\n')\n",
    "print('Sequence lengths\\n', \n",
    "      \"{}: \\n{}\\n\".format(type(d[2]), d[2]), \n",
    "      \"{}: \\n{}\\n\".format(type(d[2][0]), d[2][0]), \n",
    "      end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "################################################\n",
    "# initializing our tensor\n",
    "#\n",
    "# based off of blogpost david parks showed us:\n",
    "# https://r2rt.com/recurrent-neural-networks-in-tensorflow-iii-variable-length-sequences.html\n",
    "################################################\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def reset_graph():\n",
    "    if 'sess' in globals() and sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "\n",
    "def build_graph(vocab_size = VOCAB_SIZE, state_size = 64, batch_size = 256, num_classes = 2):\n",
    "\n",
    "    reset_graph()\n",
    "\n",
    "    # Placeholders\n",
    "    x = tf.placeholder(tf.int32, [batch_size, None]) # [batch_size, num_steps]\n",
    "    seqlen = tf.placeholder(tf.int32, [batch_size])\n",
    "    y = tf.placeholder(tf.int32, [batch_size])\n",
    "    keep_prob = tf.placeholder_with_default(1.0, [])\n",
    "\n",
    "    # Embedding layer\n",
    "    embeddings = tf.get_variable('embedding_matrix', [vocab_size, state_size])\n",
    "    rnn_inputs = tf.nn.embedding_lookup(embeddings, x)\n",
    "\n",
    "    # RNN\n",
    "    cell = tf.nn.rnn_cell.GRUCell(state_size)\n",
    "    init_state = tf.get_variable('init_state', [1, state_size],\n",
    "                                 initializer=tf.constant_initializer(0.0))\n",
    "    init_state = tf.tile(init_state, [batch_size, 1])\n",
    "    rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, rnn_inputs, sequence_length=seqlen,\n",
    "                                                 initial_state=init_state)\n",
    "\n",
    "    # Add dropout, as the model otherwise quickly overfits\n",
    "    rnn_outputs = tf.nn.dropout(rnn_outputs, keep_prob)\n",
    "    idx = tf.range(batch_size)*tf.shape(rnn_outputs)[1] + (seqlen - 1)\n",
    "    # last_rnn_output = tf.gather(tf.reshape(rnn_outputs, [-1, state_size]), idx)\n",
    "    # last_rnn_output = tf.gather_nd(rnn_outputs, tf.pack([tf.range(batch_size), seqlen-1], axis=1))\n",
    "    last_rnn_output = tf.gather_nd(rnn_outputs, tf.stack([tf.range(batch_size), seqlen-1], axis=1))\n",
    "\n",
    "    # Softmax layer\n",
    "    with tf.variable_scope('softmax'):\n",
    "        W = tf.get_variable('W', [state_size, num_classes]) # weights?\n",
    "        b = tf.get_variable('b', [num_classes], initializer=tf.constant_initializer(0.0)) # bias?\n",
    "    logits = tf.matmul(last_rnn_output, W) + b\n",
    "    preds = tf.nn.softmax(logits)\n",
    "    correct = tf.equal(tf.cast(tf.argmax(preds,1),tf.int32), y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "\n",
    "    return {\n",
    "        'x': x,\n",
    "        'seqlen': seqlen,\n",
    "        'y': y,\n",
    "        'dropout': keep_prob,\n",
    "        'loss': loss,\n",
    "        'ts': train_step,\n",
    "        'preds': preds,\n",
    "        'accuracy': accuracy\n",
    "    }\n",
    "\n",
    "\n",
    "def train_graph(g, batch_size = 256, num_epochs = 10, iterator = DataIterator):\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        tr = iterator(train)\n",
    "        te = iterator(test)\n",
    "\n",
    "        step, accuracy = 0, 0\n",
    "        tr_losses, te_losses = [], []\n",
    "        current_epoch = 0\n",
    "        while current_epoch < num_epochs:\n",
    "            step += 1\n",
    "            batch = tr.next_batch(batch_size)\n",
    "            feed = {g['x']: batch[0], g['y']: batch[1], g['seqlen']: batch[2], g['dropout']: 0.6}\n",
    "            accuracy_, _ = sess.run([g['accuracy'], g['ts']], feed_dict=feed)\n",
    "            accuracy += accuracy_\n",
    "\n",
    "            if tr.epochs > current_epoch:\n",
    "                current_epoch += 1\n",
    "                tr_losses.append(accuracy / step)\n",
    "                step, accuracy = 0, 0\n",
    "\n",
    "                #eval test set\n",
    "                te_epoch = te.epochs\n",
    "                while te.epochs == te_epoch:\n",
    "                    step += 1\n",
    "                    batch = te.next_batch(batch_size)\n",
    "                    feed = {g['x']: batch[0], g['y']: batch[1], g['seqlen']: batch[2]}\n",
    "                    accuracy_ = sess.run([g['accuracy']], feed_dict=feed)[0]\n",
    "                    accuracy += accuracy_\n",
    "\n",
    "                te_losses.append(accuracy / step)\n",
    "                step, accuracy = 0,0\n",
    "                print(\"Accuracy after epoch\", current_epoch, \" - tr:\", tr_losses[-1], \"- te:\", te_losses[-1])\n",
    "\n",
    "    return tr_losses, te_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after epoch 1  - tr: 0.502068014706 - te: 0.51171875\n",
      "Accuracy after epoch 2  - tr: 0.59423828125 - te: 0.51953125\n",
      "Accuracy after epoch 3  - tr: 0.666015625 - te: 0.53125\n",
      "Accuracy after epoch 4  - tr: 0.81640625 - te: 0.49609375\n",
      "Accuracy after epoch 5  - tr: 0.877197265625 - te: 0.453125\n",
      "Accuracy after epoch 6  - tr: 0.8896484375 - te: 0.5078125\n",
      "Accuracy after epoch 7  - tr: 0.862548828125 - te: 0.5\n",
      "Accuracy after epoch 8  - tr: 0.880615234375 - te: 0.5703125\n",
      "Accuracy after epoch 9  - tr: 0.788818359375 - te: 0.55078125\n",
      "Accuracy after epoch 10  - tr: 0.814697265625 - te: 0.5078125\n",
      "Accuracy after epoch 11  - tr: 0.920654296875 - te: 0.53125\n",
      "Accuracy after epoch 12  - tr: 0.955810546875 - te: 0.52734375\n",
      "Accuracy after epoch 13  - tr: 0.985107421875 - te: 0.5234375\n",
      "Accuracy after epoch 14  - tr: 0.995361328125 - te: 0.52734375\n",
      "Accuracy after epoch 15  - tr: 0.99609375 - te: 0.5390625\n",
      "Accuracy after epoch 16  - tr: 0.995849609375 - te: 0.5625\n",
      "Accuracy after epoch 17  - tr: 0.99755859375 - te: 0.515625\n",
      "Accuracy after epoch 18  - tr: 0.998291015625 - te: 0.52734375\n",
      "Accuracy after epoch 19  - tr: 0.998291015625 - te: 0.5546875\n",
      "Accuracy after epoch 20  - tr: 0.998046875 - te: 0.53515625\n",
      "Accuracy after epoch 21  - tr: 0.997802734375 - te: 0.5390625\n",
      "Accuracy after epoch 22  - tr: 0.998046875 - te: 0.5546875\n",
      "Accuracy after epoch 23  - tr: 0.997314453125 - te: 0.51171875\n",
      "Accuracy after epoch 24  - tr: 0.99755859375 - te: 0.57421875\n"
     ]
    }
   ],
   "source": [
    "################################################\n",
    "# run it!\n",
    "################################################\n",
    "\n",
    "# this fails, just like us\n",
    "g = build_graph()\n",
    "tr_losses, te_losses = train_graph(g, num_epochs=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEZCAYAAACervI0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2239a3e7fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "################################################\n",
    "# plot it!\n",
    "################################################\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "title = \"Mean Glove Vector\"\n",
    "title += \" (Variance Included)\" if INCLUDE_VARIANCE else \" (Variance Excluded)\"\n",
    "\n",
    "x_coords = [i+1 for i in range(len(tr_losses))]\n",
    "tr_line, = plt.plot(x_coords, tr_losses, 'r-', label=\"Train\")\n",
    "te_line, = plt.plot(x_coords, te_losses, 'b-', label=\"Test\")\n",
    "plt.axis([0, len(tr_losses), min(min(tr_losses), min(te_losses)) - .05, 1.05])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(title)\n",
    "plt.legend(handles=[tr_line, te_line], loc=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nWhat we tried:\\n\\nglove library - broke even\\n    overfit and data was varied\\nglove \"average tweet value\"\\n    overfit and test data was varied\\nglove ATV with variance \\n    this worked worse than ATV\\ntwitter glove worked worse (but need better tokenizing)\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TODO: we're overfitting, why?\n",
    "\n",
    "#TODO: try something better than averaging the word vec values\n",
    "# maybe we could do three-d arrays?  encode each word and pad the data\n",
    "\n",
    "#TODO: we strip a lot away (ie punctuation, smilies) and lose other\n",
    "# data to glove (#hashtags, @handles). how can we keep this?\n",
    "\n",
    "#TODO: how do we run this on the test dataset?\n",
    "\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "\"\"\"\n",
    "What we tried:\n",
    "\n",
    "glove library - broke even\n",
    "    overfit and data was varied\n",
    "glove \"average tweet value\"\n",
    "    overfit and test data was varied\n",
    "glove ATV with variance \n",
    "    this worked worse than ATV\n",
    "twitter glove worked worse (but need better tokenizing)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}