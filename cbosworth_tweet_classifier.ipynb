{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "################################################\n",
    "# intialize word enumeration\n",
    "################################################\n",
    "\n",
    "# config\n",
    "INCLUDE_BIGRAMS = True\n",
    "\n",
    "# definitions\n",
    "PADDING = 0\n",
    "UNKNOWN_WORD = 1\n",
    "ENUMERATION_BEGIN = 2\n",
    "\n",
    "# coverts this string to: ['converts', 'this', 'string', 'to', 'converts this', 'this string', 'string to']\n",
    "def add_bigrams_to_tweet(tweet):\n",
    "    # collect bigrams\n",
    "    bigrams = list()\n",
    "    prev = None\n",
    "    for word in tweet:\n",
    "        if prev is not None: bigrams.append(prev + \" \" + word)\n",
    "        prev = word\n",
    "    # append bigrams\n",
    "    for bigram in bigrams:\n",
    "        tweet.append(bigram)\n",
    "    # return, but this is the same list we passed in\n",
    "    return tweet\n",
    "\n",
    "\n",
    "def enumerate_words(tweets):\n",
    "    # enumerate all words (bigram or otherwise)\n",
    "    corpus = set()\n",
    "    for tweet in tweets:\n",
    "        for word in tweet:\n",
    "            corpus.add(word)\n",
    "    # save in dictionary\n",
    "    w2i = dict()\n",
    "    i = ENUMERATION_BEGIN\n",
    "    for word in list(corpus):\n",
    "        w2i[word] = i\n",
    "        i += 1\n",
    "    # return dict\n",
    "    return w2i\n",
    "\n",
    "\n",
    "def get_word_vector(w2i, tweet, enforce_length=None):\n",
    "    # prep\n",
    "    vec = list()\n",
    "    i = 0\n",
    "    # convert words to integers\n",
    "    for w in tweet:\n",
    "        if w in w2i: vec.append(w2i[w])\n",
    "        else: vec.append(UNKNOWN_WORD)\n",
    "        i += 1\n",
    "        # stop at enforce_length (if set)\n",
    "        if enforce_length is not None and i + 1 == enforce_length:\n",
    "            break\n",
    "    # pad\n",
    "    if enforce_length is not None:\n",
    "        while i < enforce_length:\n",
    "            vec.append(PADDING)\n",
    "            i += 1   \n",
    "    # fin\n",
    "    return vec\n",
    "        \n",
    "    \n",
    "# from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "# if bigram:\n",
    "#     vectorizer = CountVectorizer(ngram_range=(1, 2), token_pattern=r'\\\\b\\\\w+\\\\b',\n",
    "#                                  min_df=1, decode_error='ignore', stop_words=stop_words,\n",
    "#                                  lowercase=lowercase)\n",
    "#     vectorizer = CountVectorizer(decode_error='ignore', stop_words=stop_words, lowercase=lowercase)\\n\","
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "################################################\n",
    "# file parsing functions\n",
    "################################################\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import string, re\n",
    "import collections\n",
    "import numpy as np\n",
    "#import glove\n",
    "#from glove.glove_cython import fit_vectors, transform_paragraph\n",
    "\n",
    "# definitions\n",
    "HC=\"HillaryClinton\"\n",
    "DT=\"realDonaldTrump\"\n",
    "NA=\"none\"\n",
    "HANDLES = [HC,DT,NA]\n",
    "HANDLE_MAP = {NA:-1, HC:0, DT:1}\n",
    "\n",
    "# read csv file, return handles and tweets\n",
    "def parse_tweet_csv(file, file_encoding=\"utf8\"):\n",
    "    # init\n",
    "    handles, tweets = [], []\n",
    "    \n",
    "    # read file\n",
    "    linenr = -1\n",
    "    with open(file, encoding=file_encoding) as input:\n",
    "        try:\n",
    "            for line in input:\n",
    "                linenr += 1\n",
    "                if linenr == 0: continue\n",
    "                \n",
    "                # get contents\n",
    "                line = line.split(\",\")\n",
    "                if line[0] in HANDLES: #label and irst line of tweet\n",
    "                    handles.append(line[0])\n",
    "                    tweet = ','.join(line[1:])\n",
    "                    tweets.append(tweet)\n",
    "                else: #second+ line of tweet\n",
    "                    tweet = tweets.pop()\n",
    "                    tweet += ','.join(line)\n",
    "                    tweets.append(tweet)\n",
    "        except Exception as e:\n",
    "            print(\"Exception at line {}: {}\".format(linenr, e))\n",
    "            raise e\n",
    "    \n",
    "    # sanity checks\n",
    "    assert len(handles) == len(tweets)\n",
    "    print(\"Found {} tweets in {} lines\".format(len(tweets), linenr + 1))\n",
    "    \n",
    "    # return data\n",
    "    return handles, tweets\n",
    "\n",
    "\n",
    "##########################################\n",
    "### coverting tweet strings to numbers ###\n",
    "\n",
    "# coverting labels to integers\n",
    "def int_labels(labels):\n",
    "    return list(map(lambda x: HANDLE_MAP[x], labels))\n",
    "\n",
    "#tokenizing\n",
    "_tokenizer = TweetTokenizer()\n",
    "_punctuation = set(string.punctuation)\n",
    "def tokenize(tweet, lowercase=True, strip_urls=True, strip_punctuation=True):\n",
    "    tokens = _tokenizer.tokenize(tweet)\n",
    "    if lowercase: tokens = list(map(lambda x: x.lower(), tokens))\n",
    "    if strip_urls: tokens = list(filter(lambda x: not x.startswith(\"http\"), tokens))\n",
    "    if strip_punctuation: #https://stackoverflow.com/questions/265960/best-way-to-strip-punctuation-from-a-string-in-python\n",
    "        tokens = list(filter(lambda x: x.startswith(u'@') or x.startswith(u'#') or x not in _punctuation and not re.match(u\"[^\\w\\d'\\s$]+\", x), tokens))\n",
    "    return tokens\n",
    "\n",
    "# get all tweets\n",
    "def import_text(tweets):\n",
    "    tokenized_tweets = [tokenize(tweet) for tweet in tweets]\n",
    "    if INCLUDE_BIGRAMS:\n",
    "        for tweet in tokenized_tweets:\n",
    "            add_bigrams_to_tweet(tweet)\n",
    "    w2i = enumerate_words(tokenized_tweets)\n",
    "    return [get_word_vector(w2i, tweet, enforce_length=50) for tweet in tokenized_tweets], tokenized_tweets, w2i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#colleen version\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "def tokenize_tf_idf(trainingdata,testdata):\n",
    "    # local path for the downloaded nltk data\n",
    "    nltk.data.path.append(\"/cluster/home/cbosworth/nltk_data\")\n",
    "    vectorizer = TfidfVectorizer(input='content',stop_words=stopwords.words('english'), decode_error='ignore', norm='l2')\n",
    "    # merge train and test message lists for encoding\n",
    "    trainAndTest = trainingdata + testdata\n",
    "    X_trainAndTest = vectorizer.fit_transform(trainAndTest)\n",
    "    # split encoded train and test data\n",
    "    X = X_trainAndTest[:len(trainingdata)]\n",
    "    X_test = X_trainAndTest[len(trainingdata):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#colleen version with emojis and fun stuff\n",
    "\n",
    "_tokenizer = TweetTokenizer()\n",
    "_punctuation = set(string.punctuation)\n",
    "def tokenize(tweet, lowercase=True, strip_urls=True, strip_punctuation=True):\n",
    "    tokens = _tokenizer.tokenize(tweet)\n",
    "    if lowercase: tokens = list(map(lambda x: x.lower(), tokens))\n",
    "    if strip_urls: tokens = list(filter(lambda x: not x.startswith(\"http\"), tokens))\n",
    "    if strip_punctuation: #https://stackoverflow.com/questions/265960/best-way-to-strip-punctuation-from-a-string-in-python\n",
    "        tokens = list(filter(lambda x: x.startswith(u'@') or x.startswith(u'#') or x not in _punctuation and not re.match(u\"[^\\w\\d'\\s$]+\", x), tokens))\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4743 tweets in 6251 lines\n",
      "\n",
      "The question in this election: Who can put the plans into action that will make your life better? https://t.co/XreEY9OicG\n",
      "['the', 'question', 'in', 'this', 'election', 'who', 'can', 'put', 'the', 'plans', 'into', 'action', 'that', 'will', 'make', 'your', 'life', 'better']\n",
      "[7245, 6029, 2006, 355, 3169, 6621, 6043, 3062, 7245, 1431, 6989, 6690, 2326, 4945, 4193, 3220, 2611, 2978, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "################################################\n",
    "# get raw test data\n",
    "################################################\n",
    "import random\n",
    "\n",
    "# init\n",
    "TEST_RATIO = 0.1\n",
    "assert TEST_RATIO > 0 and TEST_RATIO < 1\n",
    "\n",
    "# get data\n",
    "text_handles, raw_tweets = parse_tweet_csv(\"train.csv\")\n",
    "handles = int_labels(text_handles)\n",
    "tweets, tokenized_tweets, word_mapping = import_text(raw_tweets)   \n",
    "data_vector_size = len(tweets[0])\n",
    "\n",
    "### validation\n",
    "for i in range(1):\n",
    "    rand_i = random.randint(0, len(raw_tweets))\n",
    "    print()\n",
    "    print(raw_tweets[i].strip())\n",
    "    print(tokenized_tweets[i])\n",
    "    print(tweets[i])\n",
    "    print(handles[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Separated into 4287 train and 456 test (9%)\n",
      "\n",
      "   handle  length                                         tweet_data\n",
      "0       0      50  [7245, 6029, 2006, 355, 3169, 6621, 6043, 3062...\n",
      "1       0      50  [1835, 781, 933, 6090, 3078, 46, 4899, 4668, 3...\n",
      "2       0      50  [5398, 1686, 3790, 3904, 2145, 2484, 1686, 719...\n",
      "3       0      50  [6123, 892, 8043, 348, 1746, 4638, 6343, 5390,...\n",
      "4       1      50  [3704, 2912, 6861, 6085, 8238, 4815, 7362, 838...\n"
     ]
    }
   ],
   "source": [
    "################################################\n",
    "# split test data into train and test\n",
    "################################################\n",
    "import pandas as pd\n",
    "\n",
    "LABEL = 'handle'\n",
    "DATA = 'tweet_data'\n",
    "LENGTH = 'length'\n",
    "\n",
    "VOCAB_SIZE = len(word_mapping) + ENUMERATION_BEGIN\n",
    "\n",
    "# split into test and train\n",
    "train_labels, train_data, test_labels, test_data = list(), list(), list(), list()\n",
    "for handle, tweet in zip(handles, tweets):\n",
    "    if random.random() < TEST_RATIO:\n",
    "        test_labels.append(handle)\n",
    "        test_data.append(tweet)\n",
    "    else:\n",
    "        train_labels.append(handle)\n",
    "        train_data.append(tweet)\n",
    "\n",
    "# document and validate\n",
    "print(\"Separated into {} train and {} test ({}%)\\n\".format(len(train_data), len(test_data), \n",
    "                                                         int(100.0 * len(test_data) / len(raw_tweets))))\n",
    "assert len(train_labels) == len(train_data) and len(train_data) > 0\n",
    "assert len(test_labels) == len(test_data) and len(test_data) > 0\n",
    "assert len(test_labels) > len(tweets) * (TEST_RATIO - .05)\n",
    "assert len(test_labels) < len(tweets) * (TEST_RATIO + .05) \n",
    "\n",
    "# save to dataframe\n",
    "train = pd.DataFrame({\n",
    "    LABEL: train_labels,\n",
    "    DATA: train_data,\n",
    "    LENGTH: [data_vector_size for _ in range(len(train_data))]\n",
    "})\n",
    "test = pd.DataFrame({\n",
    "    LABEL: test_labels,\n",
    "    DATA: test_data,\n",
    "    LENGTH: [data_vector_size for _ in range(len(test_data))]\n",
    "})\n",
    "print(train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequences:\n",
      " <class 'numpy.ndarray'>: \n",
      "[[4175 4294 7834 3067 8095 3370  261 1841 7405 3067 1079 7649 1154 4165\n",
      "  4691 7689 5345 1971    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0]\n",
      " [7411 4175 2076 1718 1114 2164 5441 6500 6861 4714 8095 4638 6708  262\n",
      "  1080 3880 1840 7611 4175    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0]]\n",
      " <class 'numpy.ndarray'>: \n",
      "[4175 4294 7834 3067 8095 3370  261 1841 7405 3067 1079 7649 1154 4165 4691\n",
      " 7689 5345 1971    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0]\n",
      " <class 'numpy.int64'>: \n",
      "4175\n",
      "\n",
      "\n",
      "Target values\n",
      " <class 'numpy.ndarray'>: \n",
      "[0 0]\n",
      " <class 'numpy.int64'>: \n",
      "0\n",
      "\n",
      "\n",
      "Sequence lengths\n",
      " <class 'numpy.ndarray'>: \n",
      "[50 50]\n",
      " <class 'numpy.int64'>: \n",
      "50\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cbseuser/anaconda/envs/tensorflow35/lib/python3.5/site-packages/ipykernel_launcher.py:24: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n"
     ]
    }
   ],
   "source": [
    "################################################\n",
    "# initializing our tensor\n",
    "#\n",
    "# based off of blogpost david parks showed us:\n",
    "# https://r2rt.com/recurrent-neural-networks-in-tensorflow-iii-variable-length-sequences.html\n",
    "################################################\n",
    "import tensorflow as tf\n",
    " \n",
    "class DataIterator():\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.size = len(self.df)\n",
    "        self.epochs = 0\n",
    "        self.shuffle()\n",
    "\n",
    "    def shuffle(self):\n",
    "        self.df = self.df.sample(frac=1).reset_index(drop=True)\n",
    "        self.cursor = 0\n",
    "\n",
    "    def next_batch(self, n):\n",
    "        if self.cursor + n - 1 > self.size:\n",
    "            self.epochs += 1\n",
    "            self.shuffle()\n",
    "        res = self.df.ix[self.cursor:self.cursor + n - 1]\n",
    "        start_idx = self.cursor\n",
    "        self.cursor += n\n",
    "        # return res[DATA], res[LABEL], res[LENGTH]\n",
    "        # the above line fails.  an error is thrown when tf attempts to call np.asarray on this.\n",
    "        # what is different about how our data is organized compared to the blog post this came from?\n",
    "        # TODO \n",
    "        data = res[DATA]\n",
    "        labels = res[LABEL]\n",
    "        length = res[LENGTH]\n",
    "        return np.asarray([data[i] for i in range(start_idx, start_idx + len(data))]), \\\n",
    "               np.asarray([labels[i] for i in range(start_idx, start_idx + len(labels))]), \\\n",
    "               np.asarray([length[i] for i in range(start_idx, start_idx + len(length))])\n",
    "\n",
    "# validate data iterator\n",
    "d = DataIterator(test).next_batch(2)\n",
    "print('Input sequences:\\n', \n",
    "      \"{}: \\n{}\\n\".format(type(d[0]), d[0]), \n",
    "      \"{}: \\n{}\\n\".format(type(d[0][0]), d[0][0]), \n",
    "      \"{}: \\n{}\\n\".format(type(d[0][0][0]), d[0][0][0]), \n",
    "      end='\\n\\n')\n",
    "print('Target values\\n', \n",
    "      \"{}: \\n{}\\n\".format(type(d[1]), d[1]), \n",
    "      \"{}: \\n{}\\n\".format(type(d[1][0]), d[1][0]), \n",
    "      end='\\n\\n')\n",
    "print('Sequence lengths\\n', \n",
    "      \"{}: \\n{}\\n\".format(type(d[2]), d[2]), \n",
    "      \"{}: \\n{}\\n\".format(type(d[2][0]), d[2][0]), \n",
    "      end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "################################################\n",
    "# initializing our tensor\n",
    "#\n",
    "# based off of blogpost david parks showed us:\n",
    "# https://r2rt.com/recurrent-neural-networks-in-tensorflow-iii-variable-length-sequences.html\n",
    "################################################\n",
    "import tensorflow as tf\n",
    "\n",
    "# this is a global variable we use to graph results.  it should be reset to 'list()' before running\n",
    "PLOTTING_INFO = None\n",
    "\n",
    "def reset_graph():\n",
    "    if 'sess' in globals() and sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "\n",
    "def build_graph(vocab_size = VOCAB_SIZE, state_size = 64, batch_size = 256, num_classes = 2):\n",
    "\n",
    "    reset_graph()\n",
    "\n",
    "    # Placeholders\n",
    "    x = tf.placeholder(tf.int32, [batch_size, None]) # [batch_size, num_steps]\n",
    "    seqlen = tf.placeholder(tf.int32, [batch_size])\n",
    "    y = tf.placeholder(tf.int32, [batch_size])\n",
    "    keep_prob = tf.placeholder_with_default(1.0, [])\n",
    "\n",
    "    # Embedding layer\n",
    "    embeddings = tf.get_variable('embedding_matrix', [vocab_size, state_size])\n",
    "    rnn_inputs = tf.nn.embedding_lookup(embeddings, x)\n",
    "\n",
    "    # RNN\n",
    "    num_layers=3\n",
    "    cell = tf.nn.rnn_cell.GRUCell(state_size)\n",
    "    #cell = tf.nn.rnn_cell.MultiRNNCell([cell] * num_layers)\n",
    "    #cell = tf.nn.rnn_cell.LSTMCell(state_size,state_is_tuple=False,forget_bias=1.0)\n",
    "    #cell = tf.nn.rnn_cell.MultiRNNCell([cell]*num_layers,state_is_tuple=True)\n",
    "    #init_state = tf.get_variable('init_state', [1, state_size],\n",
    "    #                             initializer=tf.constant_initializer(0.0))\n",
    "    #init_state = tf.tile(init_state, [batch_size, 1])\n",
    "    init_state = cell.zero_state(batch_size, tf.float32)\n",
    "    rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, rnn_inputs, sequence_length=seqlen,\n",
    "                                                 initial_state=init_state)\n",
    "\n",
    "    # Add dropout, as the model otherwise quickly overfits\n",
    "    #rnn_outputs = tf.nn.dropout(rnn_outputs, keep_prob)\n",
    "    rnn_outputs = tf.nn.dropout(rnn_outputs, 0.95)\n",
    "    idx = tf.range(batch_size)*tf.shape(rnn_outputs)[1] + (seqlen - 1)\n",
    "    # last_rnn_output = tf.gather(tf.reshape(rnn_outputs, [-1, state_size]), idx)\n",
    "    # last_rnn_output = tf.gather_nd(rnn_outputs, tf.pack([tf.range(batch_size), seqlen-1], axis=1))\n",
    "    last_rnn_output = tf.gather_nd(rnn_outputs, tf.stack([tf.range(batch_size), seqlen-1], axis=1))\n",
    "\n",
    "    # Softmax layer\n",
    "    with tf.variable_scope('softmax'):\n",
    "        W = tf.get_variable('W', [state_size, num_classes]) # weights?\n",
    "        b = tf.get_variable('b', [num_classes], initializer=tf.constant_initializer(0.0)) # bias?\n",
    "    logits = tf.matmul(last_rnn_output, W) + b\n",
    "    preds = tf.nn.softmax(logits)\n",
    "    correct = tf.equal(tf.cast(tf.argmax(preds,1),tf.int32), y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "\n",
    "    print(keep_prob)\n",
    "    return {\n",
    "        'x': x,\n",
    "        'seqlen': seqlen,\n",
    "        'y': y,\n",
    "        'dropout': keep_prob,\n",
    "        'loss': loss,\n",
    "        'ts': train_step,\n",
    "        'preds': preds,\n",
    "        'accuracy': accuracy\n",
    "    }\n",
    "\n",
    "\n",
    "def train_graph(g, batch_size = 256, num_epochs = 10, iterator = DataIterator):\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        tr = iterator(train)\n",
    "        te = iterator(test)\n",
    "\n",
    "        step, accuracy = 0, 0\n",
    "        tr_losses, te_losses = [], []\n",
    "        current_epoch = 0\n",
    "        while current_epoch < num_epochs:\n",
    "            step += 1\n",
    "            batch = tr.next_batch(batch_size)\n",
    "            feed = {g['x']: batch[0], g['y']: batch[1], g['seqlen']: batch[2], g['dropout']: 0.6}\n",
    "            accuracy_, _ = sess.run([g['accuracy'], g['ts']], feed_dict=feed)\n",
    "            accuracy += accuracy_\n",
    "\n",
    "            if tr.epochs > current_epoch:\n",
    "                current_epoch += 1\n",
    "                tr_losses.append(accuracy / step)\n",
    "                step, accuracy = 0, 0\n",
    "\n",
    "                #eval test set\n",
    "                te_epoch = te.epochs\n",
    "                while te.epochs == te_epoch:\n",
    "                    step += 1\n",
    "                    batch = te.next_batch(batch_size)\n",
    "                    feed = {g['x']: batch[0], g['y']: batch[1], g['seqlen']: batch[2]}\n",
    "                    accuracy_ = sess.run([g['accuracy']], feed_dict=feed)[0]\n",
    "                    accuracy += accuracy_\n",
    "\n",
    "                te_losses.append(accuracy / step)\n",
    "                step, accuracy = 0,0\n",
    "                print(\"Accuracy after epoch\", current_epoch, \" - tr:\", tr_losses[-1], \"- te:\", te_losses[-1])\n",
    "\n",
    "    return tr_losses, te_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"PlaceholderWithDefault:0\", shape=(), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cbseuser/anaconda/envs/tensorflow35/lib/python3.5/site-packages/ipykernel_launcher.py:24: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after epoch 1  - tr: 0.507123161765 - te: 0.431640625\n",
      "Accuracy after epoch 2  - tr: 0.504638671875 - te: 0.421875\n",
      "Accuracy after epoch 3  - tr: 0.5009765625 - te: 0.4375\n",
      "Accuracy after epoch 4  - tr: 0.502685546875 - te: 0.48046875\n",
      "Accuracy after epoch 5  - tr: 0.505859375 - te: 0.421875\n",
      "Accuracy after epoch 6  - tr: 0.498291015625 - te: 0.453125\n",
      "Accuracy after epoch 7  - tr: 0.528564453125 - te: 0.484375\n",
      "Accuracy after epoch 8  - tr: 0.53125 - te: 0.54296875\n",
      "Accuracy after epoch 9  - tr: 0.745361328125 - te: 0.88671875\n",
      "Accuracy after epoch 10  - tr: 0.9130859375 - te: 0.91015625\n",
      "Accuracy after epoch 11  - tr: 0.91552734375 - te: 0.92578125\n",
      "Accuracy after epoch 12  - tr: 0.932861328125 - te: 0.890625\n",
      "Accuracy after epoch 13  - tr: 0.94970703125 - te: 0.91015625\n",
      "Accuracy after epoch 14  - tr: 0.96484375 - te: 0.92578125\n",
      "Accuracy after epoch 15  - tr: 0.970703125 - te: 0.91796875\n",
      "Accuracy after epoch 16  - tr: 0.982177734375 - te: 0.93359375\n",
      "Accuracy after epoch 17  - tr: 0.982421875 - te: 0.92578125\n",
      "Accuracy after epoch 18  - tr: 0.987060546875 - te: 0.93359375\n",
      "Accuracy after epoch 19  - tr: 0.988525390625 - te: 0.8984375\n",
      "Accuracy after epoch 20  - tr: 0.99169921875 - te: 0.91015625\n",
      "Accuracy after epoch 21  - tr: 0.99365234375 - te: 0.9140625\n",
      "Accuracy after epoch 22  - tr: 0.993896484375 - te: 0.91015625\n",
      "Accuracy after epoch 23  - tr: 0.994140625 - te: 0.91015625\n",
      "Accuracy after epoch 24  - tr: 0.995361328125 - te: 0.921875\n",
      "Accuracy after epoch 25  - tr: 0.995849609375 - te: 0.91015625\n",
      "Accuracy after epoch 26  - tr: 0.995361328125 - te: 0.890625\n",
      "Accuracy after epoch 27  - tr: 0.9970703125 - te: 0.9140625\n",
      "Accuracy after epoch 28  - tr: 0.997314453125 - te: 0.921875\n",
      "Accuracy after epoch 29  - tr: 0.996826171875 - te: 0.9140625\n",
      "Accuracy after epoch 30  - tr: 0.996826171875 - te: 0.890625\n",
      "Accuracy after epoch 31  - tr: 0.997802734375 - te: 0.90234375\n",
      "Accuracy after epoch 32  - tr: 0.99755859375 - te: 0.91015625\n"
     ]
    }
   ],
   "source": [
    "################################################\n",
    "# run it!\n",
    "################################################\n",
    "\n",
    "# this fails, just like us\n",
    "g = build_graph()\n",
    "tr_losses, te_losses = train_graph(g, num_epochs=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nB8031eZVk1/iAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x120d8cac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "################################################\n",
    "# plot it!\n",
    "################################################\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "title = \"Word Enumeration\"\n",
    "#title += \" (Bigrams)\" if INCLUDE_BIGRAMS else \" (No Bigrams)\"\n",
    "\n",
    "x_coords = [i+1 for i in range(len(tr_losses))]\n",
    "tr_line, = plt.plot(x_coords, tr_losses, 'r-', label=\"Train\")\n",
    "te_line, = plt.plot(x_coords, te_losses, 'b-', label=\"Test\")\n",
    "plt.axis([0, len(tr_losses), min(min(tr_losses), min(te_losses)) - .05, 1.05])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(title)\n",
    "plt.legend(handles=[tr_line, te_line], loc=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nWhat we tried:\\n\\nglove library - broke even\\n    overfit and data was varied\\nglove \"average tweet value\"\\n    overfit and test data was varied\\nglove ATV with variance \\n    this worked worse than ATV\\ntwitter glove worked worse (but need better tokenizing)\\n\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TODO: we're overfitting, why?\n",
    "\n",
    "#TODO: try something better than averaging the word vec values\n",
    "# maybe we could do three-d arrays?  encode each word and pad the data\n",
    "\n",
    "#TODO: we strip a lot away (ie punctuation, smilies) and lose other\n",
    "# data to glove (#hashtags, @handles). how can we keep this?\n",
    "\n",
    "#TODO: how do we run this on the test dataset?\n",
    "\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "\"\"\"\n",
    "What we tried:\n",
    "\n",
    "glove library - broke even\n",
    "    overfit and data was varied\n",
    "glove \"average tweet value\"\n",
    "    overfit and test data was varied\n",
    "glove ATV with variance \n",
    "    this worked worse than ATV\n",
    "twitter glove worked worse (but need better tokenizing)\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python (tf35)",
   "language": "python",
   "name": "tensorflow35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
